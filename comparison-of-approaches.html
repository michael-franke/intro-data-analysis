<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>12.4 Comparison of approaches | An Introduction to Data Analysis</title>
  <meta name="description" content="Introductory text for statistics and data analysis (using R)" />
  <meta name="generator" content="bookdown 0.21.6 and GitBook 2.6.7" />

  <meta property="og:title" content="12.4 Comparison of approaches | An Introduction to Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Introductory text for statistics and data analysis (using R)" />
  <meta name="github-repo" content="michael-franke/intro-data-analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="12.4 Comparison of approaches | An Introduction to Data Analysis" />
  
  <meta name="twitter:description" content="Introductory text for statistics and data analysis (using R)" />
  

<meta name="author" content="Michael Franke" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="a-bayesian-approach.html"/>
<link rel="next" href="Chap-04-02-Bayes-regression-practice.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<!--<link rel="stylesheet" href="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-editor-1.0.9.css">-->
<link rel="stylesheet" href="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-viz-0.7.11.css">

<script src="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-editor-1.0.9.js"></script>
<script src="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-viz-0.7.11.js"></script>
<script src="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-v0.9.13.js" defer async></script>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />

<script type="application/javascript">
document.addEventListener('DOMContentLoaded', function() {
  document.querySelectorAll('.collapsibleSolution, .collapsibleProof').forEach(function(collapsible) {
    const content = collapsible.querySelector('.content')
    content.style.display = 'none';
    collapsible.querySelector('.trigger').addEventListener('click', function() {
      if (content.style.display === 'none') {
        content.style.display = 'block';
      } else {
        content.style.display = 'none';
      }
    })
  })
})
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span. { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
</style>


<link rel="stylesheet" href="styles.css" type="text/css" />
<link rel="stylesheet" href="webppl-editor.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="index.html#section"></a></li>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li class="chapter" data-level="1" data-path="general-introduction.html"><a href="general-introduction.html"><i class="fa fa-check"></i><b>1</b> General Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="Chap-01-00-intro-learning-goals.html"><a href="Chap-01-00-intro-learning-goals.html"><i class="fa fa-check"></i><b>1.1</b> Learning goals</a></li>
<li class="chapter" data-level="1.2" data-path="Chap-01-00-intro-course-structure.html"><a href="Chap-01-00-intro-course-structure.html"><i class="fa fa-check"></i><b>1.2</b> Course structure</a></li>
<li class="chapter" data-level="1.3" data-path="Chap-01-00-intro-tools.html"><a href="Chap-01-00-intro-tools.html"><i class="fa fa-check"></i><b>1.3</b> Tools used in this course</a></li>
<li class="chapter" data-level="1.4" data-path="Chap-01-00-intro-topics.html"><a href="Chap-01-00-intro-topics.html"><i class="fa fa-check"></i><b>1.4</b> Topics covered (and not covered) in the course</a></li>
<li class="chapter" data-level="1.5" data-path="Chap-01-00-intro-data-sets.html"><a href="Chap-01-00-intro-data-sets.html"><i class="fa fa-check"></i><b>1.5</b> Data sets covered</a></li>
<li class="chapter" data-level="1.6" data-path="Chap-01-00-intro-installation.html"><a href="Chap-01-00-intro-installation.html"><i class="fa fa-check"></i><b>1.6</b> Installation</a></li>
<li class="chapter" data-level="1.7" data-path="Chap-01-00-intro-schedule.html"><a href="Chap-01-00-intro-schedule.html"><i class="fa fa-check"></i><b>1.7</b> Example schedule (12 week course)</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap-01-01-R.html"><a href="Chap-01-01-R.html"><i class="fa fa-check"></i><b>2</b> Basics of R</a><ul>
<li class="chapter" data-level="2.1" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html"><i class="fa fa-check"></i><b>2.1</b> First steps</a><ul>
<li class="chapter" data-level="2.1.1" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#functions"><i class="fa fa-check"></i><b>2.1.1</b> Functions</a></li>
<li class="chapter" data-level="2.1.2" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#variables"><i class="fa fa-check"></i><b>2.1.2</b> Variables</a></li>
<li class="chapter" data-level="2.1.3" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#literate-coding"><i class="fa fa-check"></i><b>2.1.3</b> Literate coding</a></li>
<li class="chapter" data-level="2.1.4" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#objects"><i class="fa fa-check"></i><b>2.1.4</b> Objects</a></li>
<li class="chapter" data-level="2.1.5" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#packages"><i class="fa fa-check"></i><b>2.1.5</b> Packages</a></li>
<li class="chapter" data-level="2.1.6" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#Chap-01-01-R-help"><i class="fa fa-check"></i><b>2.1.6</b> Getting help</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="ch1-data-types.html"><a href="ch1-data-types.html"><i class="fa fa-check"></i><b>2.2</b> Data types</a><ul>
<li class="chapter" data-level="2.2.1" data-path="ch1-data-types.html"><a href="ch1-data-types.html#numeric-vectors-matrices"><i class="fa fa-check"></i><b>2.2.1</b> Numeric vectors &amp; matrices</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch1-data-types.html"><a href="ch1-data-types.html#booleans"><i class="fa fa-check"></i><b>2.2.2</b> Booleans</a></li>
<li class="chapter" data-level="2.2.3" data-path="ch1-data-types.html"><a href="ch1-data-types.html#special-values"><i class="fa fa-check"></i><b>2.2.3</b> Special values</a></li>
<li class="chapter" data-level="2.2.4" data-path="ch1-data-types.html"><a href="ch1-data-types.html#characters-strings"><i class="fa fa-check"></i><b>2.2.4</b> Characters (= strings)</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch1-data-types.html"><a href="ch1-data-types.html#factors"><i class="fa fa-check"></i><b>2.2.5</b> Factors</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch1-data-types.html"><a href="ch1-data-types.html#lists-data-frames-tibbles"><i class="fa fa-check"></i><b>2.2.6</b> Lists, data frames &amp; tibbles</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="Chap-01-01-functions.html"><a href="Chap-01-01-functions.html"><i class="fa fa-check"></i><b>2.3</b> Functions</a><ul>
<li class="chapter" data-level="2.3.1" data-path="Chap-01-01-functions.html"><a href="Chap-01-01-functions.html#some-important-built-in-functions"><i class="fa fa-check"></i><b>2.3.1</b> Some important built-in functions</a></li>
<li class="chapter" data-level="2.3.2" data-path="Chap-01-01-functions.html"><a href="Chap-01-01-functions.html#defining-your-own-functions"><i class="fa fa-check"></i><b>2.3.2</b> Defining your own functions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="ch-01-01-loops-and-maps.html"><a href="ch-01-01-loops-and-maps.html"><i class="fa fa-check"></i><b>2.4</b> Loops and maps</a><ul>
<li class="chapter" data-level="2.4.1" data-path="ch-01-01-loops-and-maps.html"><a href="ch-01-01-loops-and-maps.html#for-loops"><i class="fa fa-check"></i><b>2.4.1</b> For-loops</a></li>
<li class="chapter" data-level="2.4.2" data-path="ch-01-01-loops-and-maps.html"><a href="ch-01-01-loops-and-maps.html#functional-iterators"><i class="fa fa-check"></i><b>2.4.2</b> Functional iterators</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="Chap-01-01-piping.html"><a href="Chap-01-01-piping.html"><i class="fa fa-check"></i><b>2.5</b> Piping</a></li>
<li class="chapter" data-level="2.6" data-path="ch-01-01-Rmarkdown.html"><a href="ch-01-01-Rmarkdown.html"><i class="fa fa-check"></i><b>2.6</b> Rmarkdown</a></li>
</ul></li>
<li class="part"><span><b>II Data</b></span></li>
<li class="chapter" data-level="3" data-path="Chap-02-01-data.html"><a href="Chap-02-01-data.html"><i class="fa fa-check"></i><b>3</b> Data, variables &amp; experimental designs</a><ul>
<li class="chapter" data-level="3.1" data-path="Chap-02-01-data-what-is-data.html"><a href="Chap-02-01-data-what-is-data.html"><i class="fa fa-check"></i><b>3.1</b> What is data?</a></li>
<li class="chapter" data-level="3.2" data-path="Chap-02-01-data-kinds-of-data.html"><a href="Chap-02-01-data-kinds-of-data.html"><i class="fa fa-check"></i><b>3.2</b> Different kinds of data</a></li>
<li class="chapter" data-level="3.3" data-path="Chap-02-01-data-variables.html"><a href="Chap-02-01-data-variables.html"><i class="fa fa-check"></i><b>3.3</b> On the notion of “variables”</a></li>
<li class="chapter" data-level="3.4" data-path="Chap-02-01-data-exp-design.html"><a href="Chap-02-01-data-exp-design.html"><i class="fa fa-check"></i><b>3.4</b> Basics of experimental design</a><ul>
<li class="chapter" data-level="3.4.1" data-path="Chap-02-01-data-exp-design.html"><a href="Chap-02-01-data-exp-design.html#what-to-analyze-dependent-variables"><i class="fa fa-check"></i><b>3.4.1</b> What to analyze? – Dependent variables</a></li>
<li class="chapter" data-level="3.4.2" data-path="Chap-02-01-data-exp-design.html"><a href="Chap-02-01-data-exp-design.html#conditions-trials-items"><i class="fa fa-check"></i><b>3.4.2</b> Conditions, trials, items</a></li>
<li class="chapter" data-level="3.4.3" data-path="Chap-02-01-data-exp-design.html"><a href="Chap-02-01-data-exp-design.html#sample-size"><i class="fa fa-check"></i><b>3.4.3</b> Sample size</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data-wrangling.html"><a href="data-wrangling.html"><i class="fa fa-check"></i><b>4</b> Data Wrangling</a><ul>
<li class="chapter" data-level="4.1" data-path="Chap-02-02-data-IO.html"><a href="Chap-02-02-data-IO.html"><i class="fa fa-check"></i><b>4.1</b> Data in, data out</a></li>
<li class="chapter" data-level="4.2" data-path="Chap-02-02-data-tidy-data.html"><a href="Chap-02-02-data-tidy-data.html"><i class="fa fa-check"></i><b>4.2</b> Tidy data</a><ul>
<li class="chapter" data-level="4.2.1" data-path="Chap-02-02-data-tidy-data.html"><a href="Chap-02-02-data-tidy-data.html#running-example"><i class="fa fa-check"></i><b>4.2.1</b> Running example</a></li>
<li class="chapter" data-level="4.2.2" data-path="Chap-02-02-data-tidy-data.html"><a href="Chap-02-02-data-tidy-data.html#definition-of-tidy-data"><i class="fa fa-check"></i><b>4.2.2</b> Definition of <em>tidy data</em></a></li>
<li class="chapter" data-level="4.2.3" data-path="Chap-02-02-data-tidy-data.html"><a href="Chap-02-02-data-tidy-data.html#excursion-non-redundant-data"><i class="fa fa-check"></i><b>4.2.3</b> Excursion: non-redundant data</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html"><i class="fa fa-check"></i><b>4.3</b> Data manipulation: the basics</a><ul>
<li class="chapter" data-level="4.3.1" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html#pivoting"><i class="fa fa-check"></i><b>4.3.1</b> Pivoting</a></li>
<li class="chapter" data-level="4.3.2" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html#subsetting-row-columns"><i class="fa fa-check"></i><b>4.3.2</b> Subsetting row &amp; columns</a></li>
<li class="chapter" data-level="4.3.3" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html#Chap-02-02-tidy-selection"><i class="fa fa-check"></i><b>4.3.3</b> Tidy selection of column names</a></li>
<li class="chapter" data-level="4.3.4" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html#adding-changing-and-renaming-columns"><i class="fa fa-check"></i><b>4.3.4</b> Adding, changing and renaming columns</a></li>
<li class="chapter" data-level="4.3.5" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html#splitting-and-uniting-columns"><i class="fa fa-check"></i><b>4.3.5</b> Splitting and uniting columns</a></li>
<li class="chapter" data-level="4.3.6" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html#sorting-a-data-set"><i class="fa fa-check"></i><b>4.3.6</b> Sorting a data set</a></li>
<li class="chapter" data-level="4.3.7" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html#combining-tibbles"><i class="fa fa-check"></i><b>4.3.7</b> Combining tibbles</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="Chap-02-02-data-grouping-nesting.html"><a href="Chap-02-02-data-grouping-nesting.html"><i class="fa fa-check"></i><b>4.4</b> Grouped operations</a></li>
<li class="chapter" data-level="4.5" data-path="Chap-02-02-data-case-study-KoF.html"><a href="Chap-02-02-data-case-study-KoF.html"><i class="fa fa-check"></i><b>4.5</b> Case study: the King of France</a><ul>
<li class="chapter" data-level="4.5.1" data-path="Chap-02-02-data-case-study-KoF.html"><a href="Chap-02-02-data-case-study-KoF.html#cleaning-the-data"><i class="fa fa-check"></i><b>4.5.1</b> Cleaning the data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Chap-02-03-summary-statistics.html"><a href="Chap-02-03-summary-statistics.html"><i class="fa fa-check"></i><b>5</b> Summary statistics</a><ul>
<li class="chapter" data-level="5.1" data-path="Chap-02-03-summary-statistics-counts.html"><a href="Chap-02-03-summary-statistics-counts.html"><i class="fa fa-check"></i><b>5.1</b> Counts and proportions</a><ul>
<li class="chapter" data-level="5.1.1" data-path="Chap-02-03-summary-statistics-counts.html"><a href="Chap-02-03-summary-statistics-counts.html#loading-and-inspecting-the-data"><i class="fa fa-check"></i><b>5.1.1</b> Loading and inspecting the data</a></li>
<li class="chapter" data-level="5.1.2" data-path="Chap-02-03-summary-statistics-counts.html"><a href="Chap-02-03-summary-statistics-counts.html#obtaining-counts-with-n-count-and-tally"><i class="fa fa-check"></i><b>5.1.2</b> Obtaining counts with <code>n</code>, <code>count</code> and <code>tally</code></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="Chap-02-03-summary-statistics-1D.html"><a href="Chap-02-03-summary-statistics-1D.html"><i class="fa fa-check"></i><b>5.2</b> Central tendency and dispersion</a><ul>
<li class="chapter" data-level="5.2.1" data-path="Chap-02-03-summary-statistics-1D.html"><a href="Chap-02-03-summary-statistics-1D.html#the-data-for-the-remainder-of-the-chapter"><i class="fa fa-check"></i><b>5.2.1</b> The data for the remainder of the chapter</a></li>
<li class="chapter" data-level="5.2.2" data-path="Chap-02-03-summary-statistics-1D.html"><a href="Chap-02-03-summary-statistics-1D.html#measures-of-central-tendency"><i class="fa fa-check"></i><b>5.2.2</b> Measures of central tendency</a></li>
<li class="chapter" data-level="5.2.3" data-path="Chap-02-03-summary-statistics-1D.html"><a href="Chap-02-03-summary-statistics-1D.html#measures-of-dispersion"><i class="fa fa-check"></i><b>5.2.3</b> Measures of dispersion</a></li>
<li class="chapter" data-level="5.2.4" data-path="Chap-02-03-summary-statistics-1D.html"><a href="Chap-02-03-summary-statistics-1D.html#excursion-quantifying-confidence-with-bootstrapping"><i class="fa fa-check"></i><b>5.2.4</b> Excursion: Quantifying confidence with bootstrapping</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="Chap-02-03-summary-statistics-2D.html"><a href="Chap-02-03-summary-statistics-2D.html"><i class="fa fa-check"></i><b>5.3</b> Covariance and correlation</a><ul>
<li class="chapter" data-level="5.3.1" data-path="Chap-02-03-summary-statistics-2D.html"><a href="Chap-02-03-summary-statistics-2D.html#covariance"><i class="fa fa-check"></i><b>5.3.1</b> Covariance</a></li>
<li class="chapter" data-level="5.3.2" data-path="Chap-02-03-summary-statistics-2D.html"><a href="Chap-02-03-summary-statistics-2D.html#correlation"><i class="fa fa-check"></i><b>5.3.2</b> Correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Chap-02-02-visualization.html"><a href="Chap-02-02-visualization.html"><i class="fa fa-check"></i><b>6</b> Data Visualization</a><ul>
<li class="chapter" data-level="6.1" data-path="Chap-02-04-Anscombe-example.html"><a href="Chap-02-04-Anscombe-example.html"><i class="fa fa-check"></i><b>6.1</b> Motivating example: Anscombe’s quartet</a></li>
<li class="chapter" data-level="6.2" data-path="Chap-02-04-good-visualization.html"><a href="Chap-02-04-good-visualization.html"><i class="fa fa-check"></i><b>6.2</b> Visualization: the good, the bad and the infographic</a></li>
<li class="chapter" data-level="6.3" data-path="Chap-02-04-ggplot.html"><a href="Chap-02-04-ggplot.html"><i class="fa fa-check"></i><b>6.3</b> Basics of <code>ggplot</code></a><ul>
<li class="chapter" data-level="6.3.1" data-path="Chap-02-04-ggplot.html"><a href="Chap-02-04-ggplot.html#incremental-composition-of-a-plot"><i class="fa fa-check"></i><b>6.3.1</b> Incremental composition of a plot</a></li>
<li class="chapter" data-level="6.3.2" data-path="Chap-02-04-ggplot.html"><a href="Chap-02-04-ggplot.html#elements-in-the-layered-grammar-of-graphs"><i class="fa fa-check"></i><b>6.3.2</b> Elements in the layered grammar of graphs</a></li>
<li class="chapter" data-level="6.3.3" data-path="Chap-02-04-ggplot.html"><a href="Chap-02-04-ggplot.html#layers-and-groups"><i class="fa fa-check"></i><b>6.3.3</b> Layers and groups</a></li>
<li class="chapter" data-level="6.3.4" data-path="Chap-02-04-ggplot.html"><a href="Chap-02-04-ggplot.html#grouping"><i class="fa fa-check"></i><b>6.3.4</b> Grouping</a></li>
<li class="chapter" data-level="6.3.5" data-path="Chap-02-04-ggplot.html"><a href="Chap-02-04-ggplot.html#example-of-a-customized-plot"><i class="fa fa-check"></i><b>6.3.5</b> Example of a customized plot</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html"><i class="fa fa-check"></i><b>6.4</b> A rendezvous with popular geoms</a><ul>
<li class="chapter" data-level="6.4.1" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html#scatter-plots-with-geom_point"><i class="fa fa-check"></i><b>6.4.1</b> Scatter plots with <code>geom_point</code></a></li>
<li class="chapter" data-level="6.4.2" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html#smooth"><i class="fa fa-check"></i><b>6.4.2</b> Smooth</a></li>
<li class="chapter" data-level="6.4.3" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html#line"><i class="fa fa-check"></i><b>6.4.3</b> Line</a></li>
<li class="chapter" data-level="6.4.4" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html#barplot"><i class="fa fa-check"></i><b>6.4.4</b> Barplot</a></li>
<li class="chapter" data-level="6.4.5" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html#plotting-distributions-histograms-boxplots-densities-and-violins"><i class="fa fa-check"></i><b>6.4.5</b> Plotting distributions: histograms, boxplots, densities and violins</a></li>
<li class="chapter" data-level="6.4.6" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html#rugs"><i class="fa fa-check"></i><b>6.4.6</b> Rugs</a></li>
<li class="chapter" data-level="6.4.7" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html#annotation"><i class="fa fa-check"></i><b>6.4.7</b> Annotation</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="Chap-02-04-faceting.html"><a href="Chap-02-04-faceting.html"><i class="fa fa-check"></i><b>6.5</b> Faceting</a></li>
<li class="chapter" data-level="6.6" data-path="Chap-02-04-customization.html"><a href="Chap-02-04-customization.html"><i class="fa fa-check"></i><b>6.6</b> Customization etc.</a><ul>
<li class="chapter" data-level="6.6.1" data-path="Chap-02-04-customization.html"><a href="Chap-02-04-customization.html#themes"><i class="fa fa-check"></i><b>6.6.1</b> Themes</a></li>
<li class="chapter" data-level="6.6.2" data-path="Chap-02-04-customization.html"><a href="Chap-02-04-customization.html#guides"><i class="fa fa-check"></i><b>6.6.2</b> Guides</a></li>
<li class="chapter" data-level="6.6.3" data-path="Chap-02-04-customization.html"><a href="Chap-02-04-customization.html#axes-ticks-and-tick-labels"><i class="fa fa-check"></i><b>6.6.3</b> Axes, ticks and tick labels</a></li>
<li class="chapter" data-level="6.6.4" data-path="Chap-02-04-customization.html"><a href="Chap-02-04-customization.html#labels"><i class="fa fa-check"></i><b>6.6.4</b> Labels</a></li>
<li class="chapter" data-level="6.6.5" data-path="Chap-02-04-customization.html"><a href="Chap-02-04-customization.html#combining-arranging-plots"><i class="fa fa-check"></i><b>6.6.5</b> Combining &amp; arranging plots</a></li>
<li class="chapter" data-level="6.6.6" data-path="Chap-02-04-customization.html"><a href="Chap-02-04-customization.html#latex-expressions-in-plot-labels"><i class="fa fa-check"></i><b>6.6.6</b> LaTeX expressions in plot labels</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Bayesian Data Analysis</b></span></li>
<li class="chapter" data-level="7" data-path="Chap-03-01-probability.html"><a href="Chap-03-01-probability.html"><i class="fa fa-check"></i><b>7</b> Basics of Probability Theory</a><ul>
<li class="chapter" data-level="7.1" data-path="Chap-03-01-probability-basics.html"><a href="Chap-03-01-probability-basics.html"><i class="fa fa-check"></i><b>7.1</b> Probability</a><ul>
<li class="chapter" data-level="7.1.1" data-path="Chap-03-01-probability-basics.html"><a href="Chap-03-01-probability-basics.html#outcomes-events-observations"><i class="fa fa-check"></i><b>7.1.1</b> Outcomes, events, observations</a></li>
<li class="chapter" data-level="7.1.2" data-path="Chap-03-01-probability-basics.html"><a href="Chap-03-01-probability-basics.html#probability-distributions"><i class="fa fa-check"></i><b>7.1.2</b> Probability distributions</a></li>
<li class="chapter" data-level="7.1.3" data-path="Chap-03-01-probability-basics.html"><a href="Chap-03-01-probability-basics.html#interpretations-of-probability"><i class="fa fa-check"></i><b>7.1.3</b> Interpretations of probability</a></li>
<li class="chapter" data-level="7.1.4" data-path="Chap-03-01-probability-basics.html"><a href="Chap-03-01-probability-basics.html#distributions-as-samples"><i class="fa fa-check"></i><b>7.1.4</b> Distributions as samples</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="Chap-03-01-probability-marginal.html"><a href="Chap-03-01-probability-marginal.html"><i class="fa fa-check"></i><b>7.2</b> Structured events &amp; marginal distributions</a><ul>
<li class="chapter" data-level="7.2.1" data-path="Chap-03-01-probability-marginal.html"><a href="Chap-03-01-probability-marginal.html#probability-table-for-a-flip-and-draw-scenario"><i class="fa fa-check"></i><b>7.2.1</b> Probability table for a flip-and-draw scenario</a></li>
<li class="chapter" data-level="7.2.2" data-path="Chap-03-01-probability-marginal.html"><a href="Chap-03-01-probability-marginal.html#structured-events-and-joint-probability-distributions"><i class="fa fa-check"></i><b>7.2.2</b> Structured events and joint-probability distributions</a></li>
<li class="chapter" data-level="7.2.3" data-path="Chap-03-01-probability-marginal.html"><a href="Chap-03-01-probability-marginal.html#marginalization"><i class="fa fa-check"></i><b>7.2.3</b> Marginalization</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="Chap-03-01-probability-conditional.html"><a href="Chap-03-01-probability-conditional.html"><i class="fa fa-check"></i><b>7.3</b> Conditional probability</a><ul>
<li class="chapter" data-level="7.3.1" data-path="Chap-03-01-probability-conditional.html"><a href="Chap-03-01-probability-conditional.html#bayes-rule"><i class="fa fa-check"></i><b>7.3.1</b> Bayes rule</a></li>
<li class="chapter" data-level="7.3.2" data-path="Chap-03-01-probability-conditional.html"><a href="Chap-03-01-probability-conditional.html#Chap-03-01-probability-independence"><i class="fa fa-check"></i><b>7.3.2</b> Stochastic (in-)dependence</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="Chap-03-01-probability-random-variables.html"><a href="Chap-03-01-probability-random-variables.html"><i class="fa fa-check"></i><b>7.4</b> Random variables</a><ul>
<li class="chapter" data-level="7.4.1" data-path="Chap-03-01-probability-random-variables.html"><a href="Chap-03-01-probability-random-variables.html#notation-terminology"><i class="fa fa-check"></i><b>7.4.1</b> Notation &amp; terminology</a></li>
<li class="chapter" data-level="7.4.2" data-path="Chap-03-01-probability-random-variables.html"><a href="Chap-03-01-probability-random-variables.html#cumulative-distribution-functions-mass-density"><i class="fa fa-check"></i><b>7.4.2</b> Cumulative distribution functions, mass &amp; density</a></li>
<li class="chapter" data-level="7.4.3" data-path="Chap-03-01-probability-random-variables.html"><a href="Chap-03-01-probability-random-variables.html#expected-value-variance"><i class="fa fa-check"></i><b>7.4.3</b> Expected value &amp; variance</a></li>
<li class="chapter" data-level="7.4.4" data-path="Chap-03-01-probability-random-variables.html"><a href="Chap-03-01-probability-random-variables.html#composite-random-variables"><i class="fa fa-check"></i><b>7.4.4</b> Composite random variables</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="Chap-03-01-probability-R.html"><a href="Chap-03-01-probability-R.html"><i class="fa fa-check"></i><b>7.5</b> Probability distributions in R</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Chap-03-03-models.html"><a href="Chap-03-03-models.html"><i class="fa fa-check"></i><b>8</b> Statistical models</a><ul>
<li class="chapter" data-level="8.1" data-path="Chap-03-03-models-general.html"><a href="Chap-03-03-models-general.html"><i class="fa fa-check"></i><b>8.1</b> Statistical models</a></li>
<li class="chapter" data-level="8.2" data-path="Chap-03-03-models-representation.html"><a href="Chap-03-03-models-representation.html"><i class="fa fa-check"></i><b>8.2</b> Notation &amp; graphical representation</a><ul>
<li class="chapter" data-level="8.2.1" data-path="Chap-03-03-models-representation.html"><a href="Chap-03-03-models-representation.html#formula-notation"><i class="fa fa-check"></i><b>8.2.1</b> Formula notation</a></li>
<li class="chapter" data-level="8.2.2" data-path="Chap-03-03-models-representation.html"><a href="Chap-03-03-models-representation.html#graphical-notation"><i class="fa fa-check"></i><b>8.2.2</b> Graphical notation</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="Chap-03-03-models-parameters-priors.html"><a href="Chap-03-03-models-parameters-priors.html"><i class="fa fa-check"></i><b>8.3</b> Parameters, priors, and prior predictions</a><ul>
<li class="chapter" data-level="8.3.1" data-path="Chap-03-03-models-parameters-priors.html"><a href="Chap-03-03-models-parameters-priors.html#whats-a-model-parameter"><i class="fa fa-check"></i><b>8.3.1</b> What’s a model parameter?</a></li>
<li class="chapter" data-level="8.3.2" data-path="Chap-03-03-models-parameters-priors.html"><a href="Chap-03-03-models-parameters-priors.html#Chap-03-02-models-priors"><i class="fa fa-check"></i><b>8.3.2</b> Priors over parameters</a></li>
<li class="chapter" data-level="8.3.3" data-path="Chap-03-03-models-parameters-priors.html"><a href="Chap-03-03-models-parameters-priors.html#Chap-03-03-models-parameters-prior-predictive"><i class="fa fa-check"></i><b>8.3.3</b> Prior predictions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-03-04-parameter-estimation.html"><a href="ch-03-04-parameter-estimation.html"><i class="fa fa-check"></i><b>9</b> Bayesian parameter estimation</a><ul>
<li class="chapter" data-level="9.1" data-path="ch-03-03-estimation-bayes.html"><a href="ch-03-03-estimation-bayes.html"><i class="fa fa-check"></i><b>9.1</b> Bayes rule for parameter estimation</a><ul>
<li class="chapter" data-level="9.1.1" data-path="ch-03-03-estimation-bayes.html"><a href="ch-03-03-estimation-bayes.html#definitions-and-terminology"><i class="fa fa-check"></i><b>9.1.1</b> Definitions and terminology</a></li>
<li class="chapter" data-level="9.1.2" data-path="ch-03-03-estimation-bayes.html"><a href="ch-03-03-estimation-bayes.html#the-effects-of-prior-and-likelihood-on-the-posterior"><i class="fa fa-check"></i><b>9.1.2</b> The effects of prior and likelihood on the posterior</a></li>
<li class="chapter" data-level="9.1.3" data-path="ch-03-03-estimation-bayes.html"><a href="ch-03-03-estimation-bayes.html#ch-03-04-parameter-estimation-conjugacy"><i class="fa fa-check"></i><b>9.1.3</b> Computing Bayesian posteriors with conjugate priors</a></li>
<li class="chapter" data-level="9.1.4" data-path="ch-03-03-estimation-bayes.html"><a href="ch-03-03-estimation-bayes.html#excursion-sequential-updating"><i class="fa fa-check"></i><b>9.1.4</b> Excursion: Sequential updating</a></li>
<li class="chapter" data-level="9.1.5" data-path="ch-03-03-estimation-bayes.html"><a href="ch-03-03-estimation-bayes.html#posterior-predictive-distribution"><i class="fa fa-check"></i><b>9.1.5</b> Posterior predictive distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch-03-04-parameter-estimation-points-intervals.html"><a href="ch-03-04-parameter-estimation-points-intervals.html"><i class="fa fa-check"></i><b>9.2</b> Point-valued and interval-ranged estimates</a><ul>
<li class="chapter" data-level="9.2.1" data-path="ch-03-04-parameter-estimation-points-intervals.html"><a href="ch-03-04-parameter-estimation-points-intervals.html#point-valued-estimates"><i class="fa fa-check"></i><b>9.2.1</b> Point-valued estimates</a></li>
<li class="chapter" data-level="9.2.2" data-path="ch-03-04-parameter-estimation-points-intervals.html"><a href="ch-03-04-parameter-estimation-points-intervals.html#interval-ranged-estimates"><i class="fa fa-check"></i><b>9.2.2</b> Interval-ranged estimates</a></li>
<li class="chapter" data-level="9.2.3" data-path="ch-03-04-parameter-estimation-points-intervals.html"><a href="ch-03-04-parameter-estimation-points-intervals.html#computing-bayesian-estimates"><i class="fa fa-check"></i><b>9.2.3</b> Computing Bayesian estimates</a></li>
<li class="chapter" data-level="9.2.4" data-path="ch-03-04-parameter-estimation-points-intervals.html"><a href="ch-03-04-parameter-estimation-points-intervals.html#excursion-computing-mles-and-maps-in-r"><i class="fa fa-check"></i><b>9.2.4</b> Excursion: Computing MLEs and MAPs in R</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="Ch-03-03-estimation-algorithms.html"><a href="Ch-03-03-estimation-algorithms.html"><i class="fa fa-check"></i><b>9.3</b> Approximating the posterior</a><ul>
<li class="chapter" data-level="9.3.1" data-path="Ch-03-03-estimation-algorithms.html"><a href="Ch-03-03-estimation-algorithms.html#ch-03-03-MCMC"><i class="fa fa-check"></i><b>9.3.1</b> Of apples and trees: Markov Chain Monte Carlo sampling</a></li>
<li class="chapter" data-level="9.3.2" data-path="Ch-03-03-estimation-algorithms.html"><a href="Ch-03-03-estimation-algorithms.html#ch-03-03-estimation-Stan"><i class="fa fa-check"></i><b>9.3.2</b> Excursion: Probabilistic modeling with Stan</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="ch-03-04-parameter-estimation-normal.html"><a href="ch-03-04-parameter-estimation-normal.html"><i class="fa fa-check"></i><b>9.4</b> Estimating the parameters of a Normal distribution</a><ul>
<li class="chapter" data-level="9.4.1" data-path="ch-03-04-parameter-estimation-normal.html"><a href="ch-03-04-parameter-estimation-normal.html#uninformative-priors"><i class="fa fa-check"></i><b>9.4.1</b> Uninformative priors</a></li>
<li class="chapter" data-level="9.4.2" data-path="ch-03-04-parameter-estimation-normal.html"><a href="ch-03-04-parameter-estimation-normal.html#conjugate-priors"><i class="fa fa-check"></i><b>9.4.2</b> Conjugate priors</a></li>
<li class="chapter" data-level="9.4.3" data-path="ch-03-04-parameter-estimation-normal.html"><a href="ch-03-04-parameter-estimation-normal.html#estimating-the-difference-between-group-means"><i class="fa fa-check"></i><b>9.4.3</b> Estimating the difference between group means</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="Chap-03-06-model-comparison.html"><a href="Chap-03-06-model-comparison.html"><i class="fa fa-check"></i><b>10</b> Model Comparison</a><ul>
<li class="chapter" data-level="10.1" data-path="Chap-03-06-model-comparison-case-study.html"><a href="Chap-03-06-model-comparison-case-study.html"><i class="fa fa-check"></i><b>10.1</b> Case study: recall models</a></li>
<li class="chapter" data-level="10.2" data-path="Chap-03-06-model-comparison-AIC.html"><a href="Chap-03-06-model-comparison-AIC.html"><i class="fa fa-check"></i><b>10.2</b> Akaike Information Criterion</a></li>
<li class="chapter" data-level="10.3" data-path="Chap-03-06-model-comparison-BF.html"><a href="Chap-03-06-model-comparison-BF.html"><i class="fa fa-check"></i><b>10.3</b> Bayes factors</a><ul>
<li class="chapter" data-level="10.3.1" data-path="Chap-03-06-model-comparison-BF.html"><a href="Chap-03-06-model-comparison-BF.html#Chap-03-06-model-comparison-BF-grid"><i class="fa fa-check"></i><b>10.3.1</b> Grid approximation</a></li>
<li class="chapter" data-level="10.3.2" data-path="Chap-03-06-model-comparison-BF.html"><a href="Chap-03-06-model-comparison-BF.html#Chap-03-06-model-comparison-BF-naiveMC"><i class="fa fa-check"></i><b>10.3.2</b> Naive Monte Carlo</a></li>
<li class="chapter" data-level="10.3.3" data-path="Chap-03-06-model-comparison-BF.html"><a href="Chap-03-06-model-comparison-BF.html#Chap-03-06-model-comparison-BF-bridge"><i class="fa fa-check"></i><b>10.3.3</b> Excursion: Bridge sampling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-03-07-hypothesis-testing-Bayes.html"><a href="ch-03-07-hypothesis-testing-Bayes.html"><i class="fa fa-check"></i><b>11</b> Bayesian hypothesis testing</a><ul>
<li class="chapter" data-level="11.1" data-path="ch-03-07-hypothesis-testing-Bayes-hypotheses.html"><a href="ch-03-07-hypothesis-testing-Bayes-hypotheses.html"><i class="fa fa-check"></i><b>11.1</b> Statistical hypotheses</a></li>
<li class="chapter" data-level="11.2" data-path="data-and-models-for-this-chapter.html"><a href="data-and-models-for-this-chapter.html"><i class="fa fa-check"></i><b>11.2</b> Data and models for this chapter</a><ul>
<li class="chapter" data-level="11.2.1" data-path="data-and-models-for-this-chapter.html"><a href="data-and-models-for-this-chapter.html#section-1"><i class="fa fa-check"></i><b>11.2.1</b> 24/7</a></li>
<li class="chapter" data-level="11.2.2" data-path="data-and-models-for-this-chapter.html"><a href="data-and-models-for-this-chapter.html#simon-task"><i class="fa fa-check"></i><b>11.2.2</b> Simon task</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="ch-03-05-Bayes-testing-estimation.html"><a href="ch-03-05-Bayes-testing-estimation.html"><i class="fa fa-check"></i><b>11.3</b> Testing via posterior estimation</a><ul>
<li class="chapter" data-level="11.3.1" data-path="ch-03-05-Bayes-testing-estimation.html"><a href="ch-03-05-Bayes-testing-estimation.html#example-247"><i class="fa fa-check"></i><b>11.3.1</b> Example: 24/7</a></li>
<li class="chapter" data-level="11.3.2" data-path="ch-03-05-Bayes-testing-estimation.html"><a href="ch-03-05-Bayes-testing-estimation.html#example-simon-task"><i class="fa fa-check"></i><b>11.3.2</b> Example: Simon Task</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="ch-03-05-Bayesian-testing-comparison.html"><a href="ch-03-05-Bayesian-testing-comparison.html"><i class="fa fa-check"></i><b>11.4</b> Testing via model comparison</a><ul>
<li class="chapter" data-level="11.4.1" data-path="ch-03-05-Bayesian-testing-comparison.html"><a href="ch-03-05-Bayesian-testing-comparison.html#ch-03-07-hypothesis-testing-Bayes-Savage-Dickey"><i class="fa fa-check"></i><b>11.4.1</b> The Savage-Dickey method</a></li>
<li class="chapter" data-level="11.4.2" data-path="ch-03-05-Bayesian-testing-comparison.html"><a href="ch-03-05-Bayesian-testing-comparison.html#ch-03-07-hypothesis-testing-Bayes-encompassing-models"><i class="fa fa-check"></i><b>11.4.2</b> Encompassing models</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Applied (generalized) linear modeling</b></span></li>
<li class="chapter" data-level="12" data-path="Chap-04-01-simple-linear-regression.html"><a href="Chap-04-01-simple-linear-regression.html"><i class="fa fa-check"></i><b>12</b> Linear regression</a><ul>
<li class="chapter" data-level="12.1" data-path="ordinary-least-squares-regression.html"><a href="ordinary-least-squares-regression.html"><i class="fa fa-check"></i><b>12.1</b> Ordinary least squares regression</a><ul>
<li class="chapter" data-level="12.1.1" data-path="ordinary-least-squares-regression.html"><a href="ordinary-least-squares-regression.html#prediction-without-any-further-information"><i class="fa fa-check"></i><b>12.1.1</b> Prediction without any further information</a></li>
<li class="chapter" data-level="12.1.2" data-path="ordinary-least-squares-regression.html"><a href="ordinary-least-squares-regression.html#prediction-with-knowledge-of-unemployment-rate"><i class="fa fa-check"></i><b>12.1.2</b> Prediction with knowledge of unemployment rate</a></li>
<li class="chapter" data-level="12.1.3" data-path="ordinary-least-squares-regression.html"><a href="ordinary-least-squares-regression.html#linear-regression-general-problem-formulation"><i class="fa fa-check"></i><b>12.1.3</b> Linear regression: general problem formulation</a></li>
<li class="chapter" data-level="12.1.4" data-path="ordinary-least-squares-regression.html"><a href="ordinary-least-squares-regression.html#finding-the-ols-solution"><i class="fa fa-check"></i><b>12.1.4</b> Finding the OLS-solution</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="Chap-04-01-linear-regression-MLE.html"><a href="Chap-04-01-linear-regression-MLE.html"><i class="fa fa-check"></i><b>12.2</b> A maximum-likelihood approach</a><ul>
<li class="chapter" data-level="12.2.1" data-path="Chap-04-01-linear-regression-MLE.html"><a href="Chap-04-01-linear-regression-MLE.html#a-likelihood-based-model"><i class="fa fa-check"></i><b>12.2.1</b> A likelihood-based model</a></li>
<li class="chapter" data-level="12.2.2" data-path="Chap-04-01-linear-regression-MLE.html"><a href="Chap-04-01-linear-regression-MLE.html#finding-the-mle-solution-with-optim"><i class="fa fa-check"></i><b>12.2.2</b> Finding the MLE-solution with <code>optim</code></a></li>
<li class="chapter" data-level="12.2.3" data-path="Chap-04-01-linear-regression-MLE.html"><a href="Chap-04-01-linear-regression-MLE.html#finding-the-mle-solution-with-glm"><i class="fa fa-check"></i><b>12.2.3</b> Finding the MLE-solution with <code>glm</code></a></li>
<li class="chapter" data-level="12.2.4" data-path="Chap-04-01-linear-regression-MLE.html"><a href="Chap-04-01-linear-regression-MLE.html#finding-the-mle-solution-with-math"><i class="fa fa-check"></i><b>12.2.4</b> Finding the MLE-solution with math</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="a-bayesian-approach.html"><a href="a-bayesian-approach.html"><i class="fa fa-check"></i><b>12.3</b> A Bayesian approach</a></li>
<li class="chapter" data-level="12.4" data-path="comparison-of-approaches.html"><a href="comparison-of-approaches.html"><i class="fa fa-check"></i><b>12.4</b> Comparison of approaches</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Chap-04-02-Bayes-regression-practice.html"><a href="Chap-04-02-Bayes-regression-practice.html"><i class="fa fa-check"></i><b>13</b> Bayesian regression in practice</a><ul>
<li class="chapter" data-level="13.1" data-path="simple-linear-regression-with-brms.html"><a href="simple-linear-regression-with-brms.html"><i class="fa fa-check"></i><b>13.1</b> Simple linear regression with <code>brms</code></a></li>
<li class="chapter" data-level="13.2" data-path="extracting-posterior-samples.html"><a href="extracting-posterior-samples.html"><i class="fa fa-check"></i><b>13.2</b> Extracting posterior samples</a></li>
<li class="chapter" data-level="13.3" data-path="excursion-inspecting-the-underlying-stan-code.html"><a href="excursion-inspecting-the-underlying-stan-code.html"><i class="fa fa-check"></i><b>13.3</b> [Excursion:] Inspecting the underlying Stan code</a></li>
<li class="chapter" data-level="13.4" data-path="setting-priors.html"><a href="setting-priors.html"><i class="fa fa-check"></i><b>13.4</b> Setting priors</a></li>
<li class="chapter" data-level="13.5" data-path="posterior-predictions.html"><a href="posterior-predictions.html"><i class="fa fa-check"></i><b>13.5</b> Posterior predictions</a></li>
<li class="chapter" data-level="13.6" data-path="testing-hypotheses.html"><a href="testing-hypotheses.html"><i class="fa fa-check"></i><b>13.6</b> Testing hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Chap-04-03-predictors.html"><a href="Chap-04-03-predictors.html"><i class="fa fa-check"></i><b>14</b> Categorical predictors</a><ul>
<li class="chapter" data-level="14.1" data-path="Chap-04-03-predictors-two-levels.html"><a href="Chap-04-03-predictors-two-levels.html"><i class="fa fa-check"></i><b>14.1</b> Single two-level predictor</a></li>
<li class="chapter" data-level="14.2" data-path="Chap-04-03-predictors-multi-levels.html"><a href="Chap-04-03-predictors-multi-levels.html"><i class="fa fa-check"></i><b>14.2</b> Single multi-level predictor</a></li>
<li class="chapter" data-level="14.3" data-path="Chap-04-03-predictors-multiple-predictors.html"><a href="Chap-04-03-predictors-multiple-predictors.html"><i class="fa fa-check"></i><b>14.3</b> Multiple predictors</a><ul>
<li class="chapter" data-level="14.3.1" data-path="Chap-04-03-predictors-multiple-predictors.html"><a href="Chap-04-03-predictors-multiple-predictors.html#treatment-coding"><i class="fa fa-check"></i><b>14.3.1</b> Treatment coding</a></li>
<li class="chapter" data-level="14.3.2" data-path="Chap-04-03-predictors-multiple-predictors.html"><a href="Chap-04-03-predictors-multiple-predictors.html#sum-coding"><i class="fa fa-check"></i><b>14.3.2</b> Sum coding</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="Chap-04-04-GLM.html"><a href="Chap-04-04-GLM.html"><i class="fa fa-check"></i><b>15</b> Generalized linear model</a><ul>
<li class="chapter" data-level="15.1" data-path="generalizing-the-linear-regression-model.html"><a href="generalizing-the-linear-regression-model.html"><i class="fa fa-check"></i><b>15.1</b> Generalizing the linear regression model</a></li>
<li class="chapter" data-level="15.2" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>15.2</b> Logistic regression</a></li>
</ul></li>
<li class="part"><span><b>V Frequentist statistics</b></span></li>
<li class="chapter" data-level="16" data-path="ch-05-01-frequentist-hypothesis-testing.html"><a href="ch-05-01-frequentist-hypothesis-testing.html"><i class="fa fa-check"></i><b>16</b> Null Hypothesis Significance Testing</a><ul>
<li class="chapter" data-level="16.1" data-path="ch-05-01-frequentist-testing-overview.html"><a href="ch-05-01-frequentist-testing-overview.html"><i class="fa fa-check"></i><b>16.1</b> Frequentist statistics: why &amp; how</a></li>
<li class="chapter" data-level="16.2" data-path="ch-03-05-hypothesis-p-values.html"><a href="ch-03-05-hypothesis-p-values.html"><i class="fa fa-check"></i><b>16.2</b> Quantifying evidence against a null-model with <em>p</em>-values</a><ul>
<li class="chapter" data-level="16.2.1" data-path="ch-03-05-hypothesis-p-values.html"><a href="ch-03-05-hypothesis-p-values.html#frequentist-null-models"><i class="fa fa-check"></i><b>16.2.1</b> Frequentist null-models</a></li>
<li class="chapter" data-level="16.2.2" data-path="ch-03-05-hypothesis-p-values.html"><a href="ch-03-05-hypothesis-p-values.html#one--vs.two-sided-p-values"><i class="fa fa-check"></i><b>16.2.2</b> One- vs. two-sided <span class="math inline">\(p\)</span>-values</a></li>
<li class="chapter" data-level="16.2.3" data-path="ch-03-05-hypothesis-p-values.html"><a href="ch-03-05-hypothesis-p-values.html#significance-categorical-decisions"><i class="fa fa-check"></i><b>16.2.3</b> Significance &amp; categorical decisions</a></li>
<li class="chapter" data-level="16.2.4" data-path="ch-03-05-hypothesis-p-values.html"><a href="ch-03-05-hypothesis-p-values.html#how-not-to-interpret-p-values"><i class="fa fa-check"></i><b>16.2.4</b> How (not) to interpret <em>p</em>-values</a></li>
<li class="chapter" data-level="16.2.5" data-path="ch-03-05-hypothesis-p-values.html"><a href="ch-03-05-hypothesis-p-values.html#excursion-distribution-of-p-values"><i class="fa fa-check"></i><b>16.2.5</b> [Excursion] Distribution of <span class="math inline">\(p\)</span>-values</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="ch-03-05-hypothesis-testing-CLT.html"><a href="ch-03-05-hypothesis-testing-CLT.html"><i class="fa fa-check"></i><b>16.3</b> [Excursion] Central Limit Theorem</a></li>
<li class="chapter" data-level="16.4" data-path="ch-03-04-hypothesis-significance-errors.html"><a href="ch-03-04-hypothesis-significance-errors.html"><i class="fa fa-check"></i><b>16.4</b> [Excursion] The Neyman-Pearson approach</a></li>
<li class="chapter" data-level="16.5" data-path="ch-05-01-frequentist-testing-confidence-intervals.html"><a href="ch-05-01-frequentist-testing-confidence-intervals.html"><i class="fa fa-check"></i><b>16.5</b> Confidence intervals</a><ul>
<li class="chapter" data-level="16.5.1" data-path="ch-05-01-frequentist-testing-confidence-intervals.html"><a href="ch-05-01-frequentist-testing-confidence-intervals.html#relation-of-p-values-to-confidence-intervals"><i class="fa fa-check"></i><b>16.5.1</b> Relation of <em>p</em>-values to confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="ch-03-05-hypothesis-testing-tests.html"><a href="ch-03-05-hypothesis-testing-tests.html"><i class="fa fa-check"></i><b>16.6</b> Selected tests</a><ul>
<li class="chapter" data-level="16.6.1" data-path="ch-03-05-hypothesis-testing-tests.html"><a href="ch-03-05-hypothesis-testing-tests.html#ch-03-05-hypothesis-testing-Pearsons-Chi"><i class="fa fa-check"></i><b>16.6.1</b> Pearson’s <span class="math inline">\(\chi^2\)</span>-tests</a></li>
<li class="chapter" data-level="16.6.2" data-path="ch-03-05-hypothesis-testing-tests.html"><a href="ch-03-05-hypothesis-testing-tests.html#ch-03-05-hypothesis-testing-z-test"><i class="fa fa-check"></i><b>16.6.2</b> <em>z</em>-test</a></li>
<li class="chapter" data-level="16.6.3" data-path="ch-03-05-hypothesis-testing-tests.html"><a href="ch-03-05-hypothesis-testing-tests.html#ch-03-05-hypothesis-testing-t-test"><i class="fa fa-check"></i><b>16.6.3</b> <em>t</em>-tests</a></li>
<li class="chapter" data-level="16.6.4" data-path="ch-03-05-hypothesis-testing-tests.html"><a href="ch-03-05-hypothesis-testing-tests.html#ch-03-05-hypothesis-testing-ANOVA"><i class="fa fa-check"></i><b>16.6.4</b> ANOVA</a></li>
<li class="chapter" data-level="16.6.5" data-path="ch-03-05-hypothesis-testing-tests.html"><a href="ch-03-05-hypothesis-testing-tests.html#linear-regression"><i class="fa fa-check"></i><b>16.6.5</b> Linear regression</a></li>
<li class="chapter" data-level="16.6.6" data-path="ch-03-05-hypothesis-testing-tests.html"><a href="ch-03-05-hypothesis-testing-tests.html#Chap-05-01-LR-test"><i class="fa fa-check"></i><b>16.6.6</b> Likelihood-Ratio Test</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ch-05-02-comparison-freq-Bayes.html"><a href="ch-05-02-comparison-freq-Bayes.html"><i class="fa fa-check"></i><b>17</b> Comparing frequentist and Bayesian statistics</a><ul>
<li class="chapter" data-level="17.1" data-path="frequentist-and-bayesian-statistical-models.html"><a href="frequentist-and-bayesian-statistical-models.html"><i class="fa fa-check"></i><b>17.1</b> Frequentist and Bayesian statistical models</a></li>
<li class="chapter" data-level="17.2" data-path="approximation-in-the-model-or-through-the-computation.html"><a href="approximation-in-the-model-or-through-the-computation.html"><i class="fa fa-check"></i><b>17.2</b> Approximation: in the model or through the computation</a></li>
<li class="chapter" data-level="17.3" data-path="mc-simulated-p-values.html"><a href="mc-simulated-p-values.html"><i class="fa fa-check"></i><b>17.3</b> MC-simulated <span class="math inline">\(p\)</span>-values</a></li>
<li class="chapter" data-level="17.4" data-path="bayesian-p-values-model-checking.html"><a href="bayesian-p-values-model-checking.html"><i class="fa fa-check"></i><b>17.4</b> Bayesian <span class="math inline">\(p\)</span>-values &amp; model checking</a></li>
<li class="chapter" data-level="17.5" data-path="ch-05-01-estimation-comparison.html"><a href="ch-05-01-estimation-comparison.html"><i class="fa fa-check"></i><b>17.5</b> Comparing Bayesian and frequentist estimates</a></li>
<li class="chapter" data-level="17.6" data-path="beliefs-decisions-and-long-term-error.html"><a href="beliefs-decisions-and-long-term-error.html"><i class="fa fa-check"></i><b>17.6</b> Beliefs, decisions and long-term error</a></li>
<li class="chapter" data-level="17.7" data-path="evidence-for-the-null.html"><a href="evidence-for-the-null.html"><i class="fa fa-check"></i><b>17.7</b> Evidence for the null</a></li>
<li class="chapter" data-level="17.8" data-path="Chap-05-02-models-three-pillars.html"><a href="Chap-05-02-models-three-pillars.html"><i class="fa fa-check"></i><b>17.8</b> Three pillars of data analysis</a></li>
<li class="chapter" data-level="17.9" data-path="testing-hypotheses-by-estimation-comparison-model-checking.html"><a href="testing-hypotheses-by-estimation-comparison-model-checking.html"><i class="fa fa-check"></i><b>17.9</b> Testing hypotheses by estimation, comparison &amp; model checking</a></li>
<li class="chapter" data-level="17.10" data-path="jeffreys-lindley-paradox.html"><a href="jeffreys-lindley-paradox.html"><i class="fa fa-check"></i><b>17.10</b> Jeffreys-Lindley paradox</a></li>
<li class="chapter" data-level="17.11" data-path="explicit-beliefs-vs-implicit-intentions.html"><a href="explicit-beliefs-vs-implicit-intentions.html"><i class="fa fa-check"></i><b>17.11</b> Explicit beliefs vs. implicit intentions</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="app-90-further-material.html"><a href="app-90-further-material.html"><i class="fa fa-check"></i><b>A</b> Further useful material</a><ul>
<li class="chapter" data-level="A.1" data-path="material-on-introduction-to-probability.html"><a href="material-on-introduction-to-probability.html"><i class="fa fa-check"></i><b>A.1</b> Material on <em>Introduction to Probability</em>:</a></li>
<li class="chapter" data-level="A.2" data-path="material-on-bayesian-data-analysis.html"><a href="material-on-bayesian-data-analysis.html"><i class="fa fa-check"></i><b>A.2</b> Material on <em>Bayesian Data Analysis</em>:</a></li>
<li class="chapter" data-level="A.3" data-path="material-on-frequentist-statistics.html"><a href="material-on-frequentist-statistics.html"><i class="fa fa-check"></i><b>A.3</b> Material on <em>frequentist statistics</em>:</a></li>
<li class="chapter" data-level="A.4" data-path="material-on-r-tidyverse-etc-.html"><a href="material-on-r-tidyverse-etc-.html"><i class="fa fa-check"></i><b>A.4</b> Material on <em>R, tidyverse, etc.</em>:</a></li>
<li class="chapter" data-level="A.5" data-path="further-information-for-rstudio.html"><a href="further-information-for-rstudio.html"><i class="fa fa-check"></i><b>A.5</b> Further information for RStudio</a></li>
<li class="chapter" data-level="A.6" data-path="further-information-on-webppl.html"><a href="further-information-on-webppl.html"><i class="fa fa-check"></i><b>A.6</b> Further information on WebPPL</a><ul>
<li class="chapter" data-level="A.6.1" data-path="further-information-on-webppl.html"><a href="further-information-on-webppl.html#primitives-and-sampling-functions"><i class="fa fa-check"></i><b>A.6.1</b> Primitives and sampling functions</a></li>
<li class="chapter" data-level="A.6.2" data-path="further-information-on-webppl.html"><a href="further-information-on-webppl.html#inference-with-infer"><i class="fa fa-check"></i><b>A.6.2</b> Inference with <code>Infer()</code></a></li>
<li class="chapter" data-level="A.6.3" data-path="further-information-on-webppl.html"><a href="further-information-on-webppl.html#visualization"><i class="fa fa-check"></i><b>A.6.3</b> Visualization</a></li>
<li class="chapter" data-level="A.6.4" data-path="further-information-on-webppl.html"><a href="further-information-on-webppl.html#installation"><i class="fa fa-check"></i><b>A.6.4</b> Installation</a></li>
<li class="chapter" data-level="A.6.5" data-path="further-information-on-webppl.html"><a href="further-information-on-webppl.html#usage"><i class="fa fa-check"></i><b>A.6.5</b> Usage</a></li>
<li class="chapter" data-level="A.6.6" data-path="further-information-on-webppl.html"><a href="further-information-on-webppl.html#keyboard-shortcuts-for-in-browser-use"><i class="fa fa-check"></i><b>A.6.6</b> Keyboard shortcuts (for in-browser use)</a></li>
<li class="chapter" data-level="A.6.7" data-path="further-information-on-webppl.html"><a href="further-information-on-webppl.html#further-resources"><i class="fa fa-check"></i><b>A.6.7</b> Further resources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="app-91-distributions.html"><a href="app-91-distributions.html"><i class="fa fa-check"></i><b>B</b> Common probability distributions</a><ul>
<li class="chapter" data-level="B.1" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html"><i class="fa fa-check"></i><b>B.1</b> Selected continuous distributions of random variables</a><ul>
<li class="chapter" data-level="B.1.1" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html#app-91-distributions-normal"><i class="fa fa-check"></i><b>B.1.1</b> Normal distribution</a></li>
<li class="chapter" data-level="B.1.2" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html#app-91-distributions-chi2"><i class="fa fa-check"></i><b>B.1.2</b> Chi-squared distribution</a></li>
<li class="chapter" data-level="B.1.3" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html#app-91-distributions-F"><i class="fa fa-check"></i><b>B.1.3</b> F-distribution</a></li>
<li class="chapter" data-level="B.1.4" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html#app-91-distributions-students-t"><i class="fa fa-check"></i><b>B.1.4</b> Student’s <em>t</em>-distribution</a></li>
<li class="chapter" data-level="B.1.5" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html#app-91-distributions-beta"><i class="fa fa-check"></i><b>B.1.5</b> Beta distribution</a></li>
<li class="chapter" data-level="B.1.6" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html#uniform-distribution"><i class="fa fa-check"></i><b>B.1.6</b> Uniform distribution</a></li>
<li class="chapter" data-level="B.1.7" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html#app-91-distributions-dirichlet"><i class="fa fa-check"></i><b>B.1.7</b> Dirichlet distribution</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="selected-discrete-distributions-of-random-variables.html"><a href="selected-discrete-distributions-of-random-variables.html"><i class="fa fa-check"></i><b>B.2</b> Selected discrete distributions of random variables</a><ul>
<li class="chapter" data-level="B.2.1" data-path="selected-discrete-distributions-of-random-variables.html"><a href="selected-discrete-distributions-of-random-variables.html#app-91-distributions-binomial"><i class="fa fa-check"></i><b>B.2.1</b> Binomial distribution</a></li>
<li class="chapter" data-level="B.2.2" data-path="selected-discrete-distributions-of-random-variables.html"><a href="selected-discrete-distributions-of-random-variables.html#app-91-distributions-multinomial"><i class="fa fa-check"></i><b>B.2.2</b> Multinomial distribution</a></li>
<li class="chapter" data-level="B.2.3" data-path="selected-discrete-distributions-of-random-variables.html"><a href="selected-discrete-distributions-of-random-variables.html#app-91-distributions-bernoulli"><i class="fa fa-check"></i><b>B.2.3</b> Bernoulli distribution</a></li>
<li class="chapter" data-level="B.2.4" data-path="selected-discrete-distributions-of-random-variables.html"><a href="selected-discrete-distributions-of-random-variables.html#app-91-distributions-categorical"><i class="fa fa-check"></i><b>B.2.4</b> Categorical distribution</a></li>
<li class="chapter" data-level="B.2.5" data-path="selected-discrete-distributions-of-random-variables.html"><a href="selected-discrete-distributions-of-random-variables.html#app-91-distributions-beta-binomial"><i class="fa fa-check"></i><b>B.2.5</b> Beta-Binomial distribution</a></li>
<li class="chapter" data-level="B.2.6" data-path="selected-discrete-distributions-of-random-variables.html"><a href="selected-discrete-distributions-of-random-variables.html#poisson-distribution"><i class="fa fa-check"></i><b>B.2.6</b> Poisson distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="app-92-exponential-family.html"><a href="app-92-exponential-family.html"><i class="fa fa-check"></i><b>C</b> Exponential Family and Maximum Entropy</a><ul>
<li class="chapter" data-level="C.1" data-path="an-important-family-the-exponential-family.html"><a href="an-important-family-the-exponential-family.html"><i class="fa fa-check"></i><b>C.1</b> An important family: The Exponential Family</a></li>
<li class="chapter" data-level="C.2" data-path="the-maximum-entropy-principle.html"><a href="the-maximum-entropy-principle.html"><i class="fa fa-check"></i><b>C.2</b> The Maximum Entropy Principle</a><ul>
<li class="chapter" data-level="C.2.1" data-path="the-maximum-entropy-principle.html"><a href="the-maximum-entropy-principle.html#information-entropy"><i class="fa fa-check"></i><b>C.2.1</b> Information Entropy</a></li>
<li class="chapter" data-level="C.2.2" data-path="the-maximum-entropy-principle.html"><a href="the-maximum-entropy-principle.html#deriving-probability-distributions-using-the-maximum-entropy-principle"><i class="fa fa-check"></i><b>C.2.2</b> Deriving Probability Distributions using the Maximum Entropy Principle</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="D" data-path="app-93-data-sets.html"><a href="app-93-data-sets.html"><i class="fa fa-check"></i><b>D</b> Data sets used in the book</a><ul>
<li class="chapter" data-level="D.1" data-path="app-93-data-sets-mental-chronometry.html"><a href="app-93-data-sets-mental-chronometry.html"><i class="fa fa-check"></i><b>D.1</b> Mental Chronometry</a><ul>
<li class="chapter" data-level="D.1.1" data-path="app-93-data-sets-mental-chronometry.html"><a href="app-93-data-sets-mental-chronometry.html#nature-origin-and-rationale-of-the-data"><i class="fa fa-check"></i><b>D.1.1</b> Nature, origin and rationale of the data</a></li>
<li class="chapter" data-level="D.1.2" data-path="app-93-data-sets-mental-chronometry.html"><a href="app-93-data-sets-mental-chronometry.html#loading-and-preprocessing-the-data"><i class="fa fa-check"></i><b>D.1.2</b> Loading and preprocessing the data</a></li>
<li class="chapter" data-level="D.1.3" data-path="app-93-data-sets-mental-chronometry.html"><a href="app-93-data-sets-mental-chronometry.html#cleaning-the-data-1"><i class="fa fa-check"></i><b>D.1.3</b> Cleaning the data</a></li>
<li class="chapter" data-level="D.1.4" data-path="app-93-data-sets-mental-chronometry.html"><a href="app-93-data-sets-mental-chronometry.html#exploration-summary-stats-plots"><i class="fa fa-check"></i><b>D.1.4</b> Exploration: summary stats &amp; plots</a></li>
</ul></li>
<li class="chapter" data-level="D.2" data-path="app-93-data-sets-simon-task.html"><a href="app-93-data-sets-simon-task.html"><i class="fa fa-check"></i><b>D.2</b> Simon Task</a><ul>
<li class="chapter" data-level="D.2.1" data-path="app-93-data-sets-simon-task.html"><a href="app-93-data-sets-simon-task.html#experiment"><i class="fa fa-check"></i><b>D.2.1</b> Experiment</a></li>
<li class="chapter" data-level="D.2.2" data-path="app-93-data-sets-simon-task.html"><a href="app-93-data-sets-simon-task.html#hypotheses"><i class="fa fa-check"></i><b>D.2.2</b> Hypotheses</a></li>
<li class="chapter" data-level="D.2.3" data-path="app-93-data-sets-simon-task.html"><a href="app-93-data-sets-simon-task.html#results"><i class="fa fa-check"></i><b>D.2.3</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="D.3" data-path="app-93-data-sets-king-of-france.html"><a href="app-93-data-sets-king-of-france.html"><i class="fa fa-check"></i><b>D.3</b> King of France</a><ul>
<li class="chapter" data-level="D.3.1" data-path="app-93-data-sets-king-of-france.html"><a href="app-93-data-sets-king-of-france.html#app-93-data-sets-king-of-france-background"><i class="fa fa-check"></i><b>D.3.1</b> Nature, origin and rationale of the data</a></li>
<li class="chapter" data-level="D.3.2" data-path="app-93-data-sets-king-of-france.html"><a href="app-93-data-sets-king-of-france.html#loading-and-preprocessing-the-data-1"><i class="fa fa-check"></i><b>D.3.2</b> Loading and preprocessing the data</a></li>
<li class="chapter" data-level="D.3.3" data-path="app-93-data-sets-king-of-france.html"><a href="app-93-data-sets-king-of-france.html#cleaning-the-data-3"><i class="fa fa-check"></i><b>D.3.3</b> Cleaning the data</a></li>
<li class="chapter" data-level="D.3.4" data-path="app-93-data-sets-king-of-france.html"><a href="app-93-data-sets-king-of-france.html#exploration-summary-stats-plots-1"><i class="fa fa-check"></i><b>D.3.4</b> Exploration: summary stats &amp; plots</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="app-93-data-sets-BLJM.html"><a href="app-93-data-sets-BLJM.html"><i class="fa fa-check"></i><b>D.4</b> Bio-Logic Jazz-Metal (and where to consume it)</a><ul>
<li class="chapter" data-level="D.4.1" data-path="app-93-data-sets-BLJM.html"><a href="app-93-data-sets-BLJM.html#nature-origin-and-rationale-of-the-data-1"><i class="fa fa-check"></i><b>D.4.1</b> Nature, origin and rationale of the data</a></li>
<li class="chapter" data-level="D.4.2" data-path="app-93-data-sets-BLJM.html"><a href="app-93-data-sets-BLJM.html#loading-and-preprocessing-the-data-2"><i class="fa fa-check"></i><b>D.4.2</b> Loading and preprocessing the data</a></li>
<li class="chapter" data-level="D.4.3" data-path="app-93-data-sets-BLJM.html"><a href="app-93-data-sets-BLJM.html#exploration-counts-plots"><i class="fa fa-check"></i><b>D.4.3</b> Exploration: counts &amp; plots</a></li>
</ul></li>
<li class="chapter" data-level="D.5" data-path="app-93-data-sets-avocado.html"><a href="app-93-data-sets-avocado.html"><i class="fa fa-check"></i><b>D.5</b> Avocado prices</a><ul>
<li class="chapter" data-level="D.5.1" data-path="app-93-data-sets-avocado.html"><a href="app-93-data-sets-avocado.html#nature-origin-and-rationale-of-the-data-2"><i class="fa fa-check"></i><b>D.5.1</b> Nature, origin and rationale of the data</a></li>
<li class="chapter" data-level="D.5.2" data-path="app-93-data-sets-avocado.html"><a href="app-93-data-sets-avocado.html#loading-and-preprocessing-the-data-3"><i class="fa fa-check"></i><b>D.5.2</b> Loading and preprocessing the data</a></li>
<li class="chapter" data-level="D.5.3" data-path="app-93-data-sets-avocado.html"><a href="app-93-data-sets-avocado.html#summary-statistics"><i class="fa fa-check"></i><b>D.5.3</b> Summary statistics</a></li>
<li class="chapter" data-level="D.5.4" data-path="app-93-data-sets-avocado.html"><a href="app-93-data-sets-avocado.html#plots"><i class="fa fa-check"></i><b>D.5.4</b> Plots</a></li>
</ul></li>
<li class="chapter" data-level="D.6" data-path="app-93-data-sets-temperature.html"><a href="app-93-data-sets-temperature.html"><i class="fa fa-check"></i><b>D.6</b> Annual average world surface temperature</a><ul>
<li class="chapter" data-level="D.6.1" data-path="app-93-data-sets-temperature.html"><a href="app-93-data-sets-temperature.html#nature-origin-and-rationale-of-the-data-3"><i class="fa fa-check"></i><b>D.6.1</b> Nature, origin and rationale of the data</a></li>
<li class="chapter" data-level="D.6.2" data-path="app-93-data-sets-temperature.html"><a href="app-93-data-sets-temperature.html#loading-and-preprocessing-the-data-4"><i class="fa fa-check"></i><b>D.6.2</b> Loading and preprocessing the data</a></li>
<li class="chapter" data-level="D.6.3" data-path="app-93-data-sets-temperature.html"><a href="app-93-data-sets-temperature.html#hypothesis-modeling-approach"><i class="fa fa-check"></i><b>D.6.3</b> Hypothesis &amp; modeling approach</a></li>
<li class="chapter" data-level="D.6.4" data-path="app-93-data-sets-temperature.html"><a href="app-93-data-sets-temperature.html#plotting"><i class="fa fa-check"></i><b>D.6.4</b> Plotting</a></li>
</ul></li>
<li class="chapter" data-level="D.7" data-path="app-93-data-sets-murder-data.html"><a href="app-93-data-sets-murder-data.html"><i class="fa fa-check"></i><b>D.7</b> Murder data</a><ul>
<li class="chapter" data-level="D.7.1" data-path="app-93-data-sets-murder-data.html"><a href="app-93-data-sets-murder-data.html#nature-origin-and-rationale-of-the-data-4"><i class="fa fa-check"></i><b>D.7.1</b> Nature, origin and rationale of the data</a></li>
</ul></li>
<li class="chapter" data-level="D.8" data-path="app-93-data-sets-politeness.html"><a href="app-93-data-sets-politeness.html"><i class="fa fa-check"></i><b>D.8</b> Politeness data</a><ul>
<li class="chapter" data-level="D.8.1" data-path="app-93-data-sets-politeness.html"><a href="app-93-data-sets-politeness.html#nature-origin-and-rationale-of-the-data-5"><i class="fa fa-check"></i><b>D.8.1</b> Nature, origin and rationale of the data</a></li>
<li class="chapter" data-level="D.8.2" data-path="app-93-data-sets-politeness.html"><a href="app-93-data-sets-politeness.html#hypotheses-2"><i class="fa fa-check"></i><b>D.8.2</b> Hypotheses</a></li>
<li class="chapter" data-level="D.8.3" data-path="app-93-data-sets-politeness.html"><a href="app-93-data-sets-politeness.html#summary-statistics-1"><i class="fa fa-check"></i><b>D.8.3</b> Summary statistics</a></li>
<li class="chapter" data-level="D.8.4" data-path="app-93-data-sets-politeness.html"><a href="app-93-data-sets-politeness.html#visualization-1"><i class="fa fa-check"></i><b>D.8.4</b> Visualization</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="E" data-path="app-94-open-science.html"><a href="app-94-open-science.html"><i class="fa fa-check"></i><b>E</b> Open science practices</a><ul>
<li class="chapter" data-level="E.1" data-path="app-94-replication-crisis.html"><a href="app-94-replication-crisis.html"><i class="fa fa-check"></i><b>E.1</b> Psychology’s replication crisis</a><ul>
<li class="chapter" data-level="E.1.1" data-path="app-94-replication-crisis.html"><a href="app-94-replication-crisis.html#publication-bias-qrps-and-false-positives"><i class="fa fa-check"></i><b>E.1.1</b> Publication bias, QRP’s, and false-positives</a></li>
<li class="chapter" data-level="E.1.2" data-path="app-94-replication-crisis.html"><a href="app-94-replication-crisis.html#low-statistical-power"><i class="fa fa-check"></i><b>E.1.2</b> Low statistical power</a></li>
<li class="chapter" data-level="E.1.3" data-path="app-94-replication-crisis.html"><a href="app-94-replication-crisis.html#lack-of-transparency"><i class="fa fa-check"></i><b>E.1.3</b> Lack of transparency</a></li>
</ul></li>
<li class="chapter" data-level="E.2" data-path="app-94-remedies.html"><a href="app-94-remedies.html"><i class="fa fa-check"></i><b>E.2</b> Possible remedies</a><ul>
<li class="chapter" data-level="E.2.1" data-path="app-94-remedies.html"><a href="app-94-remedies.html#improve-scientific-rigor"><i class="fa fa-check"></i><b>E.2.1</b> Improve scientific rigor</a></li>
<li class="chapter" data-level="E.2.2" data-path="app-94-remedies.html"><a href="app-94-remedies.html#realigning-incentive-structures"><i class="fa fa-check"></i><b>E.2.2</b> Realigning incentive structures</a></li>
<li class="chapter" data-level="E.2.3" data-path="app-94-remedies.html"><a href="app-94-remedies.html#promote-transparency"><i class="fa fa-check"></i><b>E.2.3</b> Promote transparency</a></li>
</ul></li>
<li class="chapter" data-level="E.3" data-path="app-94-recap.html"><a href="app-94-recap.html"><i class="fa fa-check"></i><b>E.3</b> Chapter summary</a></li>
<li class="chapter" data-level="E.4" data-path="app-94-resources.html"><a href="app-94-resources.html"><i class="fa fa-check"></i><b>E.4</b> Further resources</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="comparison-of-approaches" class="section level2">
<h2><span class="header-section-number">12.4</span> Comparison of approaches</h2>
<p>We saw three conceptually different approaches to linear regression: (i) based on ordinary least squares, (ii) based on likelihood alone and (iii) based on Bayesian inference (with non-informative priors).
At the core of linear regression lies the linear predictor <span class="math inline">\(\xi = X \beta\)</span>, which all three approaches used.
The three approaches differ in how they determine the regression coefficients <span class="math inline">\(\beta\)</span> that feed this linear predictor.
Another crucial difference is in how these different approaches make predictions about new data observations <span class="math inline">\(y_\text{new}\)</span> given some (hypothetical or actually observed) vector of predictor variables <span class="math inline">\(x_\text{new}\)</span>.
Let’s go through these differences with some more eye for detail.</p>
<p>The OLS-based approach determined coefficients based on a geometric notion of distance in terms of <em>squared loss</em>.
The prediction of an OLS regression model for a new data set’s dependent variables would just be:<a href="#fn56" class="footnote-ref" id="fnref56"><sup>56</sup></a></p>
<p><span class="math display">\[y_\text{new} = \hat\xi = X_\text{new} \hat \beta\]</span></p>
<p>In words, The OLS-model predicts that <span class="math inline">\(y_\text{new}\)</span> is given as a point on the best predictor linear regression line.
This is a deterministic, very clear-cut point-valued prediction and almost certainly always false.
The OLS approach, insofar as we have seen it, does not contain a measure of spread around this best predictor.</p>
<p>The MLE-based approach uses a normal distribution to also quantify the likely spread of observations around the best predictor line.
The (posterior) predictions of a trained MLE-based regression model are probabilistic.
They are samples from a normal distribution whose central tendency is the best linear predictor:</p>
<p><span class="math display">\[
\begin{align*}
\hat \xi  &amp; = X_\text{new} \hat \beta \\
y_\text{new} &amp; \sim \text{Normal}(\hat \xi, \hat \sigma)
\end{align*}
\]</span></p>
<p>Finally, the Bayesian is even more stochastic, so to speak, than the MLE-based approach.
The Bayesian approach does not assume a single best linear predictor vector <span class="math inline">\(\hat{\xi}\)</span> for its (posterior) predictions, but rather gives us a probability distribution over linear predictors.
In vague terms, we could say that Bayesian regression gives us, not a single regression line, but a weighted cloud of (usually: infinitely many) regression lines.
A schematic representation of the posterior predictive for the new data point <span class="math inline">\(y_\text{new}\)</span> given <span class="math inline">\(x_\text{new}\)</span> in Bayesian regression is:</p>
<p><span class="math display">\[
\begin{align*}
\beta_\text{sample}, \sigma_\text{sample} &amp; \sim \text{Bayesian posterior given data} \\
\xi_\text{sample}  &amp; = X_\text{new} \beta_\text{sample} \\ 
y_\text{new} &amp; \sim \text{Normal}(\xi_\text{sample}, \sigma_\text{sample})
\end{align*}
\]</span></p>
<!-- ### Implementation in `Stan` -->
<!-- Here is an implementation of a Bayesian regression model for the running example murder data using `Stan`: -->
<!-- ```{r, echo = F, eval = F} -->
<!-- # data to be explained / predicted -->
<!-- y <- murder_data %>% pull(murder_rate) -->
<!-- # data to use for prediction / explanation -->
<!-- x <- murder_data %>% pull(unemployment) -->
<!-- y_greta     <- as_data(y) -->
<!-- x_greta     <- as_data(x) -->
<!-- # latent variables and priors -->
<!-- intercept <- student(df= 1, mu = 0, sigma = 10) -->
<!-- slope     <- student(df= 1, mu = 0, sigma = 10) -->
<!-- sigma     <- normal(0, 5, truncation = c(0, Inf)) -->
<!-- # derived latent variable (linear model) -->
<!-- y_pred <- intercept + slope * x_greta -->
<!-- # likelihood  -->
<!-- distribution(y) <- normal(y_pred, sigma) -->
<!-- # finalize model, register which parameters to monitor -->
<!-- murder_model <- model(intercept, slope, sigma) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- murder_data_4_Stan <- list( -->
<!--   N = murder_data %>% nrow(), -->
<!--   x = murder_data %>% pull(unemployment), -->
<!--   y = murder_data %>% pull(murder_rate) -->
<!-- ) -->
<!-- ``` -->
<!-- ```{mystan, eval = F} -->
<!-- data { -->
<!-- int<lower=1> N ; -->
<!-- vector[N] x ; -->
<!-- vector[N] y ; -->
<!-- } -->
<!-- parameters { -->
<!-- real intercept ; -->
<!-- real slope ; -->
<!-- real<lower=0> sigma ; -->
<!-- }  -->
<!-- model { -->
<!-- # priors -->
<!-- intercept ~ student_t(1, 0, 10) ; -->
<!-- slope ~ student_t(1, 0, 10) ; -->
<!-- sigma ~ normal(0, 5) ; -->
<!-- # likelihood -->
<!-- y ~ normal(intercept + slope * x, sigma) ; -->
<!-- } -->
<!-- ``` -->
<!-- <link rel="stylesheet" href="hljs.css"> -->
<!-- <script src="stan.js"></script> -->
<!-- <script>$('pre.mystan code').each(function(i, block) {hljs.highlightBlock(block);});</script> -->
<!-- We can draw samples from the posterior distribution as usual: -->
<!-- ```{r, echo = F, eval = F} -->
<!-- # draw samples -->
<!-- draws_murder_data <- greta::mcmc( -->
<!--   murder_model,  -->
<!--   n_samples = 2000,  -->
<!--   chains = 4,  -->
<!--   warmup = 1000 -->
<!-- ) -->
<!-- # cast results (type 'mcmc.list') into tidy tibble -->
<!-- tidy_draws_murder_data <- ggmcmc::ggs(draws_murder_data) -->
<!-- ``` -->
<!-- ```{r echo = F, eval = F} -->
<!-- draws_murder_data <- readRDS('models_greta/linear_regression_simple_murder_draws.rds') -->
<!-- tidy_draws_murder_data <- ggmcmc::ggs(draws_murder_data) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- stan_fit_linear_regression <- rstan::stan( -->
<!--   # where is the Stan code -->
<!--   file = 'models_stan/simple_linear_regression_model.stan', -->
<!--   # data to supply to the Stan program -->
<!--   data = murder_data_4_Stan, -->
<!--   # how many iterations of MCMC -->
<!--   iter = 3000, -->
<!--   # how many warmup steps -->
<!--   warmup = 500 -->
<!-- ) -->
<!-- # cast results (type 'mcmc.list') into tidy tibble -->
<!-- tidy_draws_murder_data = ggmcmc::ggs(stan_fit_linear_regression) -->
<!-- ``` -->
<!-- Here is a plot of the posterior: -->
<!-- ```{r} -->
<!-- # plot posterior -->
<!-- tidy_draws_murder_data %>%  -->
<!--   ggplot(aes(x = value)) + -->
<!--   geom_density(fill = "lightgray", alpha = 0.5) + -->
<!--   facet_wrap(~ Parameter, scales = "free") -->
<!-- ``` -->
<!-- ## Snippets -->
<!-- ### Preliminaries -->
<!-- <div class = "grey"> -->
<!-- - *data* for simple linear regression: -->
<!--     - single dependent variable $y$ with observed data points $i: y_1,..., y_n$ -->
<!--     - $k$ predictor variables $x_1, ..., x_k$ and observations $x_{j1}, ..., x_{jn}$ for each $x_j$ -->
<!-- - we consider a *regression model*: -->
<!--     - $y_i \sim Normal(\beta_0 + \beta_1 x_{1i} + ... + \beta_k x_{ki}, \sigma)$ -->
<!--     - $\beta_j$ and $\sigma$ are free parameters, as usual -->
<!-- - further notation: -->
<!--     - $\bar{y} = \frac{1}{n} \sum_{i=1}^n y_i$    -   the mean of data observations  -->
<!--     - $\hat{y}_i = \hat{\beta_0} + \hat{\beta_1} x_1i + ... + \hat{\beta_k} x_{ki}$ - prediction for i-th data point for best fitting parameter values -->
<!-- </div> -->
<!-- --- -->
<!-- ### Definitions -->
<!-- <div class = "grey"> -->
<!-- **Total sum of squares:**  $$TSS = \sum_{i=1}^n (y_i - \bar{y})^2 \tag{0.1}$$ -->
<!-- **Explained sum of squares:**  $$ESS = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2\tag{0.2}$$ -->
<!-- **Residual sum of squares:**  $$RSS = \sum_{i=1}^n (y_i - \hat{y}_i)^2\tag{0.3}$$ -->
<!-- **Likelihood:**  $$LH = \prod_{i=1}^n Normal(\beta_0 + \beta_1 x_{1i} + ... + \beta_k x_{ki}, \sigma)\tag{0.4}$$ -->
<!-- </div> -->
<!-- --- -->
<!-- ### Theorem 1 -->
<!-- #### Theorem 1a - special case: one predictor ($k=1$) -->
<!-- <div class = "blue"> -->
<!-- **Closed-form solution for parameter fit under ordinary least squares loss function (special case where $k=1$).** -->
<!-- *If there is only one predictor variable $(k=1)$, the closed-form solution of predictors in the model that minimize RSS (Residual Sum of Squares) is:* -->
<!-- $$\begin{align} -->
<!-- \hat{\beta_0} &= \bar{y} - \hat{\beta}_1 \bar{x}\textrm{, and}\\ -->
<!-- \\ -->
<!-- \hat{\beta_1} &= \frac{Cov(x,y)}{Var(x)}. -->
<!-- \end{align}$$ -->
<!-- </div> -->
<!-- ##### Proof -->
<!-- *[See e.g., @kirchner2003, pp. 1-3; @olive2017, pp. 57-59]* -->
<!-- Given a set of $n$ observations $(X_i,Y_i)$ (or points on a scatter plot), we want to find the best-fit line,  -->
<!-- $$\hat y_i=\hat\beta_0+\hat\beta_1x_i,\tag{1.1.1}$$ -->
<!-- such that the sum of squared errors (RSS) in $Y$ is minimized: -->
<!-- $$RSS=\sum_{i=1}^n (y_i - \hat{y}_i)^2 \rightarrow min.\tag{1.1.2}$$ -->
<!-- Let the *Residual Sum of Squares (RSS)*  be denoted as $Q$ with, -->
<!-- $$\begin{align} -->
<!-- Q=RSS&=\sum_{i=1}^{n}(y_i-\hat y_i)^2\\ &=\sum_{i=1}^{n}(y_i-\hat\beta_0-\hat\beta_1x_i)^2. -->
<!-- \tag{1.1.3} -->
<!-- \end{align}$$ -->
<!-- We want to minimize $Q$ (that is minimizing *RSS*) at the values of $\hat\beta_0$ and $\hat\beta_1$ for which $\frac{\partial Q}{\partial \hat\beta_0}=0$ (1) and $\frac{\partial Q}{\partial \hat\beta_1}=0$ (2). -->
<!-- The first condition (1) is, -->
<!-- $$ \begin{align} \frac{\partial Q}{\partial \hat\beta_0}=\sum_{i=1}^{n}-2(y_i-\hat\beta_0-\hat\beta_1x_i)&= 0\\ -->
<!-- &=-\sum_{i=1}^ny_i+\sum_{i=1}^n\hat \beta_0+\sum_{i=1}^n\hat\beta_1x_i\\ -->
<!-- &=-\sum_{i=1}^ny_i+n\hat\beta_0+\sum_{i=1}^n\hat\beta_1x_i -->
<!-- \tag{1.1.4} -->
<!-- \end{align}$$ -->
<!-- which, if we solve for $\hat\beta_0$, becomes -->
<!-- $$\begin{align} -->
<!-- \hat\beta_0&=\frac{1}{n}\sum_{i=1}^{n}y_i-\frac{1}{n}\hat\beta_1\sum_{i=1}^{n}x_i\\ -->
<!-- &=\bar y - \hat\beta_1\bar x, -->
<!-- \tag{1.1.5} -->
<!-- \end{align}$$ -->
<!-- which says that the constant $\hat\beta_0$ (the y-intercept) is set such that the line must go through the mean of $x$ and $y$. This makes sense because this point is the "center" of the data cloud. -->
<!-- The solution is indeed a minimum as the second partial derivative is positive: -->
<!-- $\frac{\partial^2 Q}{\partial\hat\beta_0^2}=2n>0. \tag{1.1.6}$ -->
<!-- The second condition (2) is, -->
<!-- $$ \begin{align} -->
<!-- \frac{\partial Q}{\partial \hat\beta_1}=\sum_{i=1}^{n}-2x_i(y_i-\hat\beta_0-\hat\beta_1x_i)&=0\\ -->
<!-- &=\sum_{i=1}^{n}(-x_iy_i+\hat\beta_0x_i+\hat\beta_1x_i^2)\\ -->
<!-- &=-\sum_{i=1}^{n}x_iy_i+\hat\beta_0\sum_{i=1}^{n}x_i+\hat\beta_1\sum_{i=1}^{n}x_i^2 -->
<!-- \tag{1.1.7} -->
<!-- \end{align}$$ -->
<!-- If we substitute the expression by (1.1.5), we get, -->
<!-- $$ \begin{align} -->
<!-- 0&=-\sum_{i=1}^{n}x_iy_i+(\bar y - \hat\beta_1\bar x)\sum_{i=1}^{n}x_i+\hat\beta_1\sum_{i=1}^{n}x_i^2\\ -->
<!-- &=-\sum_{i=1}^{n}x_iy_i+\bar y\sum_{i=1}^{n}x_i-\hat\beta_1\bar x\sum_{i=1}^{n}x_i+\hat\beta_1\sum_{i=1}^{n}x_i^2 -->
<!-- \tag{1.1.8} -->
<!-- \end{align}$$ -->
<!-- separating this into two sums, -->
<!-- $$ \sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)-\hat\beta_1\sum_{i=1}^{n}\left(x_i^2-x_i\bar x\right)=0 \tag{1.1.9}$$ -->
<!-- becomes, -->
<!-- $$ \hat\beta_1 = \frac{\sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)}{\sum_{i=1}^{n}\left( x_i^2-x_i\bar x\right)} = \frac{\sum_{i=1}^{n}\left( x_iy_i\right)-n\bar x\bar y}{\sum_{i=1}^{n}\left( x_i^2\right)-n \bar x^2} \tag{1.1.10}$$ -->
<!-- The model assumes that the deviation from the values from the mean is zero, so that the positive and negative values are in balance, thus -->
<!-- $$ \sum_{i=1}^{n}\left( \bar x^2-x_i\bar x\right)=0, \tag{1.1.11}$$ -->
<!-- and -->
<!-- $$ \sum_{i=1}^{n}\left(\bar x \bar y - y_i \bar x\right)=0. \tag{1.1.12}$$ -->
<!-- This can be used in order to expand the previous term and finally to rewrite $\hat\beta_1$ as the ratio of $Cov(x,y)$ to $Var(x)$: -->
<!-- $$ -->
<!-- \begin{align} -->
<!-- \hat\beta_1&=\frac{\sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)+\sum_{i=1}^{n}\left(\bar x\bar y - y_i \bar x\right)}{\sum_{i=1}^{n}\left( x_i^2-x_i\bar x\right)+\sum_{i=1}^{n}\left( \bar x^2-x_i\bar x\right)}=\frac{\sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)+0}{\sum_{i=1}^{n}\left( x_i^2-x_i\bar x\right)+0}\\ -->
<!-- \\ -->
<!-- &=\frac{\frac{1}{n}\sum_{i=1}^{n}\left( x_i-\bar x\right) \left(y_i- \bar y \right)}{\frac{1}{n}\sum_{i=1}^{n}\left( x_i-\bar x\right)^2}\\ -->
<!-- \\ -->
<!-- &=\frac{Cov(x,y)}{Var(x)}. -->
<!-- \tag{1.1.13} -->
<!-- \end{align}$$ -->
<!-- The solution is indeed a minimum as the second partial derivative is positive: -->
<!-- $$\frac{\partial^2Q}{\partial \hat\beta_1^2}= 2 \sum_{i=1}^{n}x_i^2 >0. \tag{1.1.14}$$ -->
<!-- #### Theorem 1b: Generalization of Theorem 1a to $k >=1$ -->
<!-- *[see e.g., @bremer2012, pp. 21-23; @gonzalez2014, pp. 5-15]* -->
<!-- The model of multiple linear regression is given by the following expression: -->
<!-- $$y=\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_kx_k+\epsilon \tag{1.2.1}$$ -->
<!-- Suppose we have $n$ observations, then we can write: -->
<!-- $$\begin{align} -->
<!-- y_1&=\beta_0+\beta_{1}x_{11}+\beta_2x_{21}+...+\beta_kx_{k1}+\epsilon_1\\ -->
<!-- y_2&=\beta_0+\beta_{1}x_{12}+\beta_2x_{22}+...+\beta_kx_{k2}+\epsilon_2\\ -->
<!-- ...\\ -->
<!-- y_n&=\beta_0+\beta_{1}x_{1n}+\beta_2x_{2n}+...+\beta_kx_{kn}+\epsilon_n -->
<!-- \tag{1.2.2} -->
<!-- \end{align}$$ -->
<!-- The model of multiple linear regression is often expressed in matrix notation: -->
<!-- $$\begin{bmatrix} y_1\\y_2\\...\\y_n \end{bmatrix}= \begin{bmatrix}1&x_{11}& x_{21}&...&x_{k1}\\1&x_{12}& x_{22}&...&x_{k2}\\...& ...&...&...&...\\1&x_{1n}&x_{2n}&...&x_{kn}\end{bmatrix}\begin{bmatrix}\beta_0\\\beta_1\\...\\\beta_n \end{bmatrix}+\begin{bmatrix}\epsilon_1\\\epsilon_2\\...\\\epsilon_n \end{bmatrix} \tag{1.2.3}$$ -->
<!-- Which can be expressed in a compact form as -->
<!-- $$\mathbf{Y=X\beta+\epsilon} \tag{1.2.4}$$ -->
<!-- where $y$ is a vector $n\times 1$, $X$ is a matrix $n \times k$, $\beta$ is a vector $k \times 1$ and $\epsilon$ is a vector $n \times 1$. -->
<!-- The OLS estimator is obtained (like in the special case) by minimizing the residual sum of squares (RSS).  -->
<!-- $$RSS \rightarrow min.$$  -->
<!-- The RSS for the multiple linear regression model is -->
<!-- $$Q=RSS=\sum_{i=1}^n \hat\epsilon_i^2=\sum_{i=1}^n (y_i - \hat{y}_i)^2=\sum_{i=1}^n \left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]^2 \tag{1.2.5}$$ -->
<!-- to apply the least-squares criterion in the model of multiple linear regression, thus to minimize $RSS$, we calculate the first partial derivative from $Q$ with respect to each $\hat\beta_j$in the expression: -->
<!-- $$\begin{align} -->
<!-- \frac{\partial Q}{\partial\hat\beta_0}&=2\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right][-1]\\ -->
<!-- \\ -->
<!-- \frac{\partial Q}{\partial\hat\beta_1}&=2\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right][-x_{1i}]\\ -->
<!-- \\ -->
<!-- \frac{\partial Q}{\partial\hat\beta_2}&=2\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right][-x_{2i}]\\ -->
<!-- ...\\ -->
<!-- \frac{\partial Q}{\partial\hat\beta_k}&=2\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right][-x_{ki}] -->
<!-- \tag{1.2.6} -->
<!-- \end{align}$$ -->
<!-- Then the derivative of each equation is set to zero: -->
<!-- $$\begin{align} -->
<!-- &\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]=0\\ -->
<!-- &\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]x_{1i}=0\\ -->
<!-- &\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]x_{2i}=0\\ -->
<!-- &...\\ -->
<!-- &\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]x_{ki}=0 -->
<!-- \tag{1.2.7} -->
<!-- \end{align}$$ -->
<!-- Alternatively, we can use matrix notation and combine the above equations into the following form: -->
<!-- $$\mathbf{X'Y-X'X\hat\beta=0}.\tag{1.2.8}$$ -->
<!-- Whereby the following expression is known as **normal equations**: -->
<!-- $$\mathbf{X'X\hat\beta=X'Y}.\tag{1.2.9}$$ -->
<!-- The system of normal equations in expanded matrix notation is: -->
<!-- $$\begin{bmatrix} n&\sum_{i=1}^nx_{1i}&...&\sum_{i=1}^nx_{ki}\\ -->
<!-- \sum_{i=1}^nx_{1i}&\sum_{i=1}^nx_{1i}^2&...&\sum_{i=1}^nx_{1i}x_{ki}\\...&...&...&...\\ -->
<!-- \sum_{i=1}^nx_{ki}&\sum_{i=1}^nx_{ki}x_{1i}&...&\sum_{i=1}^nx_{ki}^2\end{bmatrix}\begin{bmatrix}\hat\beta_0\\\hat\beta_1\\...\\\hat\beta_k\end{bmatrix}=\begin{bmatrix}\sum_{i=1}^ny_i\\\sum_{i=1}^nx_{1i}y_i\\...\\\sum_{i=1}^nx_{ki}y_i  -->
<!-- \tag{1.2.10} -->
<!-- \end{bmatrix}$$ -->
<!-- In order to obtain the estimator $\hat\beta$, we have to rearrange (1.2.10) and get the solution: -->
<!-- $$\begin{bmatrix}\hat\beta_0\\\hat\beta_1\\...\\\hat\beta_k\end{bmatrix}=\mathbf{\hat\beta}=[\mathbf{X'X}]^{-1}\mathbf{X'Y}\tag{1.2.11}$$ -->
<!-- Where $\hat\beta$ is a global minimizer of the OLS criterion as the second order condition is always a semidefinite positive matrix. -->
<!-- $$\frac{\partial^2 Q}{\partial \mathbf{\hat\beta}^2}=2X'X >0.$$ -->
<!-- --- -->
<!-- ### Theorem 2 -->
<!-- <div class = "blue"> -->
<!-- **Best fit under OLS is equivalent with best fit under MLE** -->
<!-- *The parameters $\beta_0, ..., \beta_k$ minimize the residual sum of squares (RSS) iff they maximize the (log-)likelihood (LH).* -->
<!-- </div> -->
<!-- ##### Proof -->
<!-- *[see e.g., @naveen2019; @croot2010; @eppes2019]* -->
<!-- #### Maximum Likelihood Estimation -->
<!-- We consider again the linear regression model of the population with: -->
<!-- $$Y= \beta_0 + \beta_1X + \epsilon.\tag{2.1}$$ -->
<!-- This simplifies to the following form on the observed data: -->
<!-- $$y= \beta_0 + \beta_1x + \epsilon.\tag{2.2}$$ -->
<!-- Using a sample in order to obtain the **maximum likelihood estimates** the equation simplifies to: -->
<!-- $$y= \hat\beta_0 + \hat\beta_1x. \tag{2.3}$$ -->
<!-- **Assumptions** that we make for the model: -->
<!-- - True underlying distribution of the errors is Gaussian, -->
<!-- - Expected value of the error term is 0, -->
<!-- - Variance of the error term is constant with respect to x, and -->
<!-- - the 'lagged' errors are independent of each other -->
<!-- where the error term is normally distributed. -->
<!-- We can write: -->
<!-- $$ \epsilon \sim N(0,\sigma^2).\tag{2.4}$$ -->
<!-- Since $Y$ is a linear function of $\epsilon$ it will also be normally distributed. -->
<!-- $$f(\epsilon|\beta_0,\beta_1)= \frac{1}{\sqrt{2\pi} \sigma} \exp\left[{-\frac{1}{2}\left(\frac{\epsilon^2}{\sigma^2}\right)}\right]. \tag{2.5}$$  -->
<!-- Given the whole data set with $i=1,...,n$ observations the **likelihood function** ($LH$) is the joint density of all the observations, given a value for the parameters $\beta_0$ and $\beta_1$. Since independence is assumed, this is simply the product of the individual densities from the previous equation.  -->
<!-- $$LH(\epsilon_i|\beta_0,\beta_1) =\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi} \sigma} \exp\left[{-\frac{1}{2}\left(\frac{\epsilon_i^2}{\sigma^2}\right)}\right].\tag{2.6}$$ -->
<!-- In order to find the maximum of (2.6) we have to find the first derivative. -->
<!-- But as the derivation of a product with a lot of factors is inconvenient, we take first the logarithm of the likelihood, called **log-likelihhod**. This is possible as $\log$ is a monotone transformation and the maximum likelihood estimate does not change on log transformation. -->
<!-- The log-likelihood is the sum of the logs of the individual densities: -->
<!-- $$\begin{align} -->
<!-- LL&=\log \left(LH(\epsilon_i|\beta_0,\beta_1)\right)\\ -->
<!-- &=-\left( \frac{n}{2}\right) \log(2\pi)-\left( \frac{n}{2}\right) \log(\sigma^2)-\left( \frac{1}{2}\sigma^2\right) \sum_{i=1}^n(\epsilon_i)^2 -->
<!-- \tag{2.7} -->
<!-- \end{align}$$ -->
<!-- The log-likelihood can be used in order to find the *Maximum Likelihood estimates* $\hat\beta_0$ and $\hat\beta_1$ for the parameters $\beta_0$ and $\beta_1$. -->
<!-- $$\hat\beta_0,\hat\beta_1=\mathrm{argmax}_{\beta_0,\beta_1}LL=\mathrm{argmax}_{\beta_0,\beta_1}\left[-\left( \frac{n}{2}\right) \log(2\pi)-\left( \frac{n}{2}\right) \log(\sigma^2)-\left( \frac{1}{2}\sigma^2\right) \sum_{i=1}^n(\epsilon_i)^2\right].\tag{2.8}$$ -->
<!-- Removing the constant terms results in: -->
<!-- $$\hat\beta_0,\hat\beta_1=\mathrm{argmax}_{\beta_0,\beta_1} \sum_{i=1}^n-\epsilon_i^2.\tag{2.9}$$ -->
<!-- Substituting $\epsilon$, derived from (2.2), gives: -->
<!-- $$\hat\beta_0,\hat\beta_1=\mathrm{argmax}_{\beta_0,\beta_1} \sum_{i=1}^n-(y-\beta_0-\beta_1x)^2.\tag{2.10}$$ -->
<!-- **Conclusion:** -->
<!-- Deriving parameter estimates according to the OLS method: -->
<!-- $$Q_{OLS}=\sum_{i=1}^{n}(y_i-\hat\beta_0-\hat\beta_1x_i)^2 \rightarrow min.$$ -->
<!-- Deriving parameter estimates according to the ML method: -->
<!-- $$Q_{ML}=\sum_{i=1}^n-(y_i-\hat\beta_0-\hat\beta_1x_i)^2 \rightarrow max.$$ -->
<!-- Maximizing $-z$ is equivalent to minimizing $z$, thus, the best parameter fit under ML is equivalent to the best fit under OLS. -->
<!-- --- -->
<!-- ### Theorem 3 -->
<!-- <div class = "blue"> -->
<!-- **The variance in a regression model can be decompossed by using the notion of sum of squares:** -->
<!-- *The following decomposition holds:* -->
<!-- $$\begin{align} -->
<!-- TSS &= ESS + RSS \textrm{, or}\\ -->
<!-- \sum_{i=1}^n (y_i - \bar{y})^2&=\sum_{i=1}^n (\hat{y}_i - \bar{y})^2+ \sum_{i=1}^n \hat u_i^2  -->
<!-- \end{align}$$ -->
<!-- </div> -->
<!-- with  -->
<!-- $$\hat u_i =y_i-\hat y_i $$ -->
<!-- which can be rewritten as -->
<!-- $$y_i=\hat y_i + \hat u_i \tag{3.1}$$ -->
<!-- #### Proof -->
<!-- [see e.g., @gonzalez2014 pp. 29-31] -->
<!-- $$\begin{align} -->
<!-- TSS=\sum_{i=1}^n (y_i - \bar{y})^2&=\sum_{i=1}^n\left( \hat y_i+\hat u_i - \bar y \right)^2\\ -->
<!-- &= \sum_{i=1}^n \left[(\hat y_i-\bar y)+\hat u_i\right]^2\\ -->
<!-- &= \sum_{i=1}^n (\hat y_i-\bar y)^2 + \sum_{i=1}^n \hat u_i^2+ 2\sum_{i=1}^n(\hat y_i-\bar y)\hat u_i\\ -->
<!-- &= \sum_{i=1}^n (\hat y_i-\bar y)^2 + \sum_{i=1}^n \hat u_i^2\\ -->
<!-- \tag{3.2} -->
<!-- \end{align}$$ -->
<!-- Or, using (3.1) we can write: -->
<!-- $$ -->
<!-- TSS=\sum_{i=1}^n (\hat{y}_i - \bar{y})^2+\sum_{i=1}^n (y_i - \hat{y}_i)^2=ESS+RSS. -->
<!-- \tag{3.3} -->
<!-- $$ -->
<!-- --- -->
<!-- ### Theorem 4 -->
<!-- <div class = "blue"> -->
<!-- **Expressing variance explained $R^2$ in terms of sum of squares** -->
<!-- *The following relationship holds:*  -->
<!--   $$R^2 = 1 - \frac{RSS}{TSS} = \frac{ESS}{TSS}$$ -->
<!-- </div> -->
<!-- #### Proof -->
<!-- [see e.g., @urban2018 pp. 56-72, 101-103 ] -->
<!-- We will first derive the variance explained $R^2$ considering a bivariate model (simple linear regression), afterward some note to the generalized case of multiple regression is made. -->
<!-- The variance explained can be derived in the bivariate model from the regression coefficient $\beta$ and the standard deviation of $X$ and $Y$, whereby the following holds: -->
<!-- $$R^2=\left( \beta \cdot \frac{S_x}{S_y}\right)^2=\beta^2 \cdot \frac{Var(X)}{Var(Y)}.\tag{4.1}$$ -->
<!-- We will show that (4.1) holds in the next steps. -->
<!-- Consider on the one hand the calculation of the correlation coefficient: -->
<!-- $$\begin{align} -->
<!-- r_{xy}&=\frac{cov(X,Y)}{S_x S_y}\\ -->
<!-- \\ -->
<!-- &=\frac{\frac{1}{n}\sum_{i=1}^n(X_i-\bar X)(Y_i-\bar Y)}{\sqrt{\frac{\sum_{i=1}^n(X_i-\bar X)^2}{n}} \sqrt{\frac{\sum_{i=1}^n(Y_i-\bar Y)^2}{n}}} -->
<!-- \tag{4.2} -->
<!-- \end{align}$$ -->
<!-- On the other hand, if we assume $Y$ as dependent and $X$ as an independent variable, we can write: -->
<!-- $$\beta_{yx}=\frac{\sum_{i=1}^{n}X_iY_i}{\sum_{i=1}^n X_i^2}. \tag{4.3}$$ -->
<!-- Or when $Y$ is the independent and $X$ the dependent variable, then: -->
<!-- $$\beta_{xy}=\frac{\sum_{i=1}^{n}Y_iX_i}{\sum_{i=1}^n Y_i^2}.\tag{4.4}$$ -->
<!-- Through dividing equations (4.3) and (4.4) by the number of observations, we get the covariances and variances of $X$ and $Y$: -->
<!-- $$\begin{align} -->
<!-- &\frac{1}{n}\sum_{i=1}^{n}X_iY_i=\frac{1}{n}(X_i-\bar X)(Y_i-\bar Y) = cov(X,Y)\tag{4.5}\\ -->
<!-- &\frac{1}{n}\sum_{i=1}^{n}Y_iX_i=\frac{1}{n}(Y_i-\bar Y)(X_i-\bar X) = cov(Y,X)\tag{4.6}\\ -->
<!-- &\frac{1}{n}\sum_{i=1}^{n}X_i^2=\frac{1}{n}\sum_{i=1}^{n}(X_i-\bar X)^2=S_x^2\tag{4.7}\\ -->
<!-- &\frac{1}{n}\sum_{i=1}^{n}Y_i^2=\frac{1}{n}\sum_{i=1}^{n}(Y_i-\bar Y)^2=S_y^2 -->
<!-- \tag{4.8} -->
<!-- \end{align}$$ -->
<!-- Substituting these results in equations (4.3) and (4.4), we get: -->
<!-- $$\beta_{yx}=\frac{cov(X,Y)}{S_x^2} \textrm{ , and } -->
<!-- \beta_{xy}=\frac{cov(Y,X)}{S_y^2}\tag{4.9}$$ -->
<!-- For calculating the *average* between $\beta_{yx}$ and $\beta_{xy}$ we have to use the *geometric mean*: -->
<!-- $$\bar\beta_{geom}=\sqrt{\frac{cov(X,Y)}{S_x^2}\frac{cov(Y,X)}{S_y^2}}=\frac{cov(X,Y)}{S_x S_y}=r_{xy}\tag{4.10}$$ -->
<!-- Thus, the geometric mean of $\beta_{yx}$ and $\beta_{xy}$ is identical to the correlation coefficient.  -->
<!-- Comparing equations (4.9) and (4.10), -->
<!-- $$\begin{align} -->
<!-- \beta_{yx}&=\frac{cov(X,Y)}{S_x^2}\textrm{ , and}\\ -->
<!-- \\ -->
<!-- r_{yx}&=\frac{cov(X,Y)}{S_x S_y}, -->
<!-- \end{align}$$ -->
<!-- shows that, we can convert $r_{yx}$ in $\beta_{yx}$: -->
<!-- $$\frac{cov(X,Y)}{S_x S_y} \cdot \frac{S_y}{S_x}=\frac{cov(X,Y)}{S_x^2}=\beta_{yx}.\tag{4.11}$$ -->
<!-- Therefore, the relationship stated in (4.1) holds: -->
<!-- $$\beta_{yx}=r_{yx}\cdot \frac{S_y}{S_x}.$$ -->
<!-- Using this definition we can now derive the **variance explained** $R^2$. -->
<!-- We have seen that the following holds: -->
<!-- $$\beta_{yx}=r_{yx}\cdot \frac{S_y}{S_x}=r_{yx}\sqrt{\frac{\frac{1}{n}\sum_{i=1}^{n}(Y_i-\bar Y)^2}{\frac{1}{n}\sum_{i=1}^{n}(X_i-\bar X)^2}}.$$ -->
<!-- Eliminating the number of observations and squaring the equation gives: -->
<!-- $$\beta_{yx}^2=r_{yx}^2\frac{\sum_{i=1}^{n}(Y_i-\bar Y)^2}{\sum_{i=1}^{n}(X_i-\bar X)^2}.\tag{4.12}$$ -->
<!-- The numerator $\sum_{i=1}^{n}(Y_i-\bar Y)^2$ is the total sum of squares (TSS), thus, from (0.1) follows: -->
<!-- $$\sum_{i=1}^{n}(Y_i-\bar Y)^2=\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2+\sum_{i=1}^{n}(Y_i-\hat Y_i)^2.\tag{4.13}$$ -->
<!-- The explained sum of squares (ESS) $\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2$ are statistically explained by the determination of the regression line: -->
<!-- $$ESS=\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2=\beta X_i=\beta(X_i-\bar X).\tag{4.14}$$ -->
<!-- Substituting the result of (4.14) in equation (4.13) gives: -->
<!-- $$\sum_{i=1}^{n}(Y_i-\bar Y)^2=\beta^2\sum_{i=1}^{n}(X_i-\bar X)^2+\sum_{i=1}^{n}(Y_i-\hat Y_i)^2.\tag{4.15}$$ -->
<!-- As we know from (4.12): -->
<!-- $$\beta_{yx}^2=r_{yx}^2\frac{\sum_{i=1}^{n}(Y_i-\bar Y)^2}{\sum_{i=1}^{n}(X_i-\bar X)^2},$$ -->
<!-- we can substitute $\beta$ in equation (4.15) with the result from (4.12) and get: -->
<!-- $$\begin{align} -->
<!-- \sum_{i=1}^{n}(Y_i-\bar Y)^2&=r_{xy}^2\frac{\sum_{i=1}^{n}(Y_i-\bar Y)^2}{\sum_{i=1}^{n}(X_i-\bar X)^2}\sum_{i=1}^{n}(X_i-\bar X)^2+\sum_{i=1}^{n}(Y_i-\hat Y_i)^2\\ -->
<!-- \\ -->
<!-- &=r_{xy}^2 \sum_{i=1}^{n}(Y_i-\bar Y)^2+\sum_{i=1}^{n}(Y_i-\hat Y_i)^2\tag{4.16}\\ -->
<!-- \\ -->
<!-- \frac{\sum_{i=1}^{n}(Y_i-\bar Y)^2-\sum_{i=1}^{n}(Y_i-\hat Y_i)^2}{\sum_{i=1}^{n}(Y_i-\bar Y)^2}&=r_{xy}^2 \textrm{ ,which is}\\ -->
<!-- \\ -->
<!-- 1-\frac{\sum_{i=1}^{n}(Y_i-\hat Y_i)^2}{\sum_{i=1}^{n}(Y_i-\bar Y)^2}=1-\frac{RSS}{TSS}&=r_{xy}^2. -->
<!-- \tag{4.17} -->
<!-- \end{align}$$ -->
<!-- Similarly, according to (0.2) the explained sum of squares (ESS) are the deviation between total (TSS) and residual sum of squares (RSS): -->
<!-- $$\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2 = \sum_{i=1}^{n}( Y_i-\bar Y)^2-\sum_{i=1}^{n}(Y_i-\hat Y)^2,$$ -->
<!-- therefore, we can write equation (4.17) as well as: -->
<!-- $$\begin{align} -->
<!-- \frac{\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2}{\sum_{i=1}^{n}(Y_i-\bar Y)^2}&=r_{xy}^2\\ -->
<!-- \\ -->
<!-- \frac{ESS}{RSS}&=r_{xy}^2=R^2. -->
<!-- \tag{4.18} -->
<!-- \end{align}$$ -->
<!-- **Final remarks:** -->
<!-- In the bivariate model (simple linear regression) the variance explained is equivalent to the squared bivariate correlation coefficient (Bravais Pearson).  -->
<!-- In multiple regression model the relation: -->
<!-- $$\frac{ESS}{TSS}=R^2$$ -->
<!-- holds as well. Here the variance explained is equivalent to the squared multiple correlation coefficient between the estimated and observed Y-value ($r_{\hat Y Y}$). -->
<!-- In the multiple linear regression model, the variance explained is also dependent on the number of X-variables. Therefore an adjusted $R^2$ measure should be used. -->

</div>
<!-- </div> -->
<div class="footnotes">
<hr />
<ol start="56">
<li id="fn56"><p>Here, <span class="math inline">\(X_\text{new}\)</span> is the predictor matrix for the new predictor vector <span class="math inline">\(x_\text{new}\)</span>.<a href="comparison-of-approaches.html#fnref56" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="a-bayesian-approach.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="Chap-04-02-Bayes-regression-practice.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["I2DA.epub", "I2DA.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
