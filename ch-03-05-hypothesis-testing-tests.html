<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>16.6 Selected tests | An Introduction to Data Analysis</title>
  <meta name="description" content="Introductory text for statistics and data analysis (using R)" />
  <meta name="generator" content="bookdown 0.21.6 and GitBook 2.6.7" />

  <meta property="og:title" content="16.6 Selected tests | An Introduction to Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Introductory text for statistics and data analysis (using R)" />
  <meta name="github-repo" content="michael-franke/intro-data-analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="16.6 Selected tests | An Introduction to Data Analysis" />
  
  <meta name="twitter:description" content="Introductory text for statistics and data analysis (using R)" />
  

<meta name="author" content="Michael Franke" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-05-01-frequentist-testing-confidence-intervals.html"/>
<link rel="next" href="ch-05-02-comparison-freq-Bayes.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<!--<link rel="stylesheet" href="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-editor-1.0.9.css">-->
<link rel="stylesheet" href="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-viz-0.7.11.css">

<script src="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-editor-1.0.9.js"></script>
<script src="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-viz-0.7.11.js"></script>
<script src="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-v0.9.13.js" defer async></script>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />

<script type="application/javascript">
document.addEventListener('DOMContentLoaded', function() {
  document.querySelectorAll('.collapsibleSolution, .collapsibleProof').forEach(function(collapsible) {
    const content = collapsible.querySelector('.content')
    content.style.display = 'none';
    collapsible.querySelector('.trigger').addEventListener('click', function() {
      if (content.style.display === 'none') {
        content.style.display = 'block';
      } else {
        content.style.display = 'none';
      }
    })
  })
})
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span. { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
</style>


<link rel="stylesheet" href="styles.css" type="text/css" />
<link rel="stylesheet" href="webppl-editor.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="index.html#section"></a></li>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li class="chapter" data-level="1" data-path="general-introduction.html"><a href="general-introduction.html"><i class="fa fa-check"></i><b>1</b> General Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="Chap-01-00-intro-learning-goals.html"><a href="Chap-01-00-intro-learning-goals.html"><i class="fa fa-check"></i><b>1.1</b> Learning goals</a></li>
<li class="chapter" data-level="1.2" data-path="Chap-01-00-intro-course-structure.html"><a href="Chap-01-00-intro-course-structure.html"><i class="fa fa-check"></i><b>1.2</b> Course structure</a></li>
<li class="chapter" data-level="1.3" data-path="Chap-01-00-intro-tools.html"><a href="Chap-01-00-intro-tools.html"><i class="fa fa-check"></i><b>1.3</b> Tools used in this course</a></li>
<li class="chapter" data-level="1.4" data-path="Chap-01-00-intro-topics.html"><a href="Chap-01-00-intro-topics.html"><i class="fa fa-check"></i><b>1.4</b> Topics covered (and not covered) in the course</a></li>
<li class="chapter" data-level="1.5" data-path="Chap-01-00-intro-data-sets.html"><a href="Chap-01-00-intro-data-sets.html"><i class="fa fa-check"></i><b>1.5</b> Data sets covered</a></li>
<li class="chapter" data-level="1.6" data-path="Chap-01-00-intro-installation.html"><a href="Chap-01-00-intro-installation.html"><i class="fa fa-check"></i><b>1.6</b> Installation</a></li>
<li class="chapter" data-level="1.7" data-path="Chap-01-00-intro-schedule.html"><a href="Chap-01-00-intro-schedule.html"><i class="fa fa-check"></i><b>1.7</b> Example schedule (12 week course)</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap-01-01-R.html"><a href="Chap-01-01-R.html"><i class="fa fa-check"></i><b>2</b> Basics of R</a><ul>
<li class="chapter" data-level="2.1" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html"><i class="fa fa-check"></i><b>2.1</b> First steps</a><ul>
<li class="chapter" data-level="2.1.1" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#functions"><i class="fa fa-check"></i><b>2.1.1</b> Functions</a></li>
<li class="chapter" data-level="2.1.2" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#variables"><i class="fa fa-check"></i><b>2.1.2</b> Variables</a></li>
<li class="chapter" data-level="2.1.3" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#literate-coding"><i class="fa fa-check"></i><b>2.1.3</b> Literate coding</a></li>
<li class="chapter" data-level="2.1.4" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#objects"><i class="fa fa-check"></i><b>2.1.4</b> Objects</a></li>
<li class="chapter" data-level="2.1.5" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#packages"><i class="fa fa-check"></i><b>2.1.5</b> Packages</a></li>
<li class="chapter" data-level="2.1.6" data-path="ch1-first-steps.html"><a href="ch1-first-steps.html#Chap-01-01-R-help"><i class="fa fa-check"></i><b>2.1.6</b> Getting help</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="ch1-data-types.html"><a href="ch1-data-types.html"><i class="fa fa-check"></i><b>2.2</b> Data types</a><ul>
<li class="chapter" data-level="2.2.1" data-path="ch1-data-types.html"><a href="ch1-data-types.html#numeric-vectors-matrices"><i class="fa fa-check"></i><b>2.2.1</b> Numeric vectors &amp; matrices</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch1-data-types.html"><a href="ch1-data-types.html#booleans"><i class="fa fa-check"></i><b>2.2.2</b> Booleans</a></li>
<li class="chapter" data-level="2.2.3" data-path="ch1-data-types.html"><a href="ch1-data-types.html#special-values"><i class="fa fa-check"></i><b>2.2.3</b> Special values</a></li>
<li class="chapter" data-level="2.2.4" data-path="ch1-data-types.html"><a href="ch1-data-types.html#characters-strings"><i class="fa fa-check"></i><b>2.2.4</b> Characters (= strings)</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch1-data-types.html"><a href="ch1-data-types.html#factors"><i class="fa fa-check"></i><b>2.2.5</b> Factors</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch1-data-types.html"><a href="ch1-data-types.html#lists-data-frames-tibbles"><i class="fa fa-check"></i><b>2.2.6</b> Lists, data frames &amp; tibbles</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="Chap-01-01-functions.html"><a href="Chap-01-01-functions.html"><i class="fa fa-check"></i><b>2.3</b> Functions</a><ul>
<li class="chapter" data-level="2.3.1" data-path="Chap-01-01-functions.html"><a href="Chap-01-01-functions.html#some-important-built-in-functions"><i class="fa fa-check"></i><b>2.3.1</b> Some important built-in functions</a></li>
<li class="chapter" data-level="2.3.2" data-path="Chap-01-01-functions.html"><a href="Chap-01-01-functions.html#defining-your-own-functions"><i class="fa fa-check"></i><b>2.3.2</b> Defining your own functions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="ch-01-01-loops-and-maps.html"><a href="ch-01-01-loops-and-maps.html"><i class="fa fa-check"></i><b>2.4</b> Loops and maps</a><ul>
<li class="chapter" data-level="2.4.1" data-path="ch-01-01-loops-and-maps.html"><a href="ch-01-01-loops-and-maps.html#for-loops"><i class="fa fa-check"></i><b>2.4.1</b> For-loops</a></li>
<li class="chapter" data-level="2.4.2" data-path="ch-01-01-loops-and-maps.html"><a href="ch-01-01-loops-and-maps.html#functional-iterators"><i class="fa fa-check"></i><b>2.4.2</b> Functional iterators</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="Chap-01-01-piping.html"><a href="Chap-01-01-piping.html"><i class="fa fa-check"></i><b>2.5</b> Piping</a></li>
<li class="chapter" data-level="2.6" data-path="ch-01-01-Rmarkdown.html"><a href="ch-01-01-Rmarkdown.html"><i class="fa fa-check"></i><b>2.6</b> Rmarkdown</a></li>
</ul></li>
<li class="part"><span><b>II Data</b></span></li>
<li class="chapter" data-level="3" data-path="Chap-02-01-data.html"><a href="Chap-02-01-data.html"><i class="fa fa-check"></i><b>3</b> Data, variables &amp; experimental designs</a><ul>
<li class="chapter" data-level="3.1" data-path="Chap-02-01-data-what-is-data.html"><a href="Chap-02-01-data-what-is-data.html"><i class="fa fa-check"></i><b>3.1</b> What is data?</a></li>
<li class="chapter" data-level="3.2" data-path="Chap-02-01-data-kinds-of-data.html"><a href="Chap-02-01-data-kinds-of-data.html"><i class="fa fa-check"></i><b>3.2</b> Different kinds of data</a></li>
<li class="chapter" data-level="3.3" data-path="Chap-02-01-data-variables.html"><a href="Chap-02-01-data-variables.html"><i class="fa fa-check"></i><b>3.3</b> On the notion of “variables”</a></li>
<li class="chapter" data-level="3.4" data-path="Chap-02-01-data-exp-design.html"><a href="Chap-02-01-data-exp-design.html"><i class="fa fa-check"></i><b>3.4</b> Basics of experimental design</a><ul>
<li class="chapter" data-level="3.4.1" data-path="Chap-02-01-data-exp-design.html"><a href="Chap-02-01-data-exp-design.html#what-to-analyze-dependent-variables"><i class="fa fa-check"></i><b>3.4.1</b> What to analyze? – Dependent variables</a></li>
<li class="chapter" data-level="3.4.2" data-path="Chap-02-01-data-exp-design.html"><a href="Chap-02-01-data-exp-design.html#conditions-trials-items"><i class="fa fa-check"></i><b>3.4.2</b> Conditions, trials, items</a></li>
<li class="chapter" data-level="3.4.3" data-path="Chap-02-01-data-exp-design.html"><a href="Chap-02-01-data-exp-design.html#sample-size"><i class="fa fa-check"></i><b>3.4.3</b> Sample size</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data-wrangling.html"><a href="data-wrangling.html"><i class="fa fa-check"></i><b>4</b> Data Wrangling</a><ul>
<li class="chapter" data-level="4.1" data-path="Chap-02-02-data-IO.html"><a href="Chap-02-02-data-IO.html"><i class="fa fa-check"></i><b>4.1</b> Data in, data out</a></li>
<li class="chapter" data-level="4.2" data-path="Chap-02-02-data-tidy-data.html"><a href="Chap-02-02-data-tidy-data.html"><i class="fa fa-check"></i><b>4.2</b> Tidy data</a><ul>
<li class="chapter" data-level="4.2.1" data-path="Chap-02-02-data-tidy-data.html"><a href="Chap-02-02-data-tidy-data.html#running-example"><i class="fa fa-check"></i><b>4.2.1</b> Running example</a></li>
<li class="chapter" data-level="4.2.2" data-path="Chap-02-02-data-tidy-data.html"><a href="Chap-02-02-data-tidy-data.html#definition-of-tidy-data"><i class="fa fa-check"></i><b>4.2.2</b> Definition of <em>tidy data</em></a></li>
<li class="chapter" data-level="4.2.3" data-path="Chap-02-02-data-tidy-data.html"><a href="Chap-02-02-data-tidy-data.html#excursion-non-redundant-data"><i class="fa fa-check"></i><b>4.2.3</b> Excursion: non-redundant data</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html"><i class="fa fa-check"></i><b>4.3</b> Data manipulation: the basics</a><ul>
<li class="chapter" data-level="4.3.1" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html#pivoting"><i class="fa fa-check"></i><b>4.3.1</b> Pivoting</a></li>
<li class="chapter" data-level="4.3.2" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html#subsetting-row-columns"><i class="fa fa-check"></i><b>4.3.2</b> Subsetting row &amp; columns</a></li>
<li class="chapter" data-level="4.3.3" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html#Chap-02-02-tidy-selection"><i class="fa fa-check"></i><b>4.3.3</b> Tidy selection of column names</a></li>
<li class="chapter" data-level="4.3.4" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html#adding-changing-and-renaming-columns"><i class="fa fa-check"></i><b>4.3.4</b> Adding, changing and renaming columns</a></li>
<li class="chapter" data-level="4.3.5" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html#splitting-and-uniting-columns"><i class="fa fa-check"></i><b>4.3.5</b> Splitting and uniting columns</a></li>
<li class="chapter" data-level="4.3.6" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html#sorting-a-data-set"><i class="fa fa-check"></i><b>4.3.6</b> Sorting a data set</a></li>
<li class="chapter" data-level="4.3.7" data-path="Chap-02-02-data-preprocessing-cleaning.html"><a href="Chap-02-02-data-preprocessing-cleaning.html#combining-tibbles"><i class="fa fa-check"></i><b>4.3.7</b> Combining tibbles</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="Chap-02-02-data-grouping-nesting.html"><a href="Chap-02-02-data-grouping-nesting.html"><i class="fa fa-check"></i><b>4.4</b> Grouped operations</a></li>
<li class="chapter" data-level="4.5" data-path="Chap-02-02-data-case-study-KoF.html"><a href="Chap-02-02-data-case-study-KoF.html"><i class="fa fa-check"></i><b>4.5</b> Case study: the King of France</a><ul>
<li class="chapter" data-level="4.5.1" data-path="Chap-02-02-data-case-study-KoF.html"><a href="Chap-02-02-data-case-study-KoF.html#cleaning-the-data"><i class="fa fa-check"></i><b>4.5.1</b> Cleaning the data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Chap-02-03-summary-statistics.html"><a href="Chap-02-03-summary-statistics.html"><i class="fa fa-check"></i><b>5</b> Summary statistics</a><ul>
<li class="chapter" data-level="5.1" data-path="Chap-02-03-summary-statistics-counts.html"><a href="Chap-02-03-summary-statistics-counts.html"><i class="fa fa-check"></i><b>5.1</b> Counts and proportions</a><ul>
<li class="chapter" data-level="5.1.1" data-path="Chap-02-03-summary-statistics-counts.html"><a href="Chap-02-03-summary-statistics-counts.html#loading-and-inspecting-the-data"><i class="fa fa-check"></i><b>5.1.1</b> Loading and inspecting the data</a></li>
<li class="chapter" data-level="5.1.2" data-path="Chap-02-03-summary-statistics-counts.html"><a href="Chap-02-03-summary-statistics-counts.html#obtaining-counts-with-n-count-and-tally"><i class="fa fa-check"></i><b>5.1.2</b> Obtaining counts with <code>n</code>, <code>count</code> and <code>tally</code></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="Chap-02-03-summary-statistics-1D.html"><a href="Chap-02-03-summary-statistics-1D.html"><i class="fa fa-check"></i><b>5.2</b> Central tendency and dispersion</a><ul>
<li class="chapter" data-level="5.2.1" data-path="Chap-02-03-summary-statistics-1D.html"><a href="Chap-02-03-summary-statistics-1D.html#the-data-for-the-remainder-of-the-chapter"><i class="fa fa-check"></i><b>5.2.1</b> The data for the remainder of the chapter</a></li>
<li class="chapter" data-level="5.2.2" data-path="Chap-02-03-summary-statistics-1D.html"><a href="Chap-02-03-summary-statistics-1D.html#measures-of-central-tendency"><i class="fa fa-check"></i><b>5.2.2</b> Measures of central tendency</a></li>
<li class="chapter" data-level="5.2.3" data-path="Chap-02-03-summary-statistics-1D.html"><a href="Chap-02-03-summary-statistics-1D.html#measures-of-dispersion"><i class="fa fa-check"></i><b>5.2.3</b> Measures of dispersion</a></li>
<li class="chapter" data-level="5.2.4" data-path="Chap-02-03-summary-statistics-1D.html"><a href="Chap-02-03-summary-statistics-1D.html#excursion-quantifying-confidence-with-bootstrapping"><i class="fa fa-check"></i><b>5.2.4</b> Excursion: Quantifying confidence with bootstrapping</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="Chap-02-03-summary-statistics-2D.html"><a href="Chap-02-03-summary-statistics-2D.html"><i class="fa fa-check"></i><b>5.3</b> Covariance and correlation</a><ul>
<li class="chapter" data-level="5.3.1" data-path="Chap-02-03-summary-statistics-2D.html"><a href="Chap-02-03-summary-statistics-2D.html#covariance"><i class="fa fa-check"></i><b>5.3.1</b> Covariance</a></li>
<li class="chapter" data-level="5.3.2" data-path="Chap-02-03-summary-statistics-2D.html"><a href="Chap-02-03-summary-statistics-2D.html#correlation"><i class="fa fa-check"></i><b>5.3.2</b> Correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Chap-02-02-visualization.html"><a href="Chap-02-02-visualization.html"><i class="fa fa-check"></i><b>6</b> Data Visualization</a><ul>
<li class="chapter" data-level="6.1" data-path="Chap-02-04-Anscombe-example.html"><a href="Chap-02-04-Anscombe-example.html"><i class="fa fa-check"></i><b>6.1</b> Motivating example: Anscombe’s quartet</a></li>
<li class="chapter" data-level="6.2" data-path="Chap-02-04-good-visualization.html"><a href="Chap-02-04-good-visualization.html"><i class="fa fa-check"></i><b>6.2</b> Visualization: the good, the bad and the infographic</a></li>
<li class="chapter" data-level="6.3" data-path="Chap-02-04-ggplot.html"><a href="Chap-02-04-ggplot.html"><i class="fa fa-check"></i><b>6.3</b> Basics of <code>ggplot</code></a><ul>
<li class="chapter" data-level="6.3.1" data-path="Chap-02-04-ggplot.html"><a href="Chap-02-04-ggplot.html#incremental-composition-of-a-plot"><i class="fa fa-check"></i><b>6.3.1</b> Incremental composition of a plot</a></li>
<li class="chapter" data-level="6.3.2" data-path="Chap-02-04-ggplot.html"><a href="Chap-02-04-ggplot.html#elements-in-the-layered-grammar-of-graphs"><i class="fa fa-check"></i><b>6.3.2</b> Elements in the layered grammar of graphs</a></li>
<li class="chapter" data-level="6.3.3" data-path="Chap-02-04-ggplot.html"><a href="Chap-02-04-ggplot.html#layers-and-groups"><i class="fa fa-check"></i><b>6.3.3</b> Layers and groups</a></li>
<li class="chapter" data-level="6.3.4" data-path="Chap-02-04-ggplot.html"><a href="Chap-02-04-ggplot.html#grouping"><i class="fa fa-check"></i><b>6.3.4</b> Grouping</a></li>
<li class="chapter" data-level="6.3.5" data-path="Chap-02-04-ggplot.html"><a href="Chap-02-04-ggplot.html#example-of-a-customized-plot"><i class="fa fa-check"></i><b>6.3.5</b> Example of a customized plot</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html"><i class="fa fa-check"></i><b>6.4</b> A rendezvous with popular geoms</a><ul>
<li class="chapter" data-level="6.4.1" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html#scatter-plots-with-geom_point"><i class="fa fa-check"></i><b>6.4.1</b> Scatter plots with <code>geom_point</code></a></li>
<li class="chapter" data-level="6.4.2" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html#smooth"><i class="fa fa-check"></i><b>6.4.2</b> Smooth</a></li>
<li class="chapter" data-level="6.4.3" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html#line"><i class="fa fa-check"></i><b>6.4.3</b> Line</a></li>
<li class="chapter" data-level="6.4.4" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html#bar-plot"><i class="fa fa-check"></i><b>6.4.4</b> Bar plot</a></li>
<li class="chapter" data-level="6.4.5" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html#plotting-distributions-histograms-boxplots-densities-and-violins"><i class="fa fa-check"></i><b>6.4.5</b> Plotting distributions: histograms, boxplots, densities and violins</a></li>
<li class="chapter" data-level="6.4.6" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html#rugs"><i class="fa fa-check"></i><b>6.4.6</b> Rugs</a></li>
<li class="chapter" data-level="6.4.7" data-path="Chap-02-04-geoms.html"><a href="Chap-02-04-geoms.html#annotation"><i class="fa fa-check"></i><b>6.4.7</b> Annotation</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="Chap-02-04-faceting.html"><a href="Chap-02-04-faceting.html"><i class="fa fa-check"></i><b>6.5</b> Faceting</a></li>
<li class="chapter" data-level="6.6" data-path="Chap-02-04-customization.html"><a href="Chap-02-04-customization.html"><i class="fa fa-check"></i><b>6.6</b> Customization etc.</a><ul>
<li class="chapter" data-level="6.6.1" data-path="Chap-02-04-customization.html"><a href="Chap-02-04-customization.html#themes"><i class="fa fa-check"></i><b>6.6.1</b> Themes</a></li>
<li class="chapter" data-level="6.6.2" data-path="Chap-02-04-customization.html"><a href="Chap-02-04-customization.html#guides"><i class="fa fa-check"></i><b>6.6.2</b> Guides</a></li>
<li class="chapter" data-level="6.6.3" data-path="Chap-02-04-customization.html"><a href="Chap-02-04-customization.html#axes-ticks-and-tick-labels"><i class="fa fa-check"></i><b>6.6.3</b> Axes, ticks and tick labels</a></li>
<li class="chapter" data-level="6.6.4" data-path="Chap-02-04-customization.html"><a href="Chap-02-04-customization.html#labels"><i class="fa fa-check"></i><b>6.6.4</b> Labels</a></li>
<li class="chapter" data-level="6.6.5" data-path="Chap-02-04-customization.html"><a href="Chap-02-04-customization.html#combining-arranging-plots"><i class="fa fa-check"></i><b>6.6.5</b> Combining &amp; arranging plots</a></li>
<li class="chapter" data-level="6.6.6" data-path="Chap-02-04-customization.html"><a href="Chap-02-04-customization.html#latex-expressions-in-plot-labels"><i class="fa fa-check"></i><b>6.6.6</b> LaTeX expressions in plot labels</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Bayesian Data Analysis</b></span></li>
<li class="chapter" data-level="7" data-path="Chap-03-01-probability.html"><a href="Chap-03-01-probability.html"><i class="fa fa-check"></i><b>7</b> Basics of Probability Theory</a><ul>
<li class="chapter" data-level="7.1" data-path="Chap-03-01-probability-basics.html"><a href="Chap-03-01-probability-basics.html"><i class="fa fa-check"></i><b>7.1</b> Probability</a><ul>
<li class="chapter" data-level="7.1.1" data-path="Chap-03-01-probability-basics.html"><a href="Chap-03-01-probability-basics.html#outcomes-events-observations"><i class="fa fa-check"></i><b>7.1.1</b> Outcomes, events, observations</a></li>
<li class="chapter" data-level="7.1.2" data-path="Chap-03-01-probability-basics.html"><a href="Chap-03-01-probability-basics.html#probability-distributions"><i class="fa fa-check"></i><b>7.1.2</b> Probability distributions</a></li>
<li class="chapter" data-level="7.1.3" data-path="Chap-03-01-probability-basics.html"><a href="Chap-03-01-probability-basics.html#interpretations-of-probability"><i class="fa fa-check"></i><b>7.1.3</b> Interpretations of probability</a></li>
<li class="chapter" data-level="7.1.4" data-path="Chap-03-01-probability-basics.html"><a href="Chap-03-01-probability-basics.html#distributions-as-samples"><i class="fa fa-check"></i><b>7.1.4</b> Distributions as samples</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="Chap-03-01-probability-marginal.html"><a href="Chap-03-01-probability-marginal.html"><i class="fa fa-check"></i><b>7.2</b> Structured events &amp; marginal distributions</a><ul>
<li class="chapter" data-level="7.2.1" data-path="Chap-03-01-probability-marginal.html"><a href="Chap-03-01-probability-marginal.html#probability-table-for-a-flip-and-draw-scenario"><i class="fa fa-check"></i><b>7.2.1</b> Probability table for a flip-and-draw scenario</a></li>
<li class="chapter" data-level="7.2.2" data-path="Chap-03-01-probability-marginal.html"><a href="Chap-03-01-probability-marginal.html#structured-events-and-joint-probability-distributions"><i class="fa fa-check"></i><b>7.2.2</b> Structured events and joint-probability distributions</a></li>
<li class="chapter" data-level="7.2.3" data-path="Chap-03-01-probability-marginal.html"><a href="Chap-03-01-probability-marginal.html#marginalization"><i class="fa fa-check"></i><b>7.2.3</b> Marginalization</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="Chap-03-01-probability-conditional.html"><a href="Chap-03-01-probability-conditional.html"><i class="fa fa-check"></i><b>7.3</b> Conditional probability</a><ul>
<li class="chapter" data-level="7.3.1" data-path="Chap-03-01-probability-conditional.html"><a href="Chap-03-01-probability-conditional.html#bayes-rule"><i class="fa fa-check"></i><b>7.3.1</b> Bayes rule</a></li>
<li class="chapter" data-level="7.3.2" data-path="Chap-03-01-probability-conditional.html"><a href="Chap-03-01-probability-conditional.html#Chap-03-01-probability-independence"><i class="fa fa-check"></i><b>7.3.2</b> Stochastic (in-)dependence</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="Chap-03-01-probability-random-variables.html"><a href="Chap-03-01-probability-random-variables.html"><i class="fa fa-check"></i><b>7.4</b> Random variables</a><ul>
<li class="chapter" data-level="7.4.1" data-path="Chap-03-01-probability-random-variables.html"><a href="Chap-03-01-probability-random-variables.html#notation-terminology"><i class="fa fa-check"></i><b>7.4.1</b> Notation &amp; terminology</a></li>
<li class="chapter" data-level="7.4.2" data-path="Chap-03-01-probability-random-variables.html"><a href="Chap-03-01-probability-random-variables.html#cumulative-distribution-functions-mass-density"><i class="fa fa-check"></i><b>7.4.2</b> Cumulative distribution functions, mass &amp; density</a></li>
<li class="chapter" data-level="7.4.3" data-path="Chap-03-01-probability-random-variables.html"><a href="Chap-03-01-probability-random-variables.html#expected-value-variance"><i class="fa fa-check"></i><b>7.4.3</b> Expected value &amp; variance</a></li>
<li class="chapter" data-level="7.4.4" data-path="Chap-03-01-probability-random-variables.html"><a href="Chap-03-01-probability-random-variables.html#composite-random-variables"><i class="fa fa-check"></i><b>7.4.4</b> Composite random variables</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="Chap-03-01-probability-R.html"><a href="Chap-03-01-probability-R.html"><i class="fa fa-check"></i><b>7.5</b> Probability distributions in R</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Chap-03-03-models.html"><a href="Chap-03-03-models.html"><i class="fa fa-check"></i><b>8</b> Statistical models</a><ul>
<li class="chapter" data-level="8.1" data-path="Chap-03-03-models-general.html"><a href="Chap-03-03-models-general.html"><i class="fa fa-check"></i><b>8.1</b> Statistical models</a></li>
<li class="chapter" data-level="8.2" data-path="Chap-03-03-models-representation.html"><a href="Chap-03-03-models-representation.html"><i class="fa fa-check"></i><b>8.2</b> Notation &amp; graphical representation</a><ul>
<li class="chapter" data-level="8.2.1" data-path="Chap-03-03-models-representation.html"><a href="Chap-03-03-models-representation.html#formula-notation"><i class="fa fa-check"></i><b>8.2.1</b> Formula notation</a></li>
<li class="chapter" data-level="8.2.2" data-path="Chap-03-03-models-representation.html"><a href="Chap-03-03-models-representation.html#graphical-notation"><i class="fa fa-check"></i><b>8.2.2</b> Graphical notation</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="Chap-03-03-models-parameters-priors.html"><a href="Chap-03-03-models-parameters-priors.html"><i class="fa fa-check"></i><b>8.3</b> Parameters, priors, and prior predictions</a><ul>
<li class="chapter" data-level="8.3.1" data-path="Chap-03-03-models-parameters-priors.html"><a href="Chap-03-03-models-parameters-priors.html#whats-a-model-parameter"><i class="fa fa-check"></i><b>8.3.1</b> What’s a model parameter?</a></li>
<li class="chapter" data-level="8.3.2" data-path="Chap-03-03-models-parameters-priors.html"><a href="Chap-03-03-models-parameters-priors.html#Chap-03-02-models-priors"><i class="fa fa-check"></i><b>8.3.2</b> Priors over parameters</a></li>
<li class="chapter" data-level="8.3.3" data-path="Chap-03-03-models-parameters-priors.html"><a href="Chap-03-03-models-parameters-priors.html#Chap-03-03-models-parameters-prior-predictive"><i class="fa fa-check"></i><b>8.3.3</b> Prior predictions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch-03-04-parameter-estimation.html"><a href="ch-03-04-parameter-estimation.html"><i class="fa fa-check"></i><b>9</b> Bayesian parameter estimation</a><ul>
<li class="chapter" data-level="9.1" data-path="ch-03-03-estimation-bayes.html"><a href="ch-03-03-estimation-bayes.html"><i class="fa fa-check"></i><b>9.1</b> Bayes rule for parameter estimation</a><ul>
<li class="chapter" data-level="9.1.1" data-path="ch-03-03-estimation-bayes.html"><a href="ch-03-03-estimation-bayes.html#definitions-and-terminology"><i class="fa fa-check"></i><b>9.1.1</b> Definitions and terminology</a></li>
<li class="chapter" data-level="9.1.2" data-path="ch-03-03-estimation-bayes.html"><a href="ch-03-03-estimation-bayes.html#the-effects-of-prior-and-likelihood-on-the-posterior"><i class="fa fa-check"></i><b>9.1.2</b> The effects of prior and likelihood on the posterior</a></li>
<li class="chapter" data-level="9.1.3" data-path="ch-03-03-estimation-bayes.html"><a href="ch-03-03-estimation-bayes.html#ch-03-04-parameter-estimation-conjugacy"><i class="fa fa-check"></i><b>9.1.3</b> Computing Bayesian posteriors with conjugate priors</a></li>
<li class="chapter" data-level="9.1.4" data-path="ch-03-03-estimation-bayes.html"><a href="ch-03-03-estimation-bayes.html#excursion-sequential-updating"><i class="fa fa-check"></i><b>9.1.4</b> Excursion: Sequential updating</a></li>
<li class="chapter" data-level="9.1.5" data-path="ch-03-03-estimation-bayes.html"><a href="ch-03-03-estimation-bayes.html#posterior-predictive-distribution"><i class="fa fa-check"></i><b>9.1.5</b> Posterior predictive distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch-03-04-parameter-estimation-points-intervals.html"><a href="ch-03-04-parameter-estimation-points-intervals.html"><i class="fa fa-check"></i><b>9.2</b> Point-valued and interval-ranged estimates</a><ul>
<li class="chapter" data-level="9.2.1" data-path="ch-03-04-parameter-estimation-points-intervals.html"><a href="ch-03-04-parameter-estimation-points-intervals.html#point-valued-estimates"><i class="fa fa-check"></i><b>9.2.1</b> Point-valued estimates</a></li>
<li class="chapter" data-level="9.2.2" data-path="ch-03-04-parameter-estimation-points-intervals.html"><a href="ch-03-04-parameter-estimation-points-intervals.html#interval-ranged-estimates"><i class="fa fa-check"></i><b>9.2.2</b> Interval-ranged estimates</a></li>
<li class="chapter" data-level="9.2.3" data-path="ch-03-04-parameter-estimation-points-intervals.html"><a href="ch-03-04-parameter-estimation-points-intervals.html#computing-bayesian-estimates"><i class="fa fa-check"></i><b>9.2.3</b> Computing Bayesian estimates</a></li>
<li class="chapter" data-level="9.2.4" data-path="ch-03-04-parameter-estimation-points-intervals.html"><a href="ch-03-04-parameter-estimation-points-intervals.html#excursion-computing-mles-and-maps-in-r"><i class="fa fa-check"></i><b>9.2.4</b> Excursion: Computing MLEs and MAPs in R</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="Ch-03-03-estimation-algorithms.html"><a href="Ch-03-03-estimation-algorithms.html"><i class="fa fa-check"></i><b>9.3</b> Approximating the posterior</a><ul>
<li class="chapter" data-level="9.3.1" data-path="Ch-03-03-estimation-algorithms.html"><a href="Ch-03-03-estimation-algorithms.html#ch-03-03-MCMC"><i class="fa fa-check"></i><b>9.3.1</b> Of apples and trees: Markov Chain Monte Carlo sampling</a></li>
<li class="chapter" data-level="9.3.2" data-path="Ch-03-03-estimation-algorithms.html"><a href="Ch-03-03-estimation-algorithms.html#ch-03-03-estimation-Stan"><i class="fa fa-check"></i><b>9.3.2</b> Excursion: Probabilistic modeling with Stan</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="ch-03-04-parameter-estimation-normal.html"><a href="ch-03-04-parameter-estimation-normal.html"><i class="fa fa-check"></i><b>9.4</b> Estimating the parameters of a Normal distribution</a><ul>
<li class="chapter" data-level="9.4.1" data-path="ch-03-04-parameter-estimation-normal.html"><a href="ch-03-04-parameter-estimation-normal.html#uninformative-priors"><i class="fa fa-check"></i><b>9.4.1</b> Uninformative priors</a></li>
<li class="chapter" data-level="9.4.2" data-path="ch-03-04-parameter-estimation-normal.html"><a href="ch-03-04-parameter-estimation-normal.html#conjugate-priors"><i class="fa fa-check"></i><b>9.4.2</b> Conjugate priors</a></li>
<li class="chapter" data-level="9.4.3" data-path="ch-03-04-parameter-estimation-normal.html"><a href="ch-03-04-parameter-estimation-normal.html#estimating-the-difference-between-group-means"><i class="fa fa-check"></i><b>9.4.3</b> Estimating the difference between group means</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="Chap-03-06-model-comparison.html"><a href="Chap-03-06-model-comparison.html"><i class="fa fa-check"></i><b>10</b> Model Comparison</a><ul>
<li class="chapter" data-level="10.1" data-path="Chap-03-06-model-comparison-case-study.html"><a href="Chap-03-06-model-comparison-case-study.html"><i class="fa fa-check"></i><b>10.1</b> Case study: recall models</a></li>
<li class="chapter" data-level="10.2" data-path="Chap-03-06-model-comparison-AIC.html"><a href="Chap-03-06-model-comparison-AIC.html"><i class="fa fa-check"></i><b>10.2</b> Akaike Information Criterion</a></li>
<li class="chapter" data-level="10.3" data-path="Chap-03-06-model-comparison-BF.html"><a href="Chap-03-06-model-comparison-BF.html"><i class="fa fa-check"></i><b>10.3</b> Bayes factors</a><ul>
<li class="chapter" data-level="10.3.1" data-path="Chap-03-06-model-comparison-BF.html"><a href="Chap-03-06-model-comparison-BF.html#Chap-03-06-model-comparison-BF-grid"><i class="fa fa-check"></i><b>10.3.1</b> Grid approximation</a></li>
<li class="chapter" data-level="10.3.2" data-path="Chap-03-06-model-comparison-BF.html"><a href="Chap-03-06-model-comparison-BF.html#Chap-03-06-model-comparison-BF-naiveMC"><i class="fa fa-check"></i><b>10.3.2</b> Naive Monte Carlo</a></li>
<li class="chapter" data-level="10.3.3" data-path="Chap-03-06-model-comparison-BF.html"><a href="Chap-03-06-model-comparison-BF.html#Chap-03-06-model-comparison-BF-bridge"><i class="fa fa-check"></i><b>10.3.3</b> Excursion: Bridge sampling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-03-07-hypothesis-testing-Bayes.html"><a href="ch-03-07-hypothesis-testing-Bayes.html"><i class="fa fa-check"></i><b>11</b> Bayesian hypothesis testing</a><ul>
<li class="chapter" data-level="11.1" data-path="ch-03-07-hypothesis-testing-Bayes-hypotheses.html"><a href="ch-03-07-hypothesis-testing-Bayes-hypotheses.html"><i class="fa fa-check"></i><b>11.1</b> Statistical hypotheses</a></li>
<li class="chapter" data-level="11.2" data-path="data-and-models-for-this-chapter.html"><a href="data-and-models-for-this-chapter.html"><i class="fa fa-check"></i><b>11.2</b> Data and models for this chapter</a><ul>
<li class="chapter" data-level="11.2.1" data-path="data-and-models-for-this-chapter.html"><a href="data-and-models-for-this-chapter.html#section-1"><i class="fa fa-check"></i><b>11.2.1</b> 24/7</a></li>
<li class="chapter" data-level="11.2.2" data-path="data-and-models-for-this-chapter.html"><a href="data-and-models-for-this-chapter.html#simon-task"><i class="fa fa-check"></i><b>11.2.2</b> Simon task</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="ch-03-05-Bayes-testing-estimation.html"><a href="ch-03-05-Bayes-testing-estimation.html"><i class="fa fa-check"></i><b>11.3</b> Testing via posterior estimation</a><ul>
<li class="chapter" data-level="11.3.1" data-path="ch-03-05-Bayes-testing-estimation.html"><a href="ch-03-05-Bayes-testing-estimation.html#example-247"><i class="fa fa-check"></i><b>11.3.1</b> Example: 24/7</a></li>
<li class="chapter" data-level="11.3.2" data-path="ch-03-05-Bayes-testing-estimation.html"><a href="ch-03-05-Bayes-testing-estimation.html#example-simon-task"><i class="fa fa-check"></i><b>11.3.2</b> Example: Simon Task</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="ch-03-05-Bayesian-testing-comparison.html"><a href="ch-03-05-Bayesian-testing-comparison.html"><i class="fa fa-check"></i><b>11.4</b> Testing via model comparison</a><ul>
<li class="chapter" data-level="11.4.1" data-path="ch-03-05-Bayesian-testing-comparison.html"><a href="ch-03-05-Bayesian-testing-comparison.html#ch-03-07-hypothesis-testing-Bayes-Savage-Dickey"><i class="fa fa-check"></i><b>11.4.1</b> The Savage-Dickey method</a></li>
<li class="chapter" data-level="11.4.2" data-path="ch-03-05-Bayesian-testing-comparison.html"><a href="ch-03-05-Bayesian-testing-comparison.html#ch-03-07-hypothesis-testing-Bayes-encompassing-models"><i class="fa fa-check"></i><b>11.4.2</b> Encompassing models</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Applied (generalized) linear modeling</b></span></li>
<li class="chapter" data-level="12" data-path="Chap-04-01-simple-linear-regression.html"><a href="Chap-04-01-simple-linear-regression.html"><i class="fa fa-check"></i><b>12</b> Linear regression</a><ul>
<li class="chapter" data-level="12.1" data-path="ordinary-least-squares-regression.html"><a href="ordinary-least-squares-regression.html"><i class="fa fa-check"></i><b>12.1</b> Ordinary least squares regression</a><ul>
<li class="chapter" data-level="12.1.1" data-path="ordinary-least-squares-regression.html"><a href="ordinary-least-squares-regression.html#prediction-without-any-further-information"><i class="fa fa-check"></i><b>12.1.1</b> Prediction without any further information</a></li>
<li class="chapter" data-level="12.1.2" data-path="ordinary-least-squares-regression.html"><a href="ordinary-least-squares-regression.html#prediction-with-knowledge-of-unemployment-rate"><i class="fa fa-check"></i><b>12.1.2</b> Prediction with knowledge of unemployment rate</a></li>
<li class="chapter" data-level="12.1.3" data-path="ordinary-least-squares-regression.html"><a href="ordinary-least-squares-regression.html#linear-regression-general-problem-formulation"><i class="fa fa-check"></i><b>12.1.3</b> Linear regression: general problem formulation</a></li>
<li class="chapter" data-level="12.1.4" data-path="ordinary-least-squares-regression.html"><a href="ordinary-least-squares-regression.html#finding-the-ols-solution"><i class="fa fa-check"></i><b>12.1.4</b> Finding the OLS-solution</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="Chap-04-01-linear-regression-MLE.html"><a href="Chap-04-01-linear-regression-MLE.html"><i class="fa fa-check"></i><b>12.2</b> A maximum-likelihood approach</a><ul>
<li class="chapter" data-level="12.2.1" data-path="Chap-04-01-linear-regression-MLE.html"><a href="Chap-04-01-linear-regression-MLE.html#a-likelihood-based-model"><i class="fa fa-check"></i><b>12.2.1</b> A likelihood-based model</a></li>
<li class="chapter" data-level="12.2.2" data-path="Chap-04-01-linear-regression-MLE.html"><a href="Chap-04-01-linear-regression-MLE.html#finding-the-mle-solution-with-optim"><i class="fa fa-check"></i><b>12.2.2</b> Finding the MLE-solution with <code>optim</code></a></li>
<li class="chapter" data-level="12.2.3" data-path="Chap-04-01-linear-regression-MLE.html"><a href="Chap-04-01-linear-regression-MLE.html#finding-the-mle-solution-with-glm"><i class="fa fa-check"></i><b>12.2.3</b> Finding the MLE-solution with <code>glm</code></a></li>
<li class="chapter" data-level="12.2.4" data-path="Chap-04-01-linear-regression-MLE.html"><a href="Chap-04-01-linear-regression-MLE.html#finding-the-mle-solution-with-math"><i class="fa fa-check"></i><b>12.2.4</b> Finding the MLE-solution with math</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="a-bayesian-approach.html"><a href="a-bayesian-approach.html"><i class="fa fa-check"></i><b>12.3</b> A Bayesian approach</a></li>
<li class="chapter" data-level="12.4" data-path="comparison-of-approaches.html"><a href="comparison-of-approaches.html"><i class="fa fa-check"></i><b>12.4</b> Comparison of approaches</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Chap-04-02-Bayes-regression-practice.html"><a href="Chap-04-02-Bayes-regression-practice.html"><i class="fa fa-check"></i><b>13</b> Bayesian regression in practice</a><ul>
<li class="chapter" data-level="13.1" data-path="simple-linear-regression-with-brms.html"><a href="simple-linear-regression-with-brms.html"><i class="fa fa-check"></i><b>13.1</b> Simple linear regression with <code>brms</code></a></li>
<li class="chapter" data-level="13.2" data-path="extracting-posterior-samples.html"><a href="extracting-posterior-samples.html"><i class="fa fa-check"></i><b>13.2</b> Extracting posterior samples</a></li>
<li class="chapter" data-level="13.3" data-path="excursion-inspecting-the-underlying-stan-code.html"><a href="excursion-inspecting-the-underlying-stan-code.html"><i class="fa fa-check"></i><b>13.3</b> [Excursion:] Inspecting the underlying Stan code</a></li>
<li class="chapter" data-level="13.4" data-path="setting-priors.html"><a href="setting-priors.html"><i class="fa fa-check"></i><b>13.4</b> Setting priors</a></li>
<li class="chapter" data-level="13.5" data-path="posterior-predictions.html"><a href="posterior-predictions.html"><i class="fa fa-check"></i><b>13.5</b> Posterior predictions</a></li>
<li class="chapter" data-level="13.6" data-path="testing-hypotheses.html"><a href="testing-hypotheses.html"><i class="fa fa-check"></i><b>13.6</b> Testing hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Chap-04-03-predictors.html"><a href="Chap-04-03-predictors.html"><i class="fa fa-check"></i><b>14</b> Categorical predictors</a><ul>
<li class="chapter" data-level="14.1" data-path="Chap-04-03-predictors-two-levels.html"><a href="Chap-04-03-predictors-two-levels.html"><i class="fa fa-check"></i><b>14.1</b> Single two-level predictor</a></li>
<li class="chapter" data-level="14.2" data-path="Chap-04-03-predictors-multi-levels.html"><a href="Chap-04-03-predictors-multi-levels.html"><i class="fa fa-check"></i><b>14.2</b> Single multi-level predictor</a></li>
<li class="chapter" data-level="14.3" data-path="Chap-04-03-predictors-multiple-predictors.html"><a href="Chap-04-03-predictors-multiple-predictors.html"><i class="fa fa-check"></i><b>14.3</b> Multiple predictors</a><ul>
<li class="chapter" data-level="14.3.1" data-path="Chap-04-03-predictors-multiple-predictors.html"><a href="Chap-04-03-predictors-multiple-predictors.html#treatment-coding"><i class="fa fa-check"></i><b>14.3.1</b> Treatment coding</a></li>
<li class="chapter" data-level="14.3.2" data-path="Chap-04-03-predictors-multiple-predictors.html"><a href="Chap-04-03-predictors-multiple-predictors.html#sum-coding"><i class="fa fa-check"></i><b>14.3.2</b> Sum coding</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="Chap-04-04-GLM.html"><a href="Chap-04-04-GLM.html"><i class="fa fa-check"></i><b>15</b> Generalized linear model</a><ul>
<li class="chapter" data-level="15.1" data-path="generalizing-the-linear-regression-model.html"><a href="generalizing-the-linear-regression-model.html"><i class="fa fa-check"></i><b>15.1</b> Generalizing the linear regression model</a></li>
<li class="chapter" data-level="15.2" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>15.2</b> Logistic regression</a></li>
</ul></li>
<li class="part"><span><b>V Frequentist statistics</b></span></li>
<li class="chapter" data-level="16" data-path="ch-05-01-frequentist-hypothesis-testing.html"><a href="ch-05-01-frequentist-hypothesis-testing.html"><i class="fa fa-check"></i><b>16</b> Null Hypothesis Significance Testing</a><ul>
<li class="chapter" data-level="16.1" data-path="ch-05-01-frequentist-testing-overview.html"><a href="ch-05-01-frequentist-testing-overview.html"><i class="fa fa-check"></i><b>16.1</b> Frequentist statistics: why &amp; how</a></li>
<li class="chapter" data-level="16.2" data-path="ch-03-05-hypothesis-p-values.html"><a href="ch-03-05-hypothesis-p-values.html"><i class="fa fa-check"></i><b>16.2</b> Quantifying evidence against a null-model with <em>p</em>-values</a><ul>
<li class="chapter" data-level="16.2.1" data-path="ch-03-05-hypothesis-p-values.html"><a href="ch-03-05-hypothesis-p-values.html#frequentist-null-models"><i class="fa fa-check"></i><b>16.2.1</b> Frequentist null-models</a></li>
<li class="chapter" data-level="16.2.2" data-path="ch-03-05-hypothesis-p-values.html"><a href="ch-03-05-hypothesis-p-values.html#one--vs.two-sided-p-values"><i class="fa fa-check"></i><b>16.2.2</b> One- vs. two-sided <span class="math inline">\(p\)</span>-values</a></li>
<li class="chapter" data-level="16.2.3" data-path="ch-03-05-hypothesis-p-values.html"><a href="ch-03-05-hypothesis-p-values.html#significance-categorical-decisions"><i class="fa fa-check"></i><b>16.2.3</b> Significance &amp; categorical decisions</a></li>
<li class="chapter" data-level="16.2.4" data-path="ch-03-05-hypothesis-p-values.html"><a href="ch-03-05-hypothesis-p-values.html#how-not-to-interpret-p-values"><i class="fa fa-check"></i><b>16.2.4</b> How (not) to interpret <em>p</em>-values</a></li>
<li class="chapter" data-level="16.2.5" data-path="ch-03-05-hypothesis-p-values.html"><a href="ch-03-05-hypothesis-p-values.html#excursion-distribution-of-p-values"><i class="fa fa-check"></i><b>16.2.5</b> [Excursion] Distribution of <span class="math inline">\(p\)</span>-values</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="ch-03-05-hypothesis-testing-CLT.html"><a href="ch-03-05-hypothesis-testing-CLT.html"><i class="fa fa-check"></i><b>16.3</b> [Excursion] Central Limit Theorem</a></li>
<li class="chapter" data-level="16.4" data-path="ch-03-04-hypothesis-significance-errors.html"><a href="ch-03-04-hypothesis-significance-errors.html"><i class="fa fa-check"></i><b>16.4</b> [Excursion] The Neyman-Pearson approach</a></li>
<li class="chapter" data-level="16.5" data-path="ch-05-01-frequentist-testing-confidence-intervals.html"><a href="ch-05-01-frequentist-testing-confidence-intervals.html"><i class="fa fa-check"></i><b>16.5</b> Confidence intervals</a><ul>
<li class="chapter" data-level="16.5.1" data-path="ch-05-01-frequentist-testing-confidence-intervals.html"><a href="ch-05-01-frequentist-testing-confidence-intervals.html#relation-of-p-values-to-confidence-intervals"><i class="fa fa-check"></i><b>16.5.1</b> Relation of <em>p</em>-values to confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="ch-03-05-hypothesis-testing-tests.html"><a href="ch-03-05-hypothesis-testing-tests.html"><i class="fa fa-check"></i><b>16.6</b> Selected tests</a><ul>
<li class="chapter" data-level="16.6.1" data-path="ch-03-05-hypothesis-testing-tests.html"><a href="ch-03-05-hypothesis-testing-tests.html#ch-03-05-hypothesis-testing-Pearsons-Chi"><i class="fa fa-check"></i><b>16.6.1</b> Pearson’s <span class="math inline">\(\chi^2\)</span>-tests</a></li>
<li class="chapter" data-level="16.6.2" data-path="ch-03-05-hypothesis-testing-tests.html"><a href="ch-03-05-hypothesis-testing-tests.html#ch-03-05-hypothesis-testing-z-test"><i class="fa fa-check"></i><b>16.6.2</b> <em>z</em>-test</a></li>
<li class="chapter" data-level="16.6.3" data-path="ch-03-05-hypothesis-testing-tests.html"><a href="ch-03-05-hypothesis-testing-tests.html#ch-03-05-hypothesis-testing-t-test"><i class="fa fa-check"></i><b>16.6.3</b> <em>t</em>-tests</a></li>
<li class="chapter" data-level="16.6.4" data-path="ch-03-05-hypothesis-testing-tests.html"><a href="ch-03-05-hypothesis-testing-tests.html#ch-03-05-hypothesis-testing-ANOVA"><i class="fa fa-check"></i><b>16.6.4</b> ANOVA</a></li>
<li class="chapter" data-level="16.6.5" data-path="ch-03-05-hypothesis-testing-tests.html"><a href="ch-03-05-hypothesis-testing-tests.html#linear-regression"><i class="fa fa-check"></i><b>16.6.5</b> Linear regression</a></li>
<li class="chapter" data-level="16.6.6" data-path="ch-03-05-hypothesis-testing-tests.html"><a href="ch-03-05-hypothesis-testing-tests.html#Chap-05-01-LR-test"><i class="fa fa-check"></i><b>16.6.6</b> Likelihood-Ratio Test</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ch-05-02-comparison-freq-Bayes.html"><a href="ch-05-02-comparison-freq-Bayes.html"><i class="fa fa-check"></i><b>17</b> Comparing frequentist and Bayesian statistics</a><ul>
<li class="chapter" data-level="17.1" data-path="frequentist-and-bayesian-statistical-models.html"><a href="frequentist-and-bayesian-statistical-models.html"><i class="fa fa-check"></i><b>17.1</b> Frequentist and Bayesian statistical models</a></li>
<li class="chapter" data-level="17.2" data-path="approximation-in-the-model-or-through-the-computation.html"><a href="approximation-in-the-model-or-through-the-computation.html"><i class="fa fa-check"></i><b>17.2</b> Approximation: in the model or through the computation</a></li>
<li class="chapter" data-level="17.3" data-path="mc-simulated-p-values.html"><a href="mc-simulated-p-values.html"><i class="fa fa-check"></i><b>17.3</b> MC-simulated <span class="math inline">\(p\)</span>-values</a></li>
<li class="chapter" data-level="17.4" data-path="bayesian-p-values-model-checking.html"><a href="bayesian-p-values-model-checking.html"><i class="fa fa-check"></i><b>17.4</b> Bayesian <span class="math inline">\(p\)</span>-values &amp; model checking</a></li>
<li class="chapter" data-level="17.5" data-path="ch-05-01-estimation-comparison.html"><a href="ch-05-01-estimation-comparison.html"><i class="fa fa-check"></i><b>17.5</b> Comparing Bayesian and frequentist estimates</a></li>
<li class="chapter" data-level="17.6" data-path="beliefs-decisions-and-long-term-error.html"><a href="beliefs-decisions-and-long-term-error.html"><i class="fa fa-check"></i><b>17.6</b> Beliefs, decisions and long-term error</a></li>
<li class="chapter" data-level="17.7" data-path="evidence-for-the-null.html"><a href="evidence-for-the-null.html"><i class="fa fa-check"></i><b>17.7</b> Evidence for the null</a></li>
<li class="chapter" data-level="17.8" data-path="Chap-05-02-models-three-pillars.html"><a href="Chap-05-02-models-three-pillars.html"><i class="fa fa-check"></i><b>17.8</b> Three pillars of data analysis</a></li>
<li class="chapter" data-level="17.9" data-path="testing-hypotheses-by-estimation-comparison-model-checking.html"><a href="testing-hypotheses-by-estimation-comparison-model-checking.html"><i class="fa fa-check"></i><b>17.9</b> Testing hypotheses by estimation, comparison &amp; model checking</a></li>
<li class="chapter" data-level="17.10" data-path="jeffreys-lindley-paradox.html"><a href="jeffreys-lindley-paradox.html"><i class="fa fa-check"></i><b>17.10</b> Jeffreys-Lindley paradox</a></li>
<li class="chapter" data-level="17.11" data-path="explicit-beliefs-vs-implicit-intentions.html"><a href="explicit-beliefs-vs-implicit-intentions.html"><i class="fa fa-check"></i><b>17.11</b> Explicit beliefs vs. implicit intentions</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="app-90-further-material.html"><a href="app-90-further-material.html"><i class="fa fa-check"></i><b>A</b> Further useful material</a><ul>
<li class="chapter" data-level="A.1" data-path="material-on-introduction-to-probability.html"><a href="material-on-introduction-to-probability.html"><i class="fa fa-check"></i><b>A.1</b> Material on <em>Introduction to Probability</em>:</a></li>
<li class="chapter" data-level="A.2" data-path="material-on-bayesian-data-analysis.html"><a href="material-on-bayesian-data-analysis.html"><i class="fa fa-check"></i><b>A.2</b> Material on <em>Bayesian Data Analysis</em>:</a></li>
<li class="chapter" data-level="A.3" data-path="material-on-frequentist-statistics.html"><a href="material-on-frequentist-statistics.html"><i class="fa fa-check"></i><b>A.3</b> Material on <em>frequentist statistics</em>:</a></li>
<li class="chapter" data-level="A.4" data-path="material-on-r-tidyverse-etc-.html"><a href="material-on-r-tidyverse-etc-.html"><i class="fa fa-check"></i><b>A.4</b> Material on <em>R, tidyverse, etc.</em>:</a></li>
<li class="chapter" data-level="A.5" data-path="further-information-for-rstudio.html"><a href="further-information-for-rstudio.html"><i class="fa fa-check"></i><b>A.5</b> Further information for RStudio</a></li>
<li class="chapter" data-level="A.6" data-path="further-information-on-webppl.html"><a href="further-information-on-webppl.html"><i class="fa fa-check"></i><b>A.6</b> Further information on WebPPL</a><ul>
<li class="chapter" data-level="A.6.1" data-path="further-information-on-webppl.html"><a href="further-information-on-webppl.html#primitives-and-sampling-functions"><i class="fa fa-check"></i><b>A.6.1</b> Primitives and sampling functions</a></li>
<li class="chapter" data-level="A.6.2" data-path="further-information-on-webppl.html"><a href="further-information-on-webppl.html#inference-with-infer"><i class="fa fa-check"></i><b>A.6.2</b> Inference with <code>Infer()</code></a></li>
<li class="chapter" data-level="A.6.3" data-path="further-information-on-webppl.html"><a href="further-information-on-webppl.html#visualization"><i class="fa fa-check"></i><b>A.6.3</b> Visualization</a></li>
<li class="chapter" data-level="A.6.4" data-path="further-information-on-webppl.html"><a href="further-information-on-webppl.html#installation"><i class="fa fa-check"></i><b>A.6.4</b> Installation</a></li>
<li class="chapter" data-level="A.6.5" data-path="further-information-on-webppl.html"><a href="further-information-on-webppl.html#usage"><i class="fa fa-check"></i><b>A.6.5</b> Usage</a></li>
<li class="chapter" data-level="A.6.6" data-path="further-information-on-webppl.html"><a href="further-information-on-webppl.html#keyboard-shortcuts-for-in-browser-use"><i class="fa fa-check"></i><b>A.6.6</b> Keyboard shortcuts (for in-browser use)</a></li>
<li class="chapter" data-level="A.6.7" data-path="further-information-on-webppl.html"><a href="further-information-on-webppl.html#further-resources"><i class="fa fa-check"></i><b>A.6.7</b> Further resources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="app-91-distributions.html"><a href="app-91-distributions.html"><i class="fa fa-check"></i><b>B</b> Common probability distributions</a><ul>
<li class="chapter" data-level="B.1" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html"><i class="fa fa-check"></i><b>B.1</b> Selected continuous distributions of random variables</a><ul>
<li class="chapter" data-level="B.1.1" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html#app-91-distributions-normal"><i class="fa fa-check"></i><b>B.1.1</b> Normal distribution</a></li>
<li class="chapter" data-level="B.1.2" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html#app-91-distributions-chi2"><i class="fa fa-check"></i><b>B.1.2</b> Chi-squared distribution</a></li>
<li class="chapter" data-level="B.1.3" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html#app-91-distributions-F"><i class="fa fa-check"></i><b>B.1.3</b> F-distribution</a></li>
<li class="chapter" data-level="B.1.4" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html#app-91-distributions-students-t"><i class="fa fa-check"></i><b>B.1.4</b> Student’s <em>t</em>-distribution</a></li>
<li class="chapter" data-level="B.1.5" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html#app-91-distributions-beta"><i class="fa fa-check"></i><b>B.1.5</b> Beta distribution</a></li>
<li class="chapter" data-level="B.1.6" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html#uniform-distribution"><i class="fa fa-check"></i><b>B.1.6</b> Uniform distribution</a></li>
<li class="chapter" data-level="B.1.7" data-path="selected-continuous-distributions-of-random-variables.html"><a href="selected-continuous-distributions-of-random-variables.html#app-91-distributions-dirichlet"><i class="fa fa-check"></i><b>B.1.7</b> Dirichlet distribution</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="selected-discrete-distributions-of-random-variables.html"><a href="selected-discrete-distributions-of-random-variables.html"><i class="fa fa-check"></i><b>B.2</b> Selected discrete distributions of random variables</a><ul>
<li class="chapter" data-level="B.2.1" data-path="selected-discrete-distributions-of-random-variables.html"><a href="selected-discrete-distributions-of-random-variables.html#app-91-distributions-binomial"><i class="fa fa-check"></i><b>B.2.1</b> Binomial distribution</a></li>
<li class="chapter" data-level="B.2.2" data-path="selected-discrete-distributions-of-random-variables.html"><a href="selected-discrete-distributions-of-random-variables.html#app-91-distributions-multinomial"><i class="fa fa-check"></i><b>B.2.2</b> Multinomial distribution</a></li>
<li class="chapter" data-level="B.2.3" data-path="selected-discrete-distributions-of-random-variables.html"><a href="selected-discrete-distributions-of-random-variables.html#app-91-distributions-bernoulli"><i class="fa fa-check"></i><b>B.2.3</b> Bernoulli distribution</a></li>
<li class="chapter" data-level="B.2.4" data-path="selected-discrete-distributions-of-random-variables.html"><a href="selected-discrete-distributions-of-random-variables.html#app-91-distributions-categorical"><i class="fa fa-check"></i><b>B.2.4</b> Categorical distribution</a></li>
<li class="chapter" data-level="B.2.5" data-path="selected-discrete-distributions-of-random-variables.html"><a href="selected-discrete-distributions-of-random-variables.html#app-91-distributions-beta-binomial"><i class="fa fa-check"></i><b>B.2.5</b> Beta-Binomial distribution</a></li>
<li class="chapter" data-level="B.2.6" data-path="selected-discrete-distributions-of-random-variables.html"><a href="selected-discrete-distributions-of-random-variables.html#poisson-distribution"><i class="fa fa-check"></i><b>B.2.6</b> Poisson distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="app-92-exponential-family.html"><a href="app-92-exponential-family.html"><i class="fa fa-check"></i><b>C</b> Exponential Family and Maximum Entropy</a><ul>
<li class="chapter" data-level="C.1" data-path="an-important-family-the-exponential-family.html"><a href="an-important-family-the-exponential-family.html"><i class="fa fa-check"></i><b>C.1</b> An important family: The Exponential Family</a></li>
<li class="chapter" data-level="C.2" data-path="the-maximum-entropy-principle.html"><a href="the-maximum-entropy-principle.html"><i class="fa fa-check"></i><b>C.2</b> The Maximum Entropy Principle</a><ul>
<li class="chapter" data-level="C.2.1" data-path="the-maximum-entropy-principle.html"><a href="the-maximum-entropy-principle.html#information-entropy"><i class="fa fa-check"></i><b>C.2.1</b> Information Entropy</a></li>
<li class="chapter" data-level="C.2.2" data-path="the-maximum-entropy-principle.html"><a href="the-maximum-entropy-principle.html#deriving-probability-distributions-using-the-maximum-entropy-principle"><i class="fa fa-check"></i><b>C.2.2</b> Deriving Probability Distributions using the Maximum Entropy Principle</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="D" data-path="app-93-data-sets.html"><a href="app-93-data-sets.html"><i class="fa fa-check"></i><b>D</b> Data sets used in the book</a><ul>
<li class="chapter" data-level="D.1" data-path="app-93-data-sets-mental-chronometry.html"><a href="app-93-data-sets-mental-chronometry.html"><i class="fa fa-check"></i><b>D.1</b> Mental Chronometry</a><ul>
<li class="chapter" data-level="D.1.1" data-path="app-93-data-sets-mental-chronometry.html"><a href="app-93-data-sets-mental-chronometry.html#nature-origin-and-rationale-of-the-data"><i class="fa fa-check"></i><b>D.1.1</b> Nature, origin and rationale of the data</a></li>
<li class="chapter" data-level="D.1.2" data-path="app-93-data-sets-mental-chronometry.html"><a href="app-93-data-sets-mental-chronometry.html#loading-and-preprocessing-the-data"><i class="fa fa-check"></i><b>D.1.2</b> Loading and preprocessing the data</a></li>
<li class="chapter" data-level="D.1.3" data-path="app-93-data-sets-mental-chronometry.html"><a href="app-93-data-sets-mental-chronometry.html#cleaning-the-data-1"><i class="fa fa-check"></i><b>D.1.3</b> Cleaning the data</a></li>
<li class="chapter" data-level="D.1.4" data-path="app-93-data-sets-mental-chronometry.html"><a href="app-93-data-sets-mental-chronometry.html#exploration-summary-stats-plots"><i class="fa fa-check"></i><b>D.1.4</b> Exploration: summary stats &amp; plots</a></li>
</ul></li>
<li class="chapter" data-level="D.2" data-path="app-93-data-sets-simon-task.html"><a href="app-93-data-sets-simon-task.html"><i class="fa fa-check"></i><b>D.2</b> Simon Task</a><ul>
<li class="chapter" data-level="D.2.1" data-path="app-93-data-sets-simon-task.html"><a href="app-93-data-sets-simon-task.html#experiment"><i class="fa fa-check"></i><b>D.2.1</b> Experiment</a></li>
<li class="chapter" data-level="D.2.2" data-path="app-93-data-sets-simon-task.html"><a href="app-93-data-sets-simon-task.html#hypotheses"><i class="fa fa-check"></i><b>D.2.2</b> Hypotheses</a></li>
<li class="chapter" data-level="D.2.3" data-path="app-93-data-sets-simon-task.html"><a href="app-93-data-sets-simon-task.html#results"><i class="fa fa-check"></i><b>D.2.3</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="D.3" data-path="app-93-data-sets-king-of-france.html"><a href="app-93-data-sets-king-of-france.html"><i class="fa fa-check"></i><b>D.3</b> King of France</a><ul>
<li class="chapter" data-level="D.3.1" data-path="app-93-data-sets-king-of-france.html"><a href="app-93-data-sets-king-of-france.html#app-93-data-sets-king-of-france-background"><i class="fa fa-check"></i><b>D.3.1</b> Nature, origin and rationale of the data</a></li>
<li class="chapter" data-level="D.3.2" data-path="app-93-data-sets-king-of-france.html"><a href="app-93-data-sets-king-of-france.html#loading-and-preprocessing-the-data-1"><i class="fa fa-check"></i><b>D.3.2</b> Loading and preprocessing the data</a></li>
<li class="chapter" data-level="D.3.3" data-path="app-93-data-sets-king-of-france.html"><a href="app-93-data-sets-king-of-france.html#cleaning-the-data-3"><i class="fa fa-check"></i><b>D.3.3</b> Cleaning the data</a></li>
<li class="chapter" data-level="D.3.4" data-path="app-93-data-sets-king-of-france.html"><a href="app-93-data-sets-king-of-france.html#exploration-summary-stats-plots-1"><i class="fa fa-check"></i><b>D.3.4</b> Exploration: summary stats &amp; plots</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="app-93-data-sets-BLJM.html"><a href="app-93-data-sets-BLJM.html"><i class="fa fa-check"></i><b>D.4</b> Bio-Logic Jazz-Metal (and where to consume it)</a><ul>
<li class="chapter" data-level="D.4.1" data-path="app-93-data-sets-BLJM.html"><a href="app-93-data-sets-BLJM.html#nature-origin-and-rationale-of-the-data-1"><i class="fa fa-check"></i><b>D.4.1</b> Nature, origin and rationale of the data</a></li>
<li class="chapter" data-level="D.4.2" data-path="app-93-data-sets-BLJM.html"><a href="app-93-data-sets-BLJM.html#loading-and-preprocessing-the-data-2"><i class="fa fa-check"></i><b>D.4.2</b> Loading and preprocessing the data</a></li>
<li class="chapter" data-level="D.4.3" data-path="app-93-data-sets-BLJM.html"><a href="app-93-data-sets-BLJM.html#exploration-counts-plots"><i class="fa fa-check"></i><b>D.4.3</b> Exploration: counts &amp; plots</a></li>
</ul></li>
<li class="chapter" data-level="D.5" data-path="app-93-data-sets-avocado.html"><a href="app-93-data-sets-avocado.html"><i class="fa fa-check"></i><b>D.5</b> Avocado prices</a><ul>
<li class="chapter" data-level="D.5.1" data-path="app-93-data-sets-avocado.html"><a href="app-93-data-sets-avocado.html#nature-origin-and-rationale-of-the-data-2"><i class="fa fa-check"></i><b>D.5.1</b> Nature, origin and rationale of the data</a></li>
<li class="chapter" data-level="D.5.2" data-path="app-93-data-sets-avocado.html"><a href="app-93-data-sets-avocado.html#loading-and-preprocessing-the-data-3"><i class="fa fa-check"></i><b>D.5.2</b> Loading and preprocessing the data</a></li>
<li class="chapter" data-level="D.5.3" data-path="app-93-data-sets-avocado.html"><a href="app-93-data-sets-avocado.html#summary-statistics"><i class="fa fa-check"></i><b>D.5.3</b> Summary statistics</a></li>
<li class="chapter" data-level="D.5.4" data-path="app-93-data-sets-avocado.html"><a href="app-93-data-sets-avocado.html#plots"><i class="fa fa-check"></i><b>D.5.4</b> Plots</a></li>
</ul></li>
<li class="chapter" data-level="D.6" data-path="app-93-data-sets-temperature.html"><a href="app-93-data-sets-temperature.html"><i class="fa fa-check"></i><b>D.6</b> Annual average world surface temperature</a><ul>
<li class="chapter" data-level="D.6.1" data-path="app-93-data-sets-temperature.html"><a href="app-93-data-sets-temperature.html#nature-origin-and-rationale-of-the-data-3"><i class="fa fa-check"></i><b>D.6.1</b> Nature, origin and rationale of the data</a></li>
<li class="chapter" data-level="D.6.2" data-path="app-93-data-sets-temperature.html"><a href="app-93-data-sets-temperature.html#loading-and-preprocessing-the-data-4"><i class="fa fa-check"></i><b>D.6.2</b> Loading and preprocessing the data</a></li>
<li class="chapter" data-level="D.6.3" data-path="app-93-data-sets-temperature.html"><a href="app-93-data-sets-temperature.html#hypothesis-modeling-approach"><i class="fa fa-check"></i><b>D.6.3</b> Hypothesis &amp; modeling approach</a></li>
<li class="chapter" data-level="D.6.4" data-path="app-93-data-sets-temperature.html"><a href="app-93-data-sets-temperature.html#plotting"><i class="fa fa-check"></i><b>D.6.4</b> Plotting</a></li>
</ul></li>
<li class="chapter" data-level="D.7" data-path="app-93-data-sets-murder-data.html"><a href="app-93-data-sets-murder-data.html"><i class="fa fa-check"></i><b>D.7</b> Murder data</a><ul>
<li class="chapter" data-level="D.7.1" data-path="app-93-data-sets-murder-data.html"><a href="app-93-data-sets-murder-data.html#nature-origin-and-rationale-of-the-data-4"><i class="fa fa-check"></i><b>D.7.1</b> Nature, origin and rationale of the data</a></li>
</ul></li>
<li class="chapter" data-level="D.8" data-path="app-93-data-sets-politeness.html"><a href="app-93-data-sets-politeness.html"><i class="fa fa-check"></i><b>D.8</b> Politeness data</a><ul>
<li class="chapter" data-level="D.8.1" data-path="app-93-data-sets-politeness.html"><a href="app-93-data-sets-politeness.html#nature-origin-and-rationale-of-the-data-5"><i class="fa fa-check"></i><b>D.8.1</b> Nature, origin and rationale of the data</a></li>
<li class="chapter" data-level="D.8.2" data-path="app-93-data-sets-politeness.html"><a href="app-93-data-sets-politeness.html#hypotheses-2"><i class="fa fa-check"></i><b>D.8.2</b> Hypotheses</a></li>
<li class="chapter" data-level="D.8.3" data-path="app-93-data-sets-politeness.html"><a href="app-93-data-sets-politeness.html#summary-statistics-1"><i class="fa fa-check"></i><b>D.8.3</b> Summary statistics</a></li>
<li class="chapter" data-level="D.8.4" data-path="app-93-data-sets-politeness.html"><a href="app-93-data-sets-politeness.html#visualization-1"><i class="fa fa-check"></i><b>D.8.4</b> Visualization</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="E" data-path="app-94-open-science.html"><a href="app-94-open-science.html"><i class="fa fa-check"></i><b>E</b> Open science practices</a><ul>
<li class="chapter" data-level="E.1" data-path="app-94-replication-crisis.html"><a href="app-94-replication-crisis.html"><i class="fa fa-check"></i><b>E.1</b> Psychology’s replication crisis</a><ul>
<li class="chapter" data-level="E.1.1" data-path="app-94-replication-crisis.html"><a href="app-94-replication-crisis.html#publication-bias-qrps-and-false-positives"><i class="fa fa-check"></i><b>E.1.1</b> Publication bias, QRP’s, and false-positives</a></li>
<li class="chapter" data-level="E.1.2" data-path="app-94-replication-crisis.html"><a href="app-94-replication-crisis.html#low-statistical-power"><i class="fa fa-check"></i><b>E.1.2</b> Low statistical power</a></li>
<li class="chapter" data-level="E.1.3" data-path="app-94-replication-crisis.html"><a href="app-94-replication-crisis.html#lack-of-transparency"><i class="fa fa-check"></i><b>E.1.3</b> Lack of transparency</a></li>
</ul></li>
<li class="chapter" data-level="E.2" data-path="app-94-remedies.html"><a href="app-94-remedies.html"><i class="fa fa-check"></i><b>E.2</b> Possible remedies</a><ul>
<li class="chapter" data-level="E.2.1" data-path="app-94-remedies.html"><a href="app-94-remedies.html#improve-scientific-rigor"><i class="fa fa-check"></i><b>E.2.1</b> Improve scientific rigor</a></li>
<li class="chapter" data-level="E.2.2" data-path="app-94-remedies.html"><a href="app-94-remedies.html#realigning-incentive-structures"><i class="fa fa-check"></i><b>E.2.2</b> Realigning incentive structures</a></li>
<li class="chapter" data-level="E.2.3" data-path="app-94-remedies.html"><a href="app-94-remedies.html#promote-transparency"><i class="fa fa-check"></i><b>E.2.3</b> Promote transparency</a></li>
</ul></li>
<li class="chapter" data-level="E.3" data-path="app-94-recap.html"><a href="app-94-recap.html"><i class="fa fa-check"></i><b>E.3</b> Chapter summary</a></li>
<li class="chapter" data-level="E.4" data-path="app-94-resources.html"><a href="app-94-resources.html"><i class="fa fa-check"></i><b>E.4</b> Further resources</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-03-05-hypothesis-testing-tests" class="section level2">
<h2><span class="header-section-number">16.6</span> Selected tests</h2>
<p>This section captures a selection of commonly used frequentist tests.</p>
<div id="ch-03-05-hypothesis-testing-Pearsons-Chi" class="section level3">
<h3><span class="header-section-number">16.6.1</span> Pearson’s <span class="math inline">\(\chi^2\)</span>-tests</h3>
<div style="float:right; width:16%;">
<p><img src="visuals/badge-BLJM.png" alt="badge-BLJM"></p>
</div>
<p>There are many tests that use the <a href="selected-continuous-distributions-of-random-variables.html#app-91-distributions-chi2"><span class="math inline">\(\chi^2\)</span>-distribution</a> as an (approximate) sampling distribution. But given relevance and historical prominence, the name “<span class="math inline">\(\chi^2\)</span>-test” is usually interpreted to refer to one of several flavor’s of what we could specifically call “Pearson’s <span class="math inline">\(\chi^2\)</span>-test”.</p>
<p>We will look at two flavors here. Pearson’s <span class="math inline">\(\chi^2\)</span>-test for <strong>goodness of fit</strong> tests whether an observed vector of counts is well explained by a given vector of predicted proportion. Pearson’s <span class="math inline">\(\chi^2\)</span>-test for <strong>independence</strong> tests whether a (two-dimensional) table of counts could plausibly have been generated by a process of independently selecting the column and the row category. We will explain how both of these tests work based on an application of the <a href="app-93-data-sets-BLJM.html#app-93-data-sets-BLJM">BLJM data</a>, which we load as usual:</p>
<div class="sourceCode" id="cb621"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb621-1" data-line-number="1">data_BLJM_processed &lt;-<span class="st"> </span>aida<span class="op">::</span>data_BLJM</a></code></pre></div>
<p>The focus is on the counts of music-subject choices:</p>
<div class="sourceCode" id="cb622"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb622-1" data-line-number="1">BLJM_associated_counts &lt;-<span class="st"> </span>data_BLJM_processed <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb622-2" data-line-number="2"><span class="st">  </span><span class="kw">select</span>(submission_id, condition, response) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb622-3" data-line-number="3"><span class="st">  </span><span class="kw">pivot_wider</span>(<span class="dt">names_from =</span> condition, <span class="dt">values_from =</span> response) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb622-4" data-line-number="4"><span class="st">  </span><span class="co"># drop the Beach-vs-Mountain condition</span></a>
<a class="sourceLine" id="cb622-5" data-line-number="5"><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>BM) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb622-6" data-line-number="6"><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">count</span>(JM,LB) </a>
<a class="sourceLine" id="cb622-7" data-line-number="7">BLJM_associated_counts</a></code></pre></div>
<pre><code>## # A tibble: 4 x 3
##   JM    LB          n
##   &lt;chr&gt; &lt;chr&gt;   &lt;int&gt;
## 1 Jazz  Biology    38
## 2 Jazz  Logic      26
## 3 Metal Biology    20
## 4 Metal Logic      18</code></pre>
<p>Remember that the lecturer’s bold conjecture was that a preference for Logic over Biology goes together with a preference for Metal over Jazz. The visualization suggests that there might be such a trend, but the (statistical) jury is still out as to whether this conjecture has empirical support.</p>
<div id="pearsons-chi2-test-for-goodness-of-fit" class="section level4">
<h4><span class="header-section-number">16.6.1.1</span> Pearson’s <span class="math inline">\(\chi^2\)</span>-test for goodness of fit</h4>
<p>“Goodness of fit” is a term used in model checking (a.k.a. model criticism, model validation, …). In such a context, tests for goodness-of-fit investigate whether a model’s predictions are compatible with the observed data. Pearson’s <span class="math inline">\(\chi^2\)</span>-test for goodness of fit does exactly this for categorical data.</p>
<p>Categorical data is data where each data observation falls into one of several unordered categories. If we have <span class="math inline">\(k\)</span> such categories, a <strong>prediction vector</strong> <span class="math inline">\(\vec{p} = \langle p_1, \dots, p_k \rangle\)</span> is a probability vector of length <span class="math inline">\(k\)</span> such that <span class="math inline">\(p_i\)</span> gives the probability with which a single data observation falls into the <span class="math inline">\(i\)</span>-th category. The likelihood of a single data observation is given by the <a href="selected-discrete-distributions-of-random-variables.html#app-91-distributions-categorical">Categorical distribution</a>, and the likelihood of <span class="math inline">\(N\)</span> data observations is given by the <a href="selected-discrete-distributions-of-random-variables.html#app-91-distributions-multinomial">Multinomial distribution</a>. These are generalizations of the Bernoulli and Binomial distributions, which expand the case of two unordered categories to more than two unordered categories.</p>
<p>The BLJM data supplies us with categorical data. Here is the vector of counts of how many participants selected a given music+subject pair:</p>
<div class="sourceCode" id="cb624"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb624-1" data-line-number="1"><span class="co"># add category names</span></a>
<a class="sourceLine" id="cb624-2" data-line-number="2">BLJM_associated_counts &lt;-<span class="st"> </span>BLJM_associated_counts <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb624-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(</a>
<a class="sourceLine" id="cb624-4" data-line-number="4">    <span class="dt">category =</span> <span class="kw">str_c</span>(</a>
<a class="sourceLine" id="cb624-5" data-line-number="5">      BLJM_associated_counts <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(LB),</a>
<a class="sourceLine" id="cb624-6" data-line-number="6">      <span class="st">&quot;-&quot;</span>,</a>
<a class="sourceLine" id="cb624-7" data-line-number="7">      BLJM_associated_counts <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(JM)</a>
<a class="sourceLine" id="cb624-8" data-line-number="8">    )</a>
<a class="sourceLine" id="cb624-9" data-line-number="9">  )</a>
<a class="sourceLine" id="cb624-10" data-line-number="10">counts_BLJM_choice_pairs_vector &lt;-<span class="st"> </span>BLJM_associated_counts <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(n)</a>
<a class="sourceLine" id="cb624-11" data-line-number="11"><span class="kw">names</span>(counts_BLJM_choice_pairs_vector) &lt;-<span class="st"> </span>BLJM_associated_counts <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(category)</a>
<a class="sourceLine" id="cb624-12" data-line-number="12">counts_BLJM_choice_pairs_vector</a></code></pre></div>
<pre><code>##  Biology-Jazz    Logic-Jazz Biology-Metal   Logic-Metal 
##            38            26            20            18</code></pre>
<p>Figure <a href="ch-03-05-hypothesis-testing-tests.html#fig:ch-03-04-BLJM-count-pairs-plot">16.10</a> shows a crude plot of these counts, together with a baseline prediction of equal proportion in each category.</p>
<div class="figure" style="text-align: center"><span id="fig:ch-03-04-BLJM-count-pairs-plot"></span>
<img src="I2DA_files/figure-html/ch-03-04-BLJM-count-pairs-plot-1.png" alt="Observed counts of choice pairs of music+subject preference in the BLJM data." width="672" />
<p class="caption">
Figure 16.10: Observed counts of choice pairs of music+subject preference in the BLJM data.
</p>
</div>
<p>Pearson’s <span class="math inline">\(\chi^2\)</span>-test for goodness of fit allows us to test whether this data could plausibly have been generated by (a model whose predictions are given by) a prediction vector <span class="math inline">\(\vec{p} = \langle p_1, \dots, p_4 \rangle\)</span>, where <span class="math inline">\(p_1\)</span> would be the predicted probability of a choice pair “Biology-Jazz” occurring for a single participant, and so on. Frequently, this test is used to check whether an equal baseline distribution could have generated the data. We do that here, too. We form the null hypothesis that <span class="math inline">\(\vec{p} = \vec{p}_0\)</span> with <span class="math inline">\(p_{0i} = \frac{1}{4}\)</span> for all categories <span class="math inline">\(i\)</span>.</p>
<p>Figure <a href="ch-03-05-hypothesis-testing-tests.html#fig:ch-03-04-chi2-model-goodness">16.11</a> shows a graphical representation of the model implicitly assumed in the background for a Pearson’s <span class="math inline">\(\chi^2\)</span>-test for goodness of fit. The model assumes that the observed vector of counts (like our <code>counts_BLJM_choice_pairs_vector</code> from above) follows a Multinomial distribution.<a href="#fn84" class="footnote-ref" id="fnref84"><sup>84</sup></a> Each vector of (hypothetical) data is associated with a test statistic, called <span class="math inline">\(\chi^2\)</span>, which sums over the standardized squared deviation of the observed counts from the predicted baseline in each cell. It can be shown that, if the number of observations <span class="math inline">\(N\)</span> is large enough, the sampling distribution of the <span class="math inline">\(\chi^2\)</span> test statistic is approximated well enough by the <a href="selected-continuous-distributions-of-random-variables.html#app-91-distributions-chi2"><span class="math inline">\(\chi^2\)</span>-distribution</a> with <span class="math inline">\(k-1\)</span> degrees of freedom (where <span class="math inline">\(k\)</span> is the number of categories).<a href="#fn85" class="footnote-ref" id="fnref85"><sup>85</sup></a> Notice that the approximation by a <span class="math inline">\(\chi^2\)</span>-distribution hinges on an approximation, which is only met when there are enough samples (just as we needed in the CLT). A rule-of-thumb is that at most 20% of all cells should have expected frequencies below 5 in order for the test to be applicable, i.e., <span class="math inline">\(np_i &lt; 5\)</span> for all <span class="math inline">\(i\)</span> in Figure <a href="ch-03-05-hypothesis-testing-tests.html#fig:ch-03-04-chi2-model-goodness">16.11</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:ch-03-04-chi2-model-goodness"></span>
<img src="visuals/chi2-model-goodness.png" alt="Graphical representation of Pearson's $\chi^2$-test for goodness of fit (testing a vector of predicted proportion)." width="90%" />
<p class="caption">
Figure 16.11: Graphical representation of Pearson’s <span class="math inline">\(\chi^2\)</span>-test for goodness of fit (testing a vector of predicted proportion).
</p>
</div>
<p>We can compute the <span class="math inline">\(\chi^2\)</span>-value associated with the observed data <span class="math inline">\(t(D_{obs})\)</span> as follows:</p>
<div class="sourceCode" id="cb626"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb626-1" data-line-number="1"><span class="co"># observed counts</span></a>
<a class="sourceLine" id="cb626-2" data-line-number="2">n &lt;-<span class="st"> </span>counts_BLJM_choice_pairs_vector</a>
<a class="sourceLine" id="cb626-3" data-line-number="3"><span class="co"># proportion predicted </span></a>
<a class="sourceLine" id="cb626-4" data-line-number="4">p &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">4</span>, <span class="dv">4</span>)</a>
<a class="sourceLine" id="cb626-5" data-line-number="5"><span class="co"># expected number in each cell</span></a>
<a class="sourceLine" id="cb626-6" data-line-number="6">e &lt;-<span class="st"> </span><span class="kw">sum</span>(n) <span class="op">*</span><span class="st"> </span>p</a>
<a class="sourceLine" id="cb626-7" data-line-number="7"><span class="co"># chi-squared for observed data</span></a>
<a class="sourceLine" id="cb626-8" data-line-number="8">chi2_observed &lt;-<span class="st"> </span><span class="kw">sum</span>((n <span class="op">-</span><span class="st"> </span>e)<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>e)</a>
<a class="sourceLine" id="cb626-9" data-line-number="9">chi2_observed</a></code></pre></div>
<pre><code>## [1] 9.529412</code></pre>
<p>We can then compare this value to the sampling distribution, which is a <span class="math inline">\(\chi^2\)</span>-distribution with <span class="math inline">\(k-1 = 3\)</span> degrees of freedom. We compute the <span class="math inline">\(p\)</span>-value associated with our data as the tail of the sampling distribution, as also shown in Figure <a href="ch-03-05-hypothesis-testing-tests.html#fig:ch-03-04-chi2-plot">16.12</a>:<a href="#fn86" class="footnote-ref" id="fnref86"><sup>86</sup></a></p>
<div class="sourceCode" id="cb628"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb628-1" data-line-number="1">p_value_BLJM &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pchisq</span>(chi2_observed, <span class="dt">df =</span> <span class="dv">3</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:ch-03-04-chi2-plot"></span>
<img src="I2DA_files/figure-html/ch-03-04-chi2-plot-1.png" alt="Sampling distribution for a Pearson's $\chi^2$-test of goodness of fit ($\chi^2$-distribution with $k-1 = 3$ degrees of freedom), testing a flat baseline null hypothesis based on the BLJM data." width="90%" />
<p class="caption">
Figure 16.12: Sampling distribution for a Pearson’s <span class="math inline">\(\chi^2\)</span>-test of goodness of fit (<span class="math inline">\(\chi^2\)</span>-distribution with <span class="math inline">\(k-1 = 3\)</span> degrees of freedom), testing a flat baseline null hypothesis based on the BLJM data.
</p>
</div>
<p>Of course, these calculations can also be performed by using a built-in R function, namely <code>chisq.test</code>:</p>
<div class="sourceCode" id="cb629"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb629-1" data-line-number="1">counts_BLJM_choice_pairs_vector &lt;-<span class="st"> </span>BLJM_associated_counts <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(n)</a>
<a class="sourceLine" id="cb629-2" data-line-number="2"><span class="kw">chisq.test</span>(counts_BLJM_choice_pairs_vector)</a></code></pre></div>
<pre><code>## 
##  Chi-squared test for given probabilities
## 
## data:  counts_BLJM_choice_pairs_vector
## X-squared = 9.5294, df = 3, p-value = 0.02302</code></pre>
<p>The common interpretation of our calculations would be to say that the test yielded a significant result, at least at the significance level of <span class="math inline">\(\alpha = 0.5\)</span>. In a research paper, we might report these results roughly as follows:</p>
<blockquote>
<p>Observed counts deviated significantly from what is expected if each category (here: pair of music+subject choice) was equally likely (<span class="math inline">\(\chi^2\)</span>-test, with <span class="math inline">\(\chi^2 \approx 9.53\)</span>, <span class="math inline">\(df = 3\)</span> and <span class="math inline">\(p \approx 0.023\)</span>).</p>
</blockquote>
<p>Notice that this test is an “omnibus test of difference”. We can conclude from a significant test result that the whole vector of observations is unlikely to have been generated by chance. Still, we cannot conclude from this result (without doing anything else) why, where or how the observations deviated from the assumed prediction vector. Looking at the plot of the data in Figure <a href="ch-03-05-hypothesis-testing-tests.html#fig:ch-03-04-BLJM-count-pairs-plot">16.10</a> above, it seems intuitive to think that Metal is disproportionally disfavored and that the combination of Biology and Jazz looks particularly outliery when compared to the baseline expectation.</p>
</div>
<div id="pearsons-chi2-test-of-independence" class="section level4">
<h4><span class="header-section-number">16.6.1.2</span> Pearson’s <span class="math inline">\(\chi^2\)</span>-test of independence</h4>
<p>The previous test of goodness of fit does not allow us to address the lecturer’s conjecture that a preference of Metal over Jazz goes with a preference of Logic over Biology. A slightly different kind of <span class="math inline">\(\chi^2\)</span>-test is better suited for this. In Pearson’s <span class="math inline">\(\chi^2\)</span>-test of independence, we look at a two-dimensional table of correlated data observations, like this one:</p>
<div class="sourceCode" id="cb631"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb631-1" data-line-number="1">BLJM_table &lt;-<span class="st"> </span>BLJM_associated_counts <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb631-2" data-line-number="2"><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>category) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb631-3" data-line-number="3"><span class="st">  </span><span class="kw">pivot_wider</span>(<span class="dt">names_from =</span> LB, <span class="dt">values_from =</span> n)</a>
<a class="sourceLine" id="cb631-4" data-line-number="4">BLJM_table</a></code></pre></div>
<pre><code>## # A tibble: 2 x 3
##   JM    Biology Logic
##   &lt;chr&gt;   &lt;int&gt; &lt;int&gt;
## 1 Jazz       38    26
## 2 Metal      20    18</code></pre>
<p>For easier computation and compatibility with the function <code>chisq.test</code>, we handle the same data but stored as a matrix:</p>
<div class="sourceCode" id="cb633"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb633-1" data-line-number="1">counts_BLJM_choice_pairs_matrix &lt;-<span class="st"> </span><span class="kw">matrix</span>(</a>
<a class="sourceLine" id="cb633-2" data-line-number="2">  counts_BLJM_choice_pairs_vector, </a>
<a class="sourceLine" id="cb633-3" data-line-number="3">  <span class="dt">nrow =</span> <span class="dv">2</span>, </a>
<a class="sourceLine" id="cb633-4" data-line-number="4">  <span class="dt">byrow =</span> T</a>
<a class="sourceLine" id="cb633-5" data-line-number="5">)</a>
<a class="sourceLine" id="cb633-6" data-line-number="6"><span class="kw">rownames</span>(counts_BLJM_choice_pairs_matrix) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Jazz&quot;</span>, <span class="st">&quot;Metal&quot;</span>)</a>
<a class="sourceLine" id="cb633-7" data-line-number="7"><span class="kw">colnames</span>(counts_BLJM_choice_pairs_matrix) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Biology&quot;</span>, <span class="st">&quot;Logic&quot;</span>)</a>
<a class="sourceLine" id="cb633-8" data-line-number="8">counts_BLJM_choice_pairs_matrix</a></code></pre></div>
<pre><code>##       Biology Logic
## Jazz       38    26
## Metal      20    18</code></pre>
<p>Pearson’s <span class="math inline">\(\chi^2\)</span>-test of independence addresses the question of whether two-dimensional tabular count data like the above could plausibly have been generated by a prediction vector <span class="math inline">\(\vec{p}\)</span>, which results from the assumption that the realizations of row- and column-choices are <a href="Chap-03-01-probability-conditional.html#Chap-03-01-probability-independence">stochastically independent</a>. If row- and column-choices are independent, the probability of seeing an outcome result in cell <span class="math inline">\(ij\)</span> is the probability of realizing row <span class="math inline">\(i\)</span> times the probability of realizing column <span class="math inline">\(j\)</span>. So, under an independence assumption, we expect a matrix and a resulting vector of choice proportions like this:</p>
<div class="sourceCode" id="cb635"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb635-1" data-line-number="1"><span class="co"># number of observations in total</span></a>
<a class="sourceLine" id="cb635-2" data-line-number="2">N &lt;-<span class="st"> </span><span class="kw">sum</span>(counts_BLJM_choice_pairs_matrix)</a>
<a class="sourceLine" id="cb635-3" data-line-number="3"><span class="co"># marginal proportions observed in the data </span></a>
<a class="sourceLine" id="cb635-4" data-line-number="4"><span class="co"># the following is the vector r in the model graph</span></a>
<a class="sourceLine" id="cb635-5" data-line-number="5">row_prob &lt;-<span class="st"> </span>counts_BLJM_choice_pairs_matrix <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">rowSums</span>() <span class="op">/</span><span class="st"> </span>N</a>
<a class="sourceLine" id="cb635-6" data-line-number="6"><span class="co"># the following is the vector c in the model graph</span></a>
<a class="sourceLine" id="cb635-7" data-line-number="7">col_prob &lt;-<span class="st"> </span>counts_BLJM_choice_pairs_matrix <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">colSums</span>() <span class="op">/</span><span class="st"> </span>N</a>
<a class="sourceLine" id="cb635-8" data-line-number="8"><span class="co"># table of expected observations under independence assumption</span></a>
<a class="sourceLine" id="cb635-9" data-line-number="9"><span class="co"># NB: %o% is the outer product of vectors</span></a>
<a class="sourceLine" id="cb635-10" data-line-number="10">BLJM_expectation_matrix &lt;-<span class="st"> </span>(row_prob <span class="op">%o%</span><span class="st"> </span>col_prob) <span class="op">*</span><span class="st"> </span>N </a>
<a class="sourceLine" id="cb635-11" data-line-number="11">BLJM_expectation_matrix</a></code></pre></div>
<pre><code>##        Biology    Logic
## Jazz  36.39216 27.60784
## Metal 21.60784 16.39216</code></pre>
<div class="sourceCode" id="cb637"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb637-1" data-line-number="1"><span class="co"># the following is the vector p in the model graph</span></a>
<a class="sourceLine" id="cb637-2" data-line-number="2">BLJM_expectation_vector &lt;-<span class="st"> </span><span class="kw">as.vector</span>(BLJM_expectation_matrix)</a>
<a class="sourceLine" id="cb637-3" data-line-number="3">BLJM_expectation_vector</a></code></pre></div>
<pre><code>## [1] 36.39216 21.60784 27.60784 16.39216</code></pre>
<p>Figure <a href="ch-03-05-hypothesis-testing-tests.html#fig:ch-03-04-chi2-model-independence">16.13</a> shows a graphical representation of the <span class="math inline">\(\chi^2\)</span>-test of independence. The main difference to the previous test of goodness of fit is that we do no longer just fix any-old prediction vector <span class="math inline">\(\vec{p}\)</span>, but consider <span class="math inline">\(\vec{p}\)</span> the deterministic results of independence <em>and</em> the best estimates (based on the data at hand) of the row- and column probabilities.</p>
<div class="figure" style="text-align: center"><span id="fig:ch-03-04-chi2-model-independence"></span>
<img src="visuals/chi2-model-independence.png" alt="Graphical representation of Pearson's $\chi^2$-test for independence." width="90%" />
<p class="caption">
Figure 16.13: Graphical representation of Pearson’s <span class="math inline">\(\chi^2\)</span>-test for independence.
</p>
</div>
<p>We can compute the observed <span class="math inline">\(\chi^2\)</span>-test statistic and the <span class="math inline">\(p\)</span>-value as follows:</p>
<div class="sourceCode" id="cb639"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb639-1" data-line-number="1">chi2_observed &lt;-<span class="st"> </span><span class="kw">sum</span>(</a>
<a class="sourceLine" id="cb639-2" data-line-number="2">  (counts_BLJM_choice_pairs_matrix <span class="op">-</span><span class="st"> </span>BLJM_expectation_matrix)<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span></a>
<a class="sourceLine" id="cb639-3" data-line-number="3"><span class="st">    </span>BLJM_expectation_matrix</a>
<a class="sourceLine" id="cb639-4" data-line-number="4">  )</a>
<a class="sourceLine" id="cb639-5" data-line-number="5">p_value_BLJM &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pchisq</span>(<span class="dt">q =</span> chi2_observed, <span class="dt">df =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb639-6" data-line-number="6"><span class="kw">round</span>(p_value_BLJM, <span class="dv">5</span>)</a></code></pre></div>
<pre><code>## [1] 0.50615</code></pre>
<p>Figure <a href="ch-03-05-hypothesis-testing-tests.html#fig:ch-03-04-chi2-plot-independence">16.14</a> shows the sampling distribution, the value of the test statistic for the observed data and the <span class="math inline">\(p\)</span>-value.</p>
<div class="figure" style="text-align: center"><span id="fig:ch-03-04-chi2-plot-independence"></span>
<img src="I2DA_files/figure-html/ch-03-04-chi2-plot-independence-1.png" alt="Sampling distribution for a Pearson's $\chi^2$ test of independence ($\chi^2$-distribution with $1$ degree of freedom), testing a flat baseline null hypothesis based on the BLJM data." width="90%" />
<p class="caption">
Figure 16.14: Sampling distribution for a Pearson’s <span class="math inline">\(\chi^2\)</span> test of independence (<span class="math inline">\(\chi^2\)</span>-distribution with <span class="math inline">\(1\)</span> degree of freedom), testing a flat baseline null hypothesis based on the BLJM data.
</p>
</div>
<p>We can also use the built-in function <code>chisq.test</code> in R to obtain this result more efficiently:</p>
<div class="sourceCode" id="cb641"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb641-1" data-line-number="1"><span class="kw">chisq.test</span>(</a>
<a class="sourceLine" id="cb641-2" data-line-number="2">  <span class="co"># supply data as a matrix, not as a vector, for a test of independence</span></a>
<a class="sourceLine" id="cb641-3" data-line-number="3">  counts_BLJM_choice_pairs_matrix, </a>
<a class="sourceLine" id="cb641-4" data-line-number="4">  <span class="co"># do not use the default correction (because we didn&#39;t introduce it)</span></a>
<a class="sourceLine" id="cb641-5" data-line-number="5">  <span class="dt">correct =</span> <span class="ot">FALSE</span></a>
<a class="sourceLine" id="cb641-6" data-line-number="6">)</a></code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  counts_BLJM_choice_pairs_matrix
## X-squared = 0.44202, df = 1, p-value = 0.5061</code></pre>
<p>With a <span class="math inline">\(p\)</span>-value of about 0.5061, we should conclude that there is no indication of strong evidence <em>against</em> the assumption of independence. Consequently, there is no evidence <em>in favor</em> of the lecturer’s conjecture of dependence of musical and academic preferences. In a research paper, we might report this result as follows:</p>
<blockquote>
<p>A <span class="math inline">\(\chi^2\)</span>-test of independence did not yield a significant test result (<span class="math inline">\(\chi^2\)</span>-test, with <span class="math inline">\(\chi^2 \approx 0.44\)</span>, <span class="math inline">\(df = 1\)</span> and <span class="math inline">\(p \approx 0.5\)</span>). Therefore, we cannot claim to have found any evidence for the research hypothesis of dependence.</p>
</blockquote>
<!-- exercise 2 -->
<!-- Taken from the prep exam (IDA-prep-exam-01.pdf) -->
<div class = "exercises">
<p><strong>Exercise 16.5: <span class="math inline">\(\chi^2\)</span>-test of independence</strong></p>
<p>Let us assume that there are two unordered categorical variables <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. Categorical variable <span class="math inline">\(A\)</span> has two levels <span class="math inline">\(a_1\)</span> and <span class="math inline">\(a_2\)</span>. Categorical variable <span class="math inline">\(B\)</span> has three levels <span class="math inline">\(b_1\)</span>, <span class="math inline">\(b_2\)</span> and <span class="math inline">\(b_3\)</span>. Let us further assume that the (marginal) probabilities of a choice from categories <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span> is as follows:</p>
<p><span class="math display">\[
P(A=a_i)=\begin{cases}
          0.3 &amp;\textbf{if \(i=1\)} \\
          0.7 &amp;\textbf{if \(i=2\)}
          \end{cases}
\quad P(B=b_i)=\begin{cases}
                0.2 &amp;\textbf{if \(i=1\)}\\
                0.3 &amp;\textbf{if \(i=2\)}\\
                0.5 &amp;\textbf{if \(i=3\)}
               \end{cases}
\]</span></p>
<ol style="list-style-type: lower-alpha">
<li>If observations of pairs of instances from categories <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are stochastically independent, what would the expected joint probability of each pair of potential observations be?</li>
</ol>
<div class="collapsibleSolution">
<button class="trigger">
Solution
</button>
<div class="content">

<div align="center">

<table style='width:70%'>
<thead>
<tr>
<th style="text-align:center;">
</th>
<th style="text-align:center;">
<span class="math inline">\(b_1\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(b_2\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(b_3\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
<span class="math inline">\(a_1\)</span>
</td>
<td style="text-align:center;">
.3 <span class="math inline">\(\times\)</span> .2 = .06
</td>
<td style="text-align:center;">
.3 <span class="math inline">\(\times\)</span> .3 = .09
</td>
<td style="text-align:center;">
.3 <span class="math inline">\(\times\)</span> .5 = .15
</td>
</tr>
<tr>
<td style="text-align:center;">
<span class="math inline">\(a_2\)</span>
</td>
<td style="text-align:center;">
.7 <span class="math inline">\(\times\)</span> .2 = .14
</td>
<td style="text-align:center;">
.7 <span class="math inline">\(\times\)</span> .3 = .21
</td>
<td style="text-align:center;">
.7 <span class="math inline">\(\times\)</span> .5 = .35
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<ol start="2" style="list-style-type: lower-alpha">
<li>Imagine you observe the following table of counts for each pair of instances of categories <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>:</li>
</ol>
<div align="center">

<table style='width:70%'>
<thead>
<tr>
<th style="text-align:center;">
</th>
<th style="text-align:center;">
<span class="math inline">\(b_1\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(b_2\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(b_3\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
<span class="math inline">\(a_1\)</span>
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
26
</td>
<td style="text-align:center;">
3
</td>
</tr>
<tr>
<td style="text-align:center;">
<span class="math inline">\(a_2\)</span>
</td>
<td style="text-align:center;">
19
</td>
<td style="text-align:center;">
4
</td>
<td style="text-align:center;">
47
</td>
</tr>
</tbody>
</table>
</div>
<p>   Which of the <span class="math inline">\(p\)</span>-values given below would you expect to see when feeding this table into a
   Pearson <span class="math inline">\(\chi^2\)</span>-test of independence? (only one correct answer)</p>
<ol type="i" start="1" style="margin-left: 2em;">
<li>
<span class="math inline">\(p \approx 1\)</span>
</li>
<li>
<span class="math inline">\(p \approx 0.5\)</span>
</li>
<li>
<span class="math inline">\(p \approx 0\)</span>
</li>
<li>
I expect no result because the test is not suitable for this kind of data.
</li>
</ol>
<div class="collapsibleSolution">
<button class="trigger">
Solution
</button>
<div class="content">
<p>The correct answer is <span class="math inline">\(p \approx 0\)</span>.</p>
</div>
</div>
<ol start="3" style="list-style-type: lower-alpha">
<li>Explain the answer you gave in the previous part in at most three concise sentences.</li>
</ol>
<div class="collapsibleSolution">
<button class="trigger">
Solution
</button>
<div class="content">
<p>As the marginal proportions of observed counts for the table in b. equal the marginal probabilities given above, the joint probability table in a. actually gives the predicted probabilities under the assumption of independence. Comparing prediction against observed proportion (obtained by dividing the table in b. by the total count of 100), we see severe divergences, especially in the middle column.</p>
</div>
</div>
</div>
</div>
</div>
<div id="ch-03-05-hypothesis-testing-z-test" class="section level3">
<h3><span class="header-section-number">16.6.2</span> <em>z</em>-test</h3>
<p>The Central Limit Theorem tells us that, given enough data, we can treat means of repeated samples from any arbitrary probability distribution as approximately normally distributed. Notice in addition that if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables following a normal distribution, then so is <span class="math inline">\(Z = X - Y\)</span> (see also the <a href="selected-continuous-distributions-of-random-variables.html#app-91-distributions-normal">chapter on the normal distribution</a>). It now becomes clear how research questions about means and differences between means (e.g., in the Mental Chronometry experiment) can be addressed, at least approximately: We conduct tests that hinge on a sampling distribution which is a normal distribution (usually a standard normal distribution).</p>
<p>The <span class="math inline">\(z\)</span>-test is perhaps the simplest of a family of tests that rely on normality of the sampling distribution. Unfortunately, what makes it so simple is also what makes it inapplicable in a wide range of cases. The <span class="math inline">\(z\)</span>-test assumes that a quantity that is normally distributed has an unknown mean (to be inferred by testing), but it also assumes that the <em>variance is known</em>. Since we do not know the variance in most cases of practical relevance, the <span class="math inline">\(z\)</span>-test needs to be replaced by a more adequate test, usually a test from the <span class="math inline">\(t\)</span>-test family, to be discussed below.</p>
<p>We start with the <span class="math inline">\(z\)</span>-test nonetheless because of the added benefit to our understanding. Figure <a href="ch-03-05-hypothesis-testing-tests.html#fig:ch-03-04-z-test-model">16.15</a> shows the model that implicitly underlies a <span class="math inline">\(z\)</span>-test. It checks whether the data <span class="math inline">\(\vec{x}\)</span>, which are assumed to be normally distributed with known <span class="math inline">\(\sigma\)</span>, could have been generated by a hypothesized mean <span class="math inline">\(\mu = \mu_0\)</span>. The sampling distribution of the derived test statistic <span class="math inline">\(z\)</span> is a standard normal distribution.</p>
<div class="figure" style="text-align: center"><span id="fig:ch-03-04-z-test-model"></span>
<img src="visuals/z-test-model.png" alt="Graphical representation of a $z$-test." width="90%" />
<p class="caption">
Figure 16.15: Graphical representation of a <span class="math inline">\(z\)</span>-test.
</p>
</div>
<p>We know that IQ test results are normally distributed around a mean of 100 with a standard deviation of 15. This holds when the sample is representative of the whole population. But suppose we have reason to believe that the sample is from CogSci students. The standard deviation in a sample from CogSci students might still plausibly be fixed to 15, but we’d like to test the assumption that <em>this</em> sample was generated by a mean <span class="math inline">\(\mu = 100\)</span>, our null hypothesis.</p>
<p>For illustration, suppose we observed the following data set of IQ test results:</p>
<div class="sourceCode" id="cb643"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb643-1" data-line-number="1"><span class="co"># fictitious IQ data</span></a>
<a class="sourceLine" id="cb643-2" data-line-number="2">IQ_data &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">87</span>, <span class="dv">91</span>, <span class="dv">93</span>, <span class="dv">97</span>, <span class="dv">100</span>, <span class="dv">101</span>, <span class="dv">103</span>, <span class="dv">104</span>, </a>
<a class="sourceLine" id="cb643-3" data-line-number="3">             <span class="dv">104</span>, <span class="dv">105</span>, <span class="dv">105</span>, <span class="dv">106</span>, <span class="dv">108</span>, <span class="dv">110</span>, <span class="dv">111</span>, </a>
<a class="sourceLine" id="cb643-4" data-line-number="4">             <span class="dv">112</span>, <span class="dv">114</span>, <span class="dv">115</span>, <span class="dv">119</span>, <span class="dv">121</span>)</a>
<a class="sourceLine" id="cb643-5" data-line-number="5"><span class="kw">mean</span>(IQ_data)</a></code></pre></div>
<pre><code>## [1] 105.3</code></pre>
<p>The mean of this data set is 105.3. Suspicious!</p>
<p>Following the model in Figure <a href="ch-03-05-hypothesis-testing-tests.html#fig:ch-03-04-z-test-model">16.15</a>, we calculate the value of the test statistic for the observed data.</p>
<div class="sourceCode" id="cb645"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb645-1" data-line-number="1"><span class="co"># number of observations</span></a>
<a class="sourceLine" id="cb645-2" data-line-number="2">N &lt;-<span class="st"> </span><span class="kw">length</span>(IQ_data)</a>
<a class="sourceLine" id="cb645-3" data-line-number="3"><span class="co"># null hypothesis to test</span></a>
<a class="sourceLine" id="cb645-4" data-line-number="4">mu_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb645-5" data-line-number="5"><span class="co"># standard deviation (known/assumed as true)</span></a>
<a class="sourceLine" id="cb645-6" data-line-number="6">sd &lt;-<span class="st"> </span><span class="dv">15</span></a>
<a class="sourceLine" id="cb645-7" data-line-number="7">z_observed &lt;-<span class="st"> </span>(<span class="kw">mean</span>(IQ_data) <span class="op">-</span><span class="st"> </span>mu_<span class="dv">0</span>) <span class="op">/</span><span class="st"> </span>(sd <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(N))</a>
<a class="sourceLine" id="cb645-8" data-line-number="8">z_observed <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">round</span>(<span class="dv">4</span>)</a></code></pre></div>
<pre><code>## [1] 1.5802</code></pre>
<p>We focus on a one-sided <span class="math inline">\(p\)</span>-value because our “research” hypothesis is that CogSci students have, on average, a higher IQ. Since we observed a mean of 105.3 in the data, which is higher than the critical value of 100, we test the null hypothesis <span class="math inline">\(\mu = 100\)</span> against an alternative hypothesis that assumes that the data was generated by a mean <em>bigger</em> than 100 (which is exactly our research hypothesis).</p>
<p>As before, we can then compute the <span class="math inline">\(p\)</span>-value by checking the area under the sampling distribution, here a standard normal, in the appropriate way. Figure <a href="ch-03-05-hypothesis-testing-tests.html#fig:ch-03-04-z-test">16.16</a> shows this result graphically.</p>
<div class="sourceCode" id="cb647"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb647-1" data-line-number="1">p_value_IQ_data_ztest &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(z_observed)</a>
<a class="sourceLine" id="cb647-2" data-line-number="2">p_value_IQ_data_ztest <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">round</span>(<span class="dv">6</span>)</a></code></pre></div>
<pre><code>## [1] 0.057036</code></pre>
<div class="figure" style="text-align: center"><span id="fig:ch-03-04-z-test"></span>
<img src="I2DA_files/figure-html/ch-03-04-z-test-1.png" alt="Sampling distribution for a $z$-test, testing the null hypothesis based on the assumption that the IQ-data was generated by $\mu = 100$ (with assumed/known $\sigma$)." width="90%" />
<p class="caption">
Figure 16.16: Sampling distribution for a <span class="math inline">\(z\)</span>-test, testing the null hypothesis based on the assumption that the IQ-data was generated by <span class="math inline">\(\mu = 100\)</span> (with assumed/known <span class="math inline">\(\sigma\)</span>).
</p>
</div>
<p>We can also use a ready-made function for the <span class="math inline">\(z\)</span>-test. However, as the <span class="math inline">\(z\)</span>-test is so uncommon, it is not built into core R. We need to rely on the <code>BSDA</code> package to find the function <code>z.test</code>.</p>
<div class="sourceCode" id="cb649"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb649-1" data-line-number="1">BSDA<span class="op">::</span><span class="kw">z.test</span>(<span class="dt">x =</span> IQ_data, <span class="dt">mu =</span> <span class="dv">100</span>, <span class="dt">sigma.x =</span> <span class="dv">15</span>, <span class="dt">alternative =</span> <span class="st">&quot;greater&quot;</span>)</a></code></pre></div>
<pre><code>## 
##  One-sample z-Test
## 
## data:  IQ_data
## z = 1.5802, p-value = 0.05704
## alternative hypothesis: true mean is greater than 100
## 95 percent confidence interval:
##  99.78299       NA
## sample estimates:
## mean of x 
##     105.3</code></pre>
<p>The conclusion to be drawn from this test could be formulated in a research report as follows:</p>
<blockquote>
<p>We tested the null hypothesis of a mean equal to 100, assuming a known standard deviation of 15, in a one-sided <span class="math inline">\(z\)</span>-test against the alternative hypothesis that the data was generated by a mean greater than 100 (our research hypothesis). The test was not significant (<span class="math inline">\(N = 20\)</span>, <span class="math inline">\(z \approx 1.5802\)</span>, <span class="math inline">\(p \approx 0.05704\)</span>), giving us no indication of strong evidence against the assumption that the mean is at most 100.</p>
</blockquote>
</div>
<div id="ch-03-05-hypothesis-testing-t-test" class="section level3">
<h3><span class="header-section-number">16.6.3</span> <em>t</em>-tests</h3>
<p>In most practical applications where a <span class="math inline">\(z\)</span>-test might be useful, the standard deviation is not known. If unknown, it should also not lightly be fixed by clever guess-work. This is where the family of <span class="math inline">\(t\)</span>-tests comes in. We will look at two examples of these: the one-sample <span class="math inline">\(t\)</span>-test, which compares one set of samples to a fixed mean, and the two-sample <span class="math inline">\(t\)</span>-test, which compares the means of two sets of samples.</p>
<div id="one-sample-t-test" class="section level4">
<h4><span class="header-section-number">16.6.3.1</span> One-sample <span class="math inline">\(t\)</span>-test</h4>
<p>The simplest example of this family, namely a <span class="math inline">\(t\)</span>-test for one metric vector <span class="math inline">\(\vec{x}\)</span> of normally distributed observations, tests the null hypothesis that <span class="math inline">\(\vec{x}\)</span> was generated by some <span class="math inline">\(\mu = \mu_0\)</span> (just like the <span class="math inline">\(z\)</span>-test). However, unlike the <span class="math inline">\(z\)</span>-test, a one-sample <span class="math inline">\(t\)</span>-test does not assume that the standard deviation is known. It rather uses the observed data to obtain an estimate for this parameter. More concretely, a one-sample <span class="math inline">\(t\)</span>-test for <span class="math inline">\(\vec{x}\)</span> estimates the standard deviation in the usual way (see Chapter <a href="Chap-02-03-summary-statistics.html#Chap-02-03-summary-statistics">5</a>):</p>
<p><span class="math display">\[\hat{\sigma}_x = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (x_i - \mu_{\vec{x}})^2}\]</span></p>
<p>Figure <a href="ch-03-05-hypothesis-testing-tests.html#fig:ch-03-04-t-test-model-one-population">16.17</a> shows a graphical representation of a one-sample <span class="math inline">\(t\)</span>-test model. The light shading of the node for the standard deviation indicates that this parameter is estimated from the observed data. Importantly, the distribution of the test statistic <span class="math inline">\(t\)</span> is no longer well approximated by a normal distribution when the sample size is low. It is better captured by a <a href="selected-continuous-distributions-of-random-variables.html#app-91-distributions-students-t">Student’s <span class="math inline">\(t\)</span> distribution</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:ch-03-04-t-test-model-one-population"></span>
<img src="visuals/t-test-model-one-population.png" alt="Graphical representation of the model underlying a frequentist one-sample $t$-test. Notice that the lightly shaded node for the standard deviation represents that the value for this parameter is estimated from the data." width="60%" />
<p class="caption">
Figure 16.17: Graphical representation of the model underlying a frequentist one-sample <span class="math inline">\(t\)</span>-test. Notice that the lightly shaded node for the standard deviation represents that the value for this parameter is estimated from the data.
</p>
</div>
<p>Let’s revisit our IQ-data set from above to calculate a <span class="math inline">\(t\)</span>-test. Using a <span class="math inline">\(t\)</span>-test implies that we are now assuming that the standard deviation is actually unknown. We can calculate the value of the test statistic for the observed data and use this to compute a <span class="math inline">\(p\)</span>-value, much like in the case of the <span class="math inline">\(z\)</span>-test before.</p>
<div class="sourceCode" id="cb651"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb651-1" data-line-number="1">N &lt;-<span class="st"> </span><span class="kw">length</span>(IQ_data)</a>
<a class="sourceLine" id="cb651-2" data-line-number="2"><span class="co"># fix the null hypothesis</span></a>
<a class="sourceLine" id="cb651-3" data-line-number="3">mean_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb651-4" data-line-number="4"><span class="co"># unlike in a z-test, we use the sample to estimate the SD</span></a>
<a class="sourceLine" id="cb651-5" data-line-number="5">sigma_hat &lt;-<span class="st"> </span><span class="kw">sd</span>(IQ_data) </a>
<a class="sourceLine" id="cb651-6" data-line-number="6">t_observed &lt;-<span class="st"> </span>(<span class="kw">mean</span>(IQ_data) <span class="op">-</span><span class="st"> </span>mean_<span class="dv">0</span>) <span class="op">/</span><span class="st"> </span>sigma_hat <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(N)</a>
<a class="sourceLine" id="cb651-7" data-line-number="7">t_observed <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">round</span>(<span class="dv">4</span>)</a></code></pre></div>
<pre><code>## [1] 2.6446</code></pre>
<p>We calculate the relevant one-sided <span class="math inline">\(p\)</span>-value using the cumulative distribution function <code>pt</code> of the <span class="math inline">\(t\)</span>-distribution.</p>
<div class="sourceCode" id="cb653"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb653-1" data-line-number="1">p_value_t_test_IQ &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pt</span>(t_observed, <span class="dt">df =</span> N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb653-2" data-line-number="2">p_value_t_test_IQ <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">round</span>(<span class="dv">6</span>)</a></code></pre></div>
<pre><code>## [1] 0.007992</code></pre>
<div class="figure" style="text-align: center"><span id="fig:ch-03-04-t-test-one-sample"></span>
<img src="I2DA_files/figure-html/ch-03-04-t-test-one-sample-1.png" alt="Sampling distribution for a $t$-test, testing the null hypothesis that the IQ-data was generated by $\mu = 100$ (with unknown $\sigma$)." width="90%" />
<p class="caption">
Figure 16.18: Sampling distribution for a <span class="math inline">\(t\)</span>-test, testing the null hypothesis that the IQ-data was generated by <span class="math inline">\(\mu = 100\)</span> (with unknown <span class="math inline">\(\sigma\)</span>).
</p>
</div>
<p>Compare these calculations against the built-in function <code>t.test</code>:</p>
<div class="sourceCode" id="cb655"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb655-1" data-line-number="1"><span class="kw">t.test</span>(<span class="dt">x =</span> IQ_data, <span class="dt">mu =</span> <span class="dv">100</span>, <span class="dt">alternative =</span> <span class="st">&quot;greater&quot;</span>)</a></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  IQ_data
## t = 2.6446, df = 19, p-value = 0.007992
## alternative hypothesis: true mean is greater than 100
## 95 percent confidence interval:
##  101.8347      Inf
## sample estimates:
## mean of x 
##     105.3</code></pre>
<p>These results could be stated in a research report much like so:</p>
<blockquote>
<p>We tested the null hypothesis of a mean equal to 100, assuming an unknown standard deviation, using a one-sided, one-sample <span class="math inline">\(t\)</span>-test against the alternative hypothesis that the data was generated by a mean greater than 100 (our research hypothesis). The significant test result (<span class="math inline">\(N = 20\)</span>, <span class="math inline">\(t \approx 2.6446\)</span>, <span class="math inline">\(p \approx 0.007992\)</span>) suggests that the data provides strong evidence against the assumption that the mean is not bigger than 100.</p>
</blockquote>
<p>Notice that the conclusions we draw from the previous <span class="math inline">\(z\)</span>-test and this one-sample <span class="math inline">\(t\)</span>-test are quite different. Why is this so? Well, it is because we (cheekily) chose a data set <code>IQ_data</code> that was actually <em>not</em> generated by a normal distribution with a standard deviation of 15, contrary to what we said about IQ-scores normally having this standard deviation. The assumption about <span class="math inline">\(\sigma\)</span> fed into the <span class="math inline">\(z\)</span>-test was (deliberately!) wrong. The result of the <span class="math inline">\(t\)</span>-test, at least for this example, is better. The data in <code>IQ_data</code> are actually samples from <span class="math inline">\(\text{Normal}(105,10)\)</span>. This demonstrates why the one-sample <span class="math inline">\(t\)</span>-test is usually preferred over a <span class="math inline">\(z\)</span>-test: unshakable, true knowledge of <span class="math inline">\(\sigma\)</span> is very rare.</p>
</div>
<div id="two-sample-t-test-for-unpaired-data-with-equal-variance-and-unequal-sample-sizes" class="section level4">
<h4><span class="header-section-number">16.6.3.2</span> Two-sample <span class="math inline">\(t\)</span>-test (for unpaired data with equal variance and unequal sample sizes)</h4>
<div style="float:right; width:16%;">
<p><img src="visuals/badge-avocado.png" alt="badge-avocado"></p>
</div>
<p>The “mother of all experimental designs” compares two groups of measurements. We give a drug to one group of patients, a placebo to another. We take a metric measure (say, blood sugar level) and ask whether there is a difference between these two groups. Section <a href="ch-03-04-parameter-estimation.html#ch-03-04-parameter-estimation">9</a> introduced the <span class="math inline">\(T\)</span>-Test Model for a Bayesian approach. Here, we look at a corresponding model for a frequentist approach, a so-called two-sample <span class="math inline">\(t\)</span>-test. There are different kinds of such two-sample <span class="math inline">\(t\)</span>-tests. The differences lie, e.g., in whether we assume that both groups have equal variance, in whether the sample sizes are the same in both groups, or in whether observations are paired (e.g., as in a within-subjects design, where we get two measurements from each participant, one from each condition/group). Here, we focus on unpaired data (as from a between-subjects design), assume equal variance but (possibly) unequal sample sizes. The case we look at is the <a href="app-93-data-sets-avocado.html#app-93-data-sets-avocado">avocado data</a>, where we want to specifically investigate whether the weekly average price of organically grown avocados is higher than that of conventionally grown avocados.<a href="#fn87" class="footnote-ref" id="fnref87"><sup>87</sup></a></p>
<p>We here consider the preprocessed avocado data set (see Appendix Chapter <a href="app-93-data-sets-avocado.html#app-93-data-sets-avocado">D.5</a> for details on how this preprocessing was performed).</p>
<div class="sourceCode" id="cb657"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb657-1" data-line-number="1">avocado_data &lt;-<span class="st"> </span>aida<span class="op">::</span>data_avocado</a></code></pre></div>
<p>Remember that the distribution of prices looks as follows:</p>
<p><img src="I2DA_files/figure-html/unnamed-chunk-450-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>A graphical representation of the two-sample <span class="math inline">\(t\)</span>-test (for unpaired data with equal variance and unequal sample sizes), which we will apply to this case, is shown in Figure <a href="ch-03-05-hypothesis-testing-tests.html#fig:ch-03-04-t-test-model-two-populations">16.19</a>. The model assumes that we have two vectors of metric measurements <span class="math inline">\(\vec{x}_A\)</span> and <span class="math inline">\(\vec{x}_B\)</span>, with length <span class="math inline">\(n_A\)</span> and <span class="math inline">\(n_B\)</span>, respectively. These are the price measures for conventionally grown and for organically grown avocados. The model assumes that measures in both <span class="math inline">\(\vec{x}_A\)</span> and <span class="math inline">\(\vec{x}_B\)</span> are i.i.d. samples from a normal distribution. The mean of one group (group <span class="math inline">\(B\)</span> in the graph) is assumed to be some unknown <span class="math inline">\(\mu\)</span>. Interestingly, this parameter will cancel out eventually: the approximation of the sampling distribution turns out to be independent of this parameter.<a href="#fn88" class="footnote-ref" id="fnref88"><sup>88</sup></a> The mean of the other group (group <span class="math inline">\(A\)</span> in the graph) is computed as <span class="math inline">\(\mu + \delta\)</span>, so with some additive parameter <span class="math inline">\(\delta\)</span> indicating the difference between means of these groups. This <span class="math inline">\(\delta\)</span> is the main parameter of interest for inferences regarding hypotheses concerning differences between groups. Finally, the model assumes that both groups have the same standard deviation, an estimate of which is derived from the data (in a rather convoluted looking formula that is not important for our introductory concerns). As indicated in Figure <a href="ch-03-05-hypothesis-testing-tests.html#fig:ch-03-04-t-test-model-two-populations">16.19</a>, the sampling distribution for this model is an instance of Student’s <span class="math inline">\(t\)</span>-distribution with mean 0, standard deviation 1 and degrees of freedom <span class="math inline">\(\nu\)</span> given as <span class="math inline">\(n_A + n_B - 2\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:ch-03-04-t-test-model-two-populations"></span>
<img src="visuals/t-test-model-two-populations.png" alt="Graphical representation of the model underlying a frequentist two-population $t$-test (for unpaired data with equal variance and unequal sample sizes). Notice that the light shading of the node for the standard deviation indicates that the value for this parameter is estimated from the data." width="90%" />
<p class="caption">
Figure 16.19: Graphical representation of the model underlying a frequentist two-population <span class="math inline">\(t\)</span>-test (for unpaired data with equal variance and unequal sample sizes). Notice that the light shading of the node for the standard deviation indicates that the value for this parameter is estimated from the data.
</p>
</div>
<p>Figure <a href="ch-03-05-hypothesis-testing-tests.html#fig:ch-03-04-t-test-model-two-populations">16.19</a> gives us the template to compute the value of the test statistic for the observed data:</p>
<div class="sourceCode" id="cb658"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb658-1" data-line-number="1"><span class="co"># fix the null hypothesis: no difference between groups</span></a>
<a class="sourceLine" id="cb658-2" data-line-number="2">delta_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb658-3" data-line-number="3"><span class="co"># data (group A)</span></a>
<a class="sourceLine" id="cb658-4" data-line-number="4">x_A &lt;-<span class="st"> </span>avocado_data <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb658-5" data-line-number="5"><span class="st">  </span><span class="kw">filter</span>(type <span class="op">==</span><span class="st"> &quot;organic&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(average_price)</a>
<a class="sourceLine" id="cb658-6" data-line-number="6"><span class="co"># data (group B)</span></a>
<a class="sourceLine" id="cb658-7" data-line-number="7">x_B &lt;-<span class="st"> </span>avocado_data <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb658-8" data-line-number="8"><span class="st">  </span><span class="kw">filter</span>(type <span class="op">==</span><span class="st"> &quot;conventional&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(average_price)</a>
<a class="sourceLine" id="cb658-9" data-line-number="9"><span class="co"># sample mean for organic (group A)</span></a>
<a class="sourceLine" id="cb658-10" data-line-number="10">mu_A &lt;-<span class="st"> </span><span class="kw">mean</span>(x_A)</a>
<a class="sourceLine" id="cb658-11" data-line-number="11"><span class="co"># sample mean for conventional (group B)</span></a>
<a class="sourceLine" id="cb658-12" data-line-number="12">mu_B &lt;-<span class="st"> </span><span class="kw">mean</span>(x_B)</a>
<a class="sourceLine" id="cb658-13" data-line-number="13"><span class="co"># numbers of observations</span></a>
<a class="sourceLine" id="cb658-14" data-line-number="14">n_A &lt;-<span class="st"> </span><span class="kw">length</span>(x_A)</a>
<a class="sourceLine" id="cb658-15" data-line-number="15">n_B &lt;-<span class="st"> </span><span class="kw">length</span>(x_B)</a>
<a class="sourceLine" id="cb658-16" data-line-number="16"><span class="co"># variance estimate</span></a>
<a class="sourceLine" id="cb658-17" data-line-number="17">sigma_AB &lt;-<span class="st"> </span><span class="kw">sqrt</span>(</a>
<a class="sourceLine" id="cb658-18" data-line-number="18">  ( ((n_A <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(x_A)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>(n_B <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(x_B)<span class="op">^</span><span class="dv">2</span> ) <span class="op">/</span><span class="st"> </span></a>
<a class="sourceLine" id="cb658-19" data-line-number="19"><span class="st">      </span>(n_A <span class="op">+</span><span class="st"> </span>n_B <span class="op">-</span><span class="st"> </span><span class="dv">2</span>) ) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">/</span>n_A <span class="op">+</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>n_B)</a>
<a class="sourceLine" id="cb658-20" data-line-number="20">)</a>
<a class="sourceLine" id="cb658-21" data-line-number="21">t_observed &lt;-<span class="st"> </span>(mu_A <span class="op">-</span><span class="st"> </span>mu_B <span class="op">-</span><span class="st"> </span>delta_<span class="dv">0</span>) <span class="op">/</span><span class="st"> </span>sigma_AB</a>
<a class="sourceLine" id="cb658-22" data-line-number="22">t_observed  </a></code></pre></div>
<pre><code>## [1] 105.5878</code></pre>
<p>We can use the value of the test statistic for the observed data to compute a one-sided <span class="math inline">\(p\)</span>-value, as before. Notice that we use a one-sided test because we hypothesize that organically grown avocados are more expensive, not just that they have a different price (more expensive or cheaper).</p>
<div class="sourceCode" id="cb660"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb660-1" data-line-number="1">p_value_t_test_avocado &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pt</span>(<span class="dt">q =</span> t_observed, <span class="dt">df =</span> n_A <span class="op">+</span><span class="st"> </span>n_B <span class="op">-</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb660-2" data-line-number="2">p_value_t_test_avocado</a></code></pre></div>
<pre><code>## [1] 0</code></pre>
<p>Owing to number imprecision, the calculated <span class="math inline">\(p\)</span>-value comes up as a flat zero. We have a lot of data, and the task of defending that conventionally grown avocados are not less expensive than organically grown is very tough. This also shows in the corresponding picture in Figure <a href="ch-03-05-hypothesis-testing-tests.html#fig:ch-03-04-t-test-two-sample">16.20</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:ch-03-04-t-test-two-sample"></span>
<img src="I2DA_files/figure-html/ch-03-04-t-test-two-sample-1.png" alt="Sampling distribution for a two-sample $t$-test, testing the null hypothesis of no difference between groups, based on the avocado data." width="90%" />
<p class="caption">
Figure 16.20: Sampling distribution for a two-sample <span class="math inline">\(t\)</span>-test, testing the null hypothesis of no difference between groups, based on the avocado data.
</p>
</div>
<p>We can also, of course, calculate this test result with the built-in function <code>t.test</code>:</p>
<div class="sourceCode" id="cb662"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb662-1" data-line-number="1"><span class="kw">t.test</span>(</a>
<a class="sourceLine" id="cb662-2" data-line-number="2">  <span class="dt">x =</span> x_A,           <span class="co"># first vector of data measurements</span></a>
<a class="sourceLine" id="cb662-3" data-line-number="3">  <span class="dt">y =</span> x_B,           <span class="co"># second vector of data measurements</span></a>
<a class="sourceLine" id="cb662-4" data-line-number="4">  <span class="dt">paired =</span> <span class="ot">FALSE</span>,    <span class="co"># measurements are to be treated as unpaired</span></a>
<a class="sourceLine" id="cb662-5" data-line-number="5">  <span class="dt">var.equal =</span> <span class="ot">TRUE</span>,  <span class="co"># we assume equal variance in both groups</span></a>
<a class="sourceLine" id="cb662-6" data-line-number="6">  <span class="dt">mu =</span> <span class="dv">0</span>             <span class="co"># NH is delta = 0 (name &#39;mu&#39; is misleading!)</span></a>
<a class="sourceLine" id="cb662-7" data-line-number="7">)</a></code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  x_A and x_B
## t = 105.59, df = 18247, p-value &lt; 2.2e-16
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.4867522 0.5051658
## sample estimates:
## mean of x mean of y 
##  1.653999  1.158040</code></pre>
<p>The result could be reported as follows:</p>
<blockquote>
<p>We conducted a two-sample <span class="math inline">\(t\)</span>-test of differences of means (unpaired samples, equal variance, unequal sample sizes) to compare the average weekly price of conventionally grown avocados to that of organically grown avocados. The test result indicates a significant difference for the null hypothesis that conventionally grown avocados are not cheaper (<span class="math inline">\(N_A = 9123\)</span>, <span class="math inline">\(N_B = 9126\)</span>, <span class="math inline">\(t \approx 105.59\)</span>, <span class="math inline">\(p \approx 0\)</span>).</p>
</blockquote>
<!-- exercise 3 -->
<div class = "exercises">
<p><strong>Exercise 16.6: Two-sample <span class="math inline">\(t\)</span>-test</strong></p>
<p>Your fellow student is skeptical of her flatmate’s claim that pizzas from place <span class="math inline">\(A\)</span> have a smaller diameter than place <span class="math inline">\(B\)</span> (both pizzerias have just one pizza size, namely <span class="math inline">\(\varnothing\ 32\ cm\)</span>). She decides to test that claim with a two-sample <span class="math inline">\(t\)</span>-test and sets <span class="math inline">\(H_0: \mu_A = \mu_B\)</span> (<span class="math inline">\(\delta = 0\)</span>), <span class="math inline">\(H_a: \mu_A &lt; \mu_B\)</span>, <span class="math inline">\(\alpha = 0.05\)</span>. She then asks your class to always measure the pizza’s diameter if ordered from one of the two places. At the end of the semester, she has the following table:</p>
<div align="center">

<table style='width:70%'>
<thead>
<tr>
<th style="text-align:center;">
</th>
<th style="text-align:center;">
Pizzeria <span class="math inline">\(A\)</span>
</th>
<th style="text-align:center;">
Pizzeria <span class="math inline">\(B\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
mean
</td>
<td style="text-align:center;">
30.9
</td>
<td style="text-align:center;">
31.8
</td>
</tr>
<tr>
<td style="text-align:center;">
standard deviation
</td>
<td style="text-align:center;">
2.3
</td>
<td style="text-align:center;">
2
</td>
</tr>
<tr>
<td style="text-align:center;">
sample size
</td>
<td style="text-align:center;">
38
</td>
<td style="text-align:center;">
44
</td>
</tr>
</tbody>
</table>
</div>
<ol style="list-style-type: lower-alpha">
<li>How many degrees of freedom <span class="math inline">\(\nu\)</span> are there?</li>
</ol>
<div class="collapsibleSolution">
<button class="trigger">
Solution
</button>
<div class="content">
<p><span class="math inline">\(\nu = n_A+n_B-2 = 38+44-2 = 80\)</span> degrees of freedom.</p>
</div>
</div>
<ol start="2" style="list-style-type: lower-alpha">
<li>Given the table above, calculate the test statistic <span class="math inline">\(t\)</span>.</li>
</ol>
<div class="collapsibleSolution">
<button class="trigger">
Solution
</button>
<div class="content">
<p><span class="math display">\[
\hat{\sigma}=\sqrt{\frac{(n_A-1)\hat{\sigma}_A^2+(n_B-1)\hat{\sigma}^2_B}{n_A+n_B-2}(\frac{1}{n_A}+\frac{1}{n_B})}\\
\hat{\sigma}=\sqrt{\frac{37\cdot2.3^2+43\cdot2^2}{80}(\frac{1}{38}+\frac{1}{44})}\approx 0.47\\
t=((\bar{x}_A-\bar{x}_B)-\delta)\cdot\frac{1}{\hat{\sigma}}\\
t=\frac{30.9-31.8}{0.47}\approx -1.91
\]</span></p>
</div>
</div>
<ol start="3" style="list-style-type: lower-alpha">
<li>Look at this so-called <a href="http://www.ttable.org/">t table</a> and determine the critical value to be exceeded in order to get a statistically significant result. NB: We are looking for the critical value that is on the <em>left</em> side of the distribution. So, in order to have a statistically significant result, the test statistic from b. has to be smaller than the <em>negated</em> critical value in the table.</li>
</ol>
<div class="collapsibleSolution">
<button class="trigger">
Solution
</button>
<div class="content">
<p>The critical value is -1.664.</p>
</div>
</div>
<ol start="4" style="list-style-type: lower-alpha">
<li>Compare the test statistic from b. with the critical value from c. and interpret the result.</li>
</ol>
<div class="collapsibleSolution">
<button class="trigger">
Solution
</button>
<div class="content">
<p>The calculated test statistic from b. is smaller than the critical value. We therefore know that the <span class="math inline">\(p\)</span>-value is statistically significant. The fellow student should reject the null hypothesis of equal pizza diameters.</p>
</div>
</div>
</div>
</div>
</div>
<div id="ch-03-05-hypothesis-testing-ANOVA" class="section level3">
<h3><span class="header-section-number">16.6.4</span> ANOVA</h3>
<p>ANOVA is short for “analysis of variance”.
It’s an umbrella term for a number of different models centered around testing the influence of one or several categorical predictors on a metric measurement.
In previous sections, we have summoned regression models for this task.
This is indeed the more modern and preferred approach, especially when the regression modeling also takes random effects (so-called hierarchical modeling) into account.
Nonetheless, it is good to have a basic understanding of ANOVAs, as they are featured prominently in a lot of published research papers, whose findings are still relevant.
Also, in some areas of empirical science, ANOVAs are still commonly used.</p>
<p>Here we are just going to cover the most basic type of ANOVA, which is called a <em>one-way ANOVA</em>.
A one-way ANOVA is, in regression jargon, a suitable approach for the case of a single categorical predictor with more than two levels (otherwise a <span class="math inline">\(t\)</span>-test would be enough) and a metric dependent variable.
For illustration we will here consider a fictitious case of metric measurement for three groups: A, B, and C.
These groups are levels of a categorical predictor <code>group</code>.
We want to address the research question of whether the means of the measurements of groups A, B and C could plausibly be identical.</p>
<p>The main idea behind analysis of variance is <em>not</em> to look at the means of measurements to be compared, but rather to compare the <em>between-group variances</em> to the <em>within-group variances</em>.
Whence the name “analysis of variance”.
While mathematically complex, the idea is quite intuitive.
Figure <a href="ch-03-05-hypothesis-testing-tests.html#fig:ch-05-01-examples-F-score">16.21</a> shows four different (made-up) data sets, each with different measurements for groups A, B and C.
It also shows the “pooled data”, i.e., the data from all three groups combined.
What is also shown in each panel is the so-called F-statistic, which is a number derived from a sample in the following way.</p>
<p>We have <span class="math inline">\(k \ge 2\)</span> groups of metric observations.
For group <span class="math inline">\(1 \le j \le k\)</span>, there are <span class="math inline">\(n_j\)</span> observations.
Let <span class="math inline">\(x_{ij}\)</span> be the observation <span class="math inline">\(1 \le i \le n_j\)</span> for group <span class="math inline">\(1 \le j \le k\)</span>.
Let <span class="math inline">\(\bar{x}_j = \frac{1}{n_j} \sum_{i = 1}^{n_j} x_{ij}\)</span> be the mean of group <span class="math inline">\(j\)</span> and let <span class="math inline">\(\bar{\bar{x}} = \frac{1}{k} \sum_{j=1}^k \frac{1}{n_j} \sum_{i=1}^{n_j} x_{ij}\)</span> be the grand mean of all data points.
The <strong>between-group variance</strong> measures how much, on average, the mean of each group deviates from the grand mean of all data points (where distance is squared distance, as usual):</p>
<p><span class="math display">\[
\hat{\sigma}_{\mathrm{between}} = \frac{\sum_{j=1}^k n_j (\bar{x}_j - \bar{\bar{x}})^2}{k-1}
\]</span>
The <strong>within-group variance</strong> is a measure of the average variance of the data points inside of each group:</p>
<p><span class="math display">\[
\hat{\sigma}_{\mathrm{within}} = \frac{\sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2}{\sum_{i=1}^k (n_i - 1)}
\]</span></p>
<p>Now, if the means of different groups are rather different from each other, the between-group variance should be high.
But absolute numbers may be misleading, so we need to scale the between-group variance also by how much variance we see, on average, in each group, i.e., the within-group variance.
That is why the <span class="math inline">\(F\)</span>-statistic is defined as:</p>
<p><span class="math display">\[
F = \frac{\hat{\sigma}_{\mathrm{between}}}{\mathrm{\hat{\sigma}_{\mathrm{within}}}}
\]</span>
For illustration, Figure <a href="ch-03-05-hypothesis-testing-tests.html#fig:ch-05-01-examples-F-score">16.21</a> shows four different scenarios with associated measures of <span class="math inline">\(F\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:ch-05-01-examples-F-score"></span>
<img src="I2DA_files/figure-html/ch-05-01-examples-F-score-1.png" alt="Different examples of metric measurements for three groups (A, B, C), shown here together with a plot of the combined (= pooled) data. We see that, as the means of measurements go apart, so does the ratio of between-group variance and within-group variance." width="672" />
<p class="caption">
Figure 16.21: Different examples of metric measurements for three groups (A, B, C), shown here together with a plot of the combined (= pooled) data. We see that, as the means of measurements go apart, so does the ratio of between-group variance and within-group variance.
</p>
</div>
<p>It can be shown that, under the assumption that the <span class="math inline">\(k\)</span> groups have identical means, the sampling distribution of the <span class="math inline">\(F\)</span> statistic follows an <a href="selected-continuous-distributions-of-random-variables.html#app-91-distributions-F"><span class="math inline">\(F\)</span>-distribution</a> with appropriate parameters (which is, unsurprisingly, the distribution constructed for exactly this purpose):</p>
<p><span class="math display">\[
F \sim F\mathrm{\text{-}distribution}\left(k - 1, \sum_{i=1}^k (n_i - 1) \right)
\]</span></p>
<p>The complete frequentist model of a one-way ANOVA is shown in Figure <a href="ch-03-05-hypothesis-testing-tests.html#fig:ch-05-01-ANOVA-onway-model">16.22</a>.
Notice that the null hypothesis of equal means is not shown explicitly, but rather only a single mean <span class="math inline">\(\mu\)</span> is shown, which functions as the mean for all groups.</p>
<div class="figure" style="text-align: center"><span id="fig:ch-05-01-ANOVA-onway-model"></span>
<img src="visuals/anova-oneway-model.png" alt="Graphical representation of the model underlying a one-way ANOVA." width="60%" />
<p class="caption">
Figure 16.22: Graphical representation of the model underlying a one-way ANOVA.
</p>
</div>
<p>Let’s consider some concrete, but fictitious data for a full example:</p>
<div class="sourceCode" id="cb664"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb664-1" data-line-number="1"><span class="co"># fictitious data</span></a>
<a class="sourceLine" id="cb664-2" data-line-number="2">x_A &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">78</span>, <span class="dv">43</span>, <span class="dv">60</span>, <span class="dv">60</span>, <span class="dv">60</span>, <span class="dv">50</span>, <span class="dv">57</span>, <span class="dv">58</span>, <span class="dv">64</span>, <span class="dv">64</span>, <span class="dv">56</span>, <span class="dv">62</span>, <span class="dv">66</span>, <span class="dv">53</span>, <span class="dv">59</span>)</a>
<a class="sourceLine" id="cb664-3" data-line-number="3">x_B &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">52</span>, <span class="dv">53</span>, <span class="dv">51</span>, <span class="dv">49</span>, <span class="dv">64</span>, <span class="dv">60</span>, <span class="dv">45</span>, <span class="dv">50</span>, <span class="dv">55</span>, <span class="dv">65</span>, <span class="dv">76</span>, <span class="dv">62</span>, <span class="dv">62</span>, <span class="dv">45</span>)</a>
<a class="sourceLine" id="cb664-4" data-line-number="4">x_C &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">78</span>, <span class="dv">66</span>, <span class="dv">74</span>, <span class="dv">57</span>, <span class="dv">75</span>, <span class="dv">64</span>, <span class="dv">64</span>, <span class="dv">53</span>, <span class="dv">63</span>, <span class="dv">60</span>, <span class="dv">79</span>, <span class="dv">68</span>, <span class="dv">68</span>, <span class="dv">47</span>, <span class="dv">63</span>, <span class="dv">67</span>)</a>
<a class="sourceLine" id="cb664-5" data-line-number="5"><span class="co"># number of observations in each group</span></a>
<a class="sourceLine" id="cb664-6" data-line-number="6">n_A &lt;-<span class="st"> </span><span class="kw">length</span>(x_A)</a>
<a class="sourceLine" id="cb664-7" data-line-number="7">n_B &lt;-<span class="st"> </span><span class="kw">length</span>(x_B) </a>
<a class="sourceLine" id="cb664-8" data-line-number="8">n_C &lt;-<span class="st"> </span><span class="kw">length</span>(x_C)</a>
<a class="sourceLine" id="cb664-9" data-line-number="9"><span class="co"># in tibble form</span></a>
<a class="sourceLine" id="cb664-10" data-line-number="10">anova_data &lt;-<span class="st"> </span><span class="kw">tibble</span>(</a>
<a class="sourceLine" id="cb664-11" data-line-number="11">  <span class="dt">condition =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb664-12" data-line-number="12">    <span class="kw">rep</span>(<span class="st">&quot;A&quot;</span>, n_A),</a>
<a class="sourceLine" id="cb664-13" data-line-number="13">    <span class="kw">rep</span>(<span class="st">&quot;B&quot;</span>, n_B),</a>
<a class="sourceLine" id="cb664-14" data-line-number="14">    <span class="kw">rep</span>(<span class="st">&quot;C&quot;</span>, n_C)</a>
<a class="sourceLine" id="cb664-15" data-line-number="15">    ),</a>
<a class="sourceLine" id="cb664-16" data-line-number="16">  <span class="dt">value =</span> <span class="kw">c</span>(x_A, x_B, x_C)</a>
<a class="sourceLine" id="cb664-17" data-line-number="17">)</a></code></pre></div>
<p>Here’s a plot of this data:</p>
<p><img src="I2DA_files/figure-html/unnamed-chunk-456-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We want to know whether it is plausible to entertain the idea that the means of these three groups are identical.
We can calculate the one-way ANOVA explicitly as follows, following the calculations described in Figure <a href="ch-03-05-hypothesis-testing-tests.html#fig:ch-05-01-ANOVA-onway-model">16.22</a>:</p>
<div class="sourceCode" id="cb665"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb665-1" data-line-number="1"><span class="co"># compute grand_mean </span></a>
<a class="sourceLine" id="cb665-2" data-line-number="2">grand_mean &lt;-<span class="st"> </span>anova_data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(value) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mean</span>()</a>
<a class="sourceLine" id="cb665-3" data-line-number="3"></a>
<a class="sourceLine" id="cb665-4" data-line-number="4"><span class="co"># compute degrees of freedom (parameters to F-distribution)</span></a>
<a class="sourceLine" id="cb665-5" data-line-number="5">df1 &lt;-<span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb665-6" data-line-number="6">df2 &lt;-<span class="st"> </span>n_A <span class="op">+</span><span class="st"> </span>n_B <span class="op">+</span><span class="st"> </span>n_C <span class="op">-</span><span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb665-7" data-line-number="7"></a>
<a class="sourceLine" id="cb665-8" data-line-number="8"><span class="co"># between-group variance</span></a>
<a class="sourceLine" id="cb665-9" data-line-number="9">between_group_variance &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">/</span>df1 <span class="op">*</span></a>
<a class="sourceLine" id="cb665-10" data-line-number="10"><span class="st">  </span>(</a>
<a class="sourceLine" id="cb665-11" data-line-number="11">    n_A <span class="op">*</span><span class="st"> </span>(<span class="kw">mean</span>(x_A) <span class="op">-</span><span class="st"> </span>grand_mean)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span></a>
<a class="sourceLine" id="cb665-12" data-line-number="12"><span class="st">    </span>n_B <span class="op">*</span><span class="st"> </span>(<span class="kw">mean</span>(x_B) <span class="op">-</span><span class="st"> </span>grand_mean)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span></a>
<a class="sourceLine" id="cb665-13" data-line-number="13"><span class="st">    </span>n_C <span class="op">*</span><span class="st"> </span>(<span class="kw">mean</span>(x_C) <span class="op">-</span><span class="st"> </span>grand_mean)<span class="op">^</span><span class="dv">2</span>  </a>
<a class="sourceLine" id="cb665-14" data-line-number="14">  )</a>
<a class="sourceLine" id="cb665-15" data-line-number="15">  </a>
<a class="sourceLine" id="cb665-16" data-line-number="16"><span class="co"># within-group variance</span></a>
<a class="sourceLine" id="cb665-17" data-line-number="17">within_group_variance &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">/</span>df2 <span class="op">*</span><span class="st"> </span></a>
<a class="sourceLine" id="cb665-18" data-line-number="18"><span class="st">  </span>(</a>
<a class="sourceLine" id="cb665-19" data-line-number="19">    <span class="kw">sum</span>((x_A <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(x_A))<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb665-20" data-line-number="20"><span class="st">    </span><span class="kw">sum</span>((x_B <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(x_B))<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb665-21" data-line-number="21"><span class="st">    </span><span class="kw">sum</span>((x_C <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(x_C))<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb665-22" data-line-number="22">  )</a>
<a class="sourceLine" id="cb665-23" data-line-number="23"><span class="co"># test statistic of observed data</span></a>
<a class="sourceLine" id="cb665-24" data-line-number="24">F_observed &lt;-<span class="st">  </span>between_group_variance <span class="op">/</span><span class="st"> </span>within_group_variance</a>
<a class="sourceLine" id="cb665-25" data-line-number="25"></a>
<a class="sourceLine" id="cb665-26" data-line-number="26"><span class="co"># retrieving the p-value (using the F-distribution)</span></a>
<a class="sourceLine" id="cb665-27" data-line-number="27">p_value_anova &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pf</span>(F_observed, <span class="dv">2</span>, n_A <span class="op">+</span><span class="st"> </span>n_B <span class="op">+</span><span class="st"> </span>n_C <span class="op">-</span><span class="st"> </span><span class="dv">3</span>) </a>
<a class="sourceLine" id="cb665-28" data-line-number="28">p_value_anova <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">round</span>(<span class="dv">4</span>)</a></code></pre></div>
<pre><code>## [1] 0.0172</code></pre>
<p>Compare this to the result of calling R’s built-in function <code>aov</code>:</p>
<div class="sourceCode" id="cb667"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb667-1" data-line-number="1"><span class="kw">aov</span>(<span class="dt">formula =</span> value <span class="op">~</span><span class="st"> </span>condition, anova_data) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summary</span>()</a></code></pre></div>
<pre><code>##             Df Sum Sq Mean Sq F value Pr(&gt;F)  
## condition    2  640.8   320.4   4.485 0.0172 *
## Residuals   42 3000.3    71.4                 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>To report these results, we could use a statement like this:</p>
<blockquote>
<p>Based on a one-way ANOVA, we find evidence against the assumption of equal means across all groups (<span class="math inline">\(F(2, 42) \approx 4.485\)</span>, <span class="math inline">\(p \approx 0.0172\)</span>).</p>
</blockquote>
<!-- We have $k \ge 2$ groups of metric observations. For group $1 \le j \le k$, there are $n_j$ observations. Let $x_{ij}$ be the observation $1 \le i \le n_j$ for group $1 \le j \le k$. Let $\bar{x}_j = \frac{1}{n_j} \sum_{i = 1}^{n_j} x_{ij}$ be the mean of group $j$ and let $\bar{\bar{x}} = \frac{1}{k} \sum_{j=1}^k \frac{1}{n_j} \sum_{i=1}^{n_j} x_{ij}$ be the grand mean of all data points. We would like to show that the total sum of squares can be decomposed into two summands: the within-group sum of squares and the between-group sum of squares: -->
<!-- $$  -->
<!-- \underbrace{ -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{\bar{x}})^2 -->
<!-- }_{\text{Total SS}}  =  -->
<!-- \underbrace{ -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2 -->
<!-- }_{\text{Within-Group SS}} + -->
<!-- \underbrace{ -->
<!--   \sum_{j=1}^k n_j (\bar{x}_j - \bar{\bar{x}})^2 -->
<!-- }_{\text{Between-Group SS}} -->
<!-- $$  -->
<!-- To show this, we first establish a lemma, which will also be useful later: -->
<!-- <div class = "mathstuff"> -->
<!-- ```{lemma, label = "SS-cancellation", name = "Sum of squares cancellation"} -->
<!-- Let $\vec{x}$ be a vector of $n$ real-valued numbers, and let $\bar{x} = \frac{1}{n} \sum_{i=i}^n x_i$ be its mean. The sum of squares around the mean is zero: -->
<!-- $$ -->
<!--   \sum_{i=1}^n (x_i - \bar{x}) = 0 -->
<!-- $$ -->
<!-- ``` -->
<!-- <div class="collapsibleProof"> -->
<!-- <button class="trigger">Proof</button> -->
<!-- <div class="content"> -->
<!-- ```{proof} -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!--   \sum_{i=i}^n (x_i - \bar{x})  -->
<!--   & =  -->
<!--   \sum_{i=i}^n (x_i - \frac{1}{n} \sum_{j=1}^n x_j)  -->
<!--   &&  -->
<!--   [\text{by def. of mean}]  -->
<!--   \\ -->
<!--   & =  -->
<!--   \sum_{i=i}^n x_i - \frac{n}{n} \sum_{j=1}^n x_j)  -->
<!--   &&  -->
<!--   [\text{second summand independent of } i]  -->
<!--   \\  -->
<!--   & = 0 -->
<!--   \\  -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- ``` -->
<!-- &nbsp; -->
<!-- </div> -->
<!-- </div> -->
<!-- </div> -->
<!-- <div class = "mathstuff"> -->
<!-- ```{proposition, label = "ANOVA-SS-decomposition", name = "Sum of squares decomposition (ANOVA)"} -->
<!-- If $x_{ij}$ is observation $1 \le i \le n_j$ for group $1 \le j \le k$, $\bar{x}_j = \frac{1}{n} \sum_{i = 1}^{n_j} x_{ij}$ the mean of group $j$ and $\bar{\bar{x}} = \frac{1}{k} \sum_{j=1}^k \frac{1}{n_j} \sum_{i=1}^{n_j} x_{ij}$ be the grand mean of all data points, then: -->
<!-- $$  -->
<!-- \underbrace{ -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{\bar{x}})^2 -->
<!-- }_{\text{Total SS}}  =  -->
<!-- \underbrace{ -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2 -->
<!-- }_{\text{Within-Group SS}} + -->
<!-- \underbrace{ -->
<!--   \sum_{j=1}^k n_j (\bar{x}_j - \bar{\bar{x}})^2 -->
<!-- }_{\text{Between-Group SS}} -->
<!-- $$  -->
<!-- ``` -->
<!-- <div class="collapsibleProof"> -->
<!-- <button class="trigger">Proof</button> -->
<!-- <div class="content"> -->
<!-- ```{proof} -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!--   &  -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{\bar{x}})^2  -->
<!--   \\ -->
<!--   = &   -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j + \bar{x}_j - \bar{\bar{x}})^2 -->
<!--   &&  -->
<!--   [- \bar{x}_j + \bar{x}_j = 0]  -->
<!--   \\ -->
<!--   = &   -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} \left [ (x_{ij} - \bar{x}_j)^2 + 2(x_{ij} - \bar{x}_j)(\bar{x}_j - \bar{\bar{x}}) + (\bar{x}_j - \bar{\bar{x}})^2 \right ] -->
<!--   &&  -->
<!--   [\text{binomial theorem}]  -->
<!--   \\ -->
<!--   = &   -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2 +  -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} (\bar{x}_j - \bar{\bar{x}})^2 +  -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} 2(x_{ij} - \bar{x}_j)(\bar{x}_j - \bar{\bar{x}}) -->
<!--   &&  -->
<!--   [\text{rearranging} ]  -->
<!--   \\ -->
<!--   = &   -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2 +  -->
<!--   \sum_{j=1}^k n_j (\bar{x}_j - \bar{\bar{x}})^2 +  -->
<!--   2 \sum_{j=1}^k (\bar{x}_j - \bar{\bar{x}}) \underbrace{\sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)}_{\text{=0 by lemma}} -->
<!--   &&  -->
<!--   [\text{independences} ]  -->
<!--   \\ -->
<!--     = &   -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2 +  -->
<!--   \sum_{j=1}^k n_j (\bar{x}_j - \bar{\bar{x}})^2 -->
<!--   &&  -->
<!--   [\text{by lemma} ]  -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- ``` -->
<!-- &nbsp; -->
<!-- </div> -->
<!-- </div> -->
<!-- </div> -->
</div>
<div id="linear-regression" class="section level3">
<h3><span class="header-section-number">16.6.5</span> Linear regression</h3>
<p>Significance testing for linear regression parameters follows the same logic as for other models as well.
In particular, it can be shown that the relevant test statistic for ML-estimates of regression coefficients <span class="math inline">\(\hat\beta_i\)</span>, under the assumption that the true model has <span class="math inline">\(\beta_i = 0\)</span>, follows a <span class="math inline">\(t\)</span>-distribution.
We can run a linear regression model (with a Gaussian noise function) using the built-in function <code>glm</code> (for “generalized linear model”):</p>
<div class="sourceCode" id="cb669"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb669-1" data-line-number="1">fit_murder_mle &lt;-<span class="st"> </span><span class="kw">glm</span>(</a>
<a class="sourceLine" id="cb669-2" data-line-number="2">  <span class="dt">formula =</span> murder_rate <span class="op">~</span><span class="st"> </span>low_income,</a>
<a class="sourceLine" id="cb669-3" data-line-number="3">  <span class="dt">data =</span> aida<span class="op">::</span>data_murder</a>
<a class="sourceLine" id="cb669-4" data-line-number="4">)</a></code></pre></div>
<p>If we inspect a summary for the model fit, we see the results of a <span class="math inline">\(t\)</span>-test, one for each coefficient, based on the null-hypothesis that this coefficient’s true value is 0.</p>
<div class="sourceCode" id="cb670"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb670-1" data-line-number="1"><span class="kw">summary</span>(fit_murder_mle)</a></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = murder_rate ~ low_income, data = aida::data_murder)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -9.1663  -2.5613  -0.9552   2.8887  12.3475  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -29.901      7.789  -3.839   0.0012 ** 
## low_income     2.559      0.390   6.562 3.64e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 30.38125)
## 
##     Null deviance: 1855.20  on 19  degrees of freedom
## Residual deviance:  546.86  on 18  degrees of freedom
## AIC: 128.93
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<p>So, in the case of the <code>murder_data</code>, we would conclude that there is strong evidence <em>against</em> the assumption that the data could have been generated by a model whose slope parameter for <code>low_income</code> is set to 0.</p>
</div>
<div id="Chap-05-01-LR-test" class="section level3">
<h3><span class="header-section-number">16.6.6</span> Likelihood-Ratio Test</h3>
<p>The likelihood-ratio (LR) test is a very popular frequentist method of model comparison.
The LR-test assimilates model comparison to frequentist hypothesis testing.
It defines a suitable test statistic and supplies an approximation of the sampling distribution.
The LR-test first and foremost applies to the comparison of <strong>nested models</strong>, but there are results about how approximate results can be obtained when comparing non-nested models with an LR-test <span class="citation">(Vuong <a href="#ref-Vuong1989:Likelihood-Rati">1989</a>)</span>.</p>
<p>A frequentist model <span class="math inline">\(M_i\)</span> is <strong>nested</strong> inside another frequentist model <span class="math inline">\(M_j\)</span> iff <span class="math inline">\(M_i\)</span> can be obtained from <span class="math inline">\(M_j\)</span> by fixing at least one of <span class="math inline">\(M_j\)</span>’s free parameters to a specific value.
If <span class="math inline">\(M_i\)</span> is nested under <span class="math inline">\(M_j\)</span>, <span class="math inline">\(M_i\)</span> is called the <strong>nested model</strong>, and <span class="math inline">\(M_j\)</span> is called the <strong>nesting model</strong> or the <strong>encompassing model</strong>.
Obviously, the nested model is simpler (of lower complexity) than the nesting model.</p>
<p>For example, we had the two-parameter exponential model of forgetting previously in Chapter <a href="Chap-03-06-model-comparison.html#Chap-03-06-model-comparison">10</a>:</p>
<p><span class="math display">\[
\begin{aligned}
P(D = \langle k, N \rangle \mid \langle a, b\rangle) &amp; = \text{Binom}(k,N, a \exp (-bt)), \ \ \ \ \text{where } a,b&gt;0 
\end{aligned}
\]</span></p>
<p>We wanted to explain the following “forgetting data”:</p>
<div class="sourceCode" id="cb672"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb672-1" data-line-number="1"><span class="co"># time after memorization (in seconds)</span></a>
<a class="sourceLine" id="cb672-2" data-line-number="2">t &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">9</span>, <span class="dv">12</span>, <span class="dv">18</span>)</a>
<a class="sourceLine" id="cb672-3" data-line-number="3"><span class="co"># proportion (out of 100) of correct recall</span></a>
<a class="sourceLine" id="cb672-4" data-line-number="4">y &lt;-<span class="st"> </span><span class="kw">c</span>(.<span class="dv">94</span>, <span class="fl">.77</span>, <span class="fl">.40</span>, <span class="fl">.26</span>, <span class="fl">.24</span>, <span class="fl">.16</span>)</a>
<a class="sourceLine" id="cb672-5" data-line-number="5"><span class="co"># number of observed correct recalls (out of 100)</span></a>
<a class="sourceLine" id="cb672-6" data-line-number="6">obs &lt;-<span class="st"> </span>y <span class="op">*</span><span class="st"> </span><span class="dv">100</span></a></code></pre></div>
<p>An example of a model that is nested under this two-parameter model is the following one-parameter model, which fixes <span class="math inline">\(a = 1.1\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
P(D = \langle k, N \rangle \mid b) &amp; = \text{Binom}(k,N, 1.1 \ \exp (-bt)), \ \ \ \ \text{where } b&gt;0 
\end{aligned}
\]</span></p>
<p>Here’s an ML-estimation for the nested nested model (the best fit for the nesting model <code>bestExpo</code> was obtained in Chapter <a href="Chap-03-06-model-comparison.html#Chap-03-06-model-comparison">10</a>):</p>
<div class="sourceCode" id="cb673"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb673-1" data-line-number="1">nLL_expo_nested &lt;-<span class="st"> </span><span class="cf">function</span>(b) {</a>
<a class="sourceLine" id="cb673-2" data-line-number="2">  <span class="co"># calculate predicted recall rates for given parameters</span></a>
<a class="sourceLine" id="cb673-3" data-line-number="3">  theta &lt;-<span class="st"> </span><span class="fl">1.1</span> <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>b <span class="op">*</span><span class="st"> </span>t)  <span class="co"># one-param exponential model </span></a>
<a class="sourceLine" id="cb673-4" data-line-number="4">  <span class="co"># avoid edge cases of infinite log-likelihood</span></a>
<a class="sourceLine" id="cb673-5" data-line-number="5">  theta[theta <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.0</span>] &lt;-<span class="st"> </span><span class="fl">1.0e-4</span></a>
<a class="sourceLine" id="cb673-6" data-line-number="6">  theta[theta <span class="op">&gt;=</span><span class="st"> </span><span class="fl">1.0</span>] &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="fl">1.0e-4</span></a>
<a class="sourceLine" id="cb673-7" data-line-number="7">  <span class="co"># return negative log-likelihood of data</span></a>
<a class="sourceLine" id="cb673-8" data-line-number="8">  <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(<span class="kw">dbinom</span>(<span class="dt">x =</span> obs, <span class="dt">prob =</span> theta, <span class="dt">size =</span> <span class="dv">100</span>, <span class="dt">log =</span> T))</a>
<a class="sourceLine" id="cb673-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb673-10" data-line-number="10"></a>
<a class="sourceLine" id="cb673-11" data-line-number="11">bestExpo_nested &lt;-<span class="st"> </span><span class="kw">optim</span>(</a>
<a class="sourceLine" id="cb673-12" data-line-number="12">  nLL_expo_nested, </a>
<a class="sourceLine" id="cb673-13" data-line-number="13">  <span class="dt">par =</span> <span class="fl">0.5</span>, </a>
<a class="sourceLine" id="cb673-14" data-line-number="14">  <span class="dt">method =</span> <span class="st">&quot;Brent&quot;</span>, </a>
<a class="sourceLine" id="cb673-15" data-line-number="15">  <span class="dt">lower =</span> <span class="dv">0</span>, </a>
<a class="sourceLine" id="cb673-16" data-line-number="16">  <span class="dt">upper =</span> <span class="dv">20</span></a>
<a class="sourceLine" id="cb673-17" data-line-number="17">)</a>
<a class="sourceLine" id="cb673-18" data-line-number="18">bestExpo_nested</a></code></pre></div>
<pre><code>## $par
## [1] 0.1372445
## 
## $value
## [1] 19.21569
## 
## $counts
## function gradient 
##       NA       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
<p>The LR-test looks at the likelihood ratio of the nested model <span class="math inline">\(M_0\)</span> over the encompassing model <span class="math inline">\(M_1\)</span> using the following test statistic:</p>
<p><span class="math display">\[\text{LR}(M_1, M_0) = -2\log \left(\frac{P_{M_0}(D_\text{obs} \mid \hat{\theta}_0)}{P_{M_1}(D_\text{obs} \mid \hat{\theta}_1)}\right)\]</span></p>
<p>We can calculate the value of this test statistic for the current example as follows:</p>
<div class="sourceCode" id="cb675"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb675-1" data-line-number="1">LR_observed &lt;-<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>bestExpo_nested<span class="op">$</span>value <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>bestExpo<span class="op">$</span>value</a>
<a class="sourceLine" id="cb675-2" data-line-number="2">LR_observed</a></code></pre></div>
<pre><code>## [1] 1.098429</code></pre>
<p>If the simpler (nested) model is true, the sampling distribution of this test statistic approximates a <span class="math inline">\(\chi^2\)</span>-distribution with <span class="math inline">\(d\)</span> if we have more and more data.
The degrees of freedom <span class="math inline">\(d\)</span> are given by the difference in free parameters, i.e., the number of parameters the nested model fixes to specific values, but which are free in the nesting model.</p>
<p>We can therefore calculate the <span class="math inline">\(p\)</span>-value for the LR-test for our current example like so:</p>
<div class="sourceCode" id="cb677"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb677-1" data-line-number="1">p_value_LR_test &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pchisq</span>(LR_observed, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb677-2" data-line-number="2">p_value_LR_test</a></code></pre></div>
<pre><code>## [1] 0.2946111</code></pre>
<p>The <span class="math inline">\(p\)</span>-value of this test quantifies the evidence against the assumption that the data was generated by the simpler model.
A significant test result would therefore indicate that it would be surprising if the data was generated by the simpler model.
This is usually taken as evidence in favor of the more complex, nesting model.
Given the current <span class="math inline">\(p\)</span>-value <span class="math inline">\(p \approx 0.2946\)</span>, we would conclude that there is no strong evidence against the simpler model.
Often this may lead researchers to favor the nested model due to its simplicity; the data at hand does not seem to warrant the added complexity of the nesting model; the nested model seems to suffice.</p>
<div class="exercises">
<p><strong>Exercise 16.7</strong></p>
<p>TRUE OR FALSE?</p>
<ol style="list-style-type: lower-alpha">
<li>The nested model usually has more free parameters than the nesting model.</li>
<li>When we perform the LR-test, we initially assume that the nested model is more plausible.</li>
<li>An LR-test can only compare the nested model with nesting models.</li>
<li>If the LR-test result has a <span class="math inline">\(p\)</span>-value equal to 1.0, one can conclude that it’s a piece of evidence in favor of the simpler model.</li>
</ol>
<div class="collapsibleSolution">
<button class="trigger">
Solution
</button>
<div class="content">
<ol style="list-style-type: lower-alpha">
<li>False</li>
<li>True</li>
<li>False</li>
<li>True</li>
</ol>
</div>
</div>
</div>

</div>
</div>
<!-- </div> -->
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Vuong1989:Likelihood-Rati">
<p>Vuong, Quang H. 1989. “Likelihood Ratio Tests for Model Selection and Non-Nested Hypotheses.” <em>Econometrica</em> 57 (2): 307–33.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="84">
<li id="fn84"><p>Notice that for economy of presentation, we now (again) gloss over the “raw” data of individual choices and present the summarized count data instead. In the previous case of the Binomial Test, it made good pedagogical sense to tease apart the “raw” observations from the summarized counts because this helped to show what the test statistic is for a case, where the choice of it was very, very obvious; so much so, that we would normally not even bother to make it explicit. Now that we understood what a test statistic is in principle, we can gloss over some steps of data summarizing.<a href="ch-03-05-hypothesis-testing-tests.html#fnref84" class="footnote-back">↩</a></p></li>
<li id="fn85"><p>A proof of this fact is non-trivial, but an intuition why this might be so is available if we think of each cell independently first. In each cell, with more and more samples, the distribution of counts will approximate a normal distribution by the CLT. The <span class="math inline">\(\chi^2\)</span>-distribution rests (by construction) on a sum of squared samples from a standard normal distribution.<a href="ch-03-05-hypothesis-testing-tests.html#fnref85" class="footnote-back">↩</a></p></li>
<li id="fn86"><p>Notice that this is a one-sided test due to the nature of the test statistic, which measures squared deviation from the baseline and not deviation in any particular direction (because it is hard to say what a “direction” would be in this case anyway).<a href="ch-03-05-hypothesis-testing-tests.html#fnref86" class="footnote-back">↩</a></p></li>
<li id="fn87"><p>Notice that the original avocado data set contains information also about the place of measurement, which would in principle allow us to treat the price measurements as paired samples (one pair for each week and place). For simplicity, but with a note of care that this makes us lose possibly relevant structural information, we here treat the avocado data as if it contained unpaired samples.<a href="ch-03-05-hypothesis-testing-tests.html#fnref87" class="footnote-back">↩</a></p></li>
<li id="fn88"><p>This is intuitively so because the test statistic is concerned only with the difference between sample means.<a href="ch-03-05-hypothesis-testing-tests.html#fnref88" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-05-01-frequentist-testing-confidence-intervals.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-05-02-comparison-freq-Bayes.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["I2DA.epub", "I2DA.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
