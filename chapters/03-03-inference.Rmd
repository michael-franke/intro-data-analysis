# Parameter estimation {#ch-03-04-parameter-estimation}

<hr>

<div style = "float:right; width:40%;">
<img src="visuals/badge-estimation.png" alt="badge estimation">  
</div>  

Based on a model $M$ with parameters $\theta$, parameter estimation addresses the question of which values of $\theta$ are good estimates, given some data $D$. Bayesian and frequentist approaches differ in what they consider a good, or the best estimate. Bayesian approaches take the prior into account, frequentist approaches do not (unsurprisingly). 

Parameter estimation is traditionally governed by two measures: (i) a point-estimate for the best parameter value, and (ii) an interval-estimate for a range of values that are considered "good enough". Table \@ref(tab:ch-03-03-estimation-overview) gives the most salient answers that each approach gives.

```{r ch-03-03-estimation-overview, echo = F}
table_data <- tribble(
  ~estimate, ~Bayesian, ~frequentist,
  "best value", "mean of posterior  posterior", "maximum likelihood estimate",
  "interval range", "credible interval (HDI)", "confidence interval"
)
knitr::kable(
  table_data,
  escape = F,
  caption = "Common methods of obtaining point-valued and interval-range estimates for parameters, given some data, in frequentist and Bayesian approaches.", 
  booktabs = TRUE
)
```


```{block, type='infobox'}
The learning goals for this chapter are:

- understand how Bayes rule applies to parameter estimation
  - role of prior and likelihood
  - understand notion of conjugate prior
- become familiar with and able to compute point-valued estimators
  - frequentist: MLE
  - Bayes: mean of posterior
- become familiar with interval-range estimators
  - frequentist: confidence intervals
  - Bayesian: credible intervals
- be able to implement probabilistic models in `greta` and compute with posterior samples 
```


## Bayes rule of parameter estimation  {#ch-03-03-estimation-bayes}

Fix a Bayesian model $M$ with likelihood $P(D \mid \theta)$ for observed data $D$ and prior over parameters $P_M(\theta)$. We then update our prior beliefs $P(\theta)$ to obtained posterior beliefs by Bayes rule:^[Since parameter estimation is only about one model, it should do not harm to omit the index $M$ in the probability notation. Moreover, since in many contexts the meaning will be  clear enough, we follow common practice and write $P(D \mid \theta)$ as a shortcut for $P(\mathcal{D} = D \mid \Theta = \theta)$. Here $\mathcal{D}$ is the class of all relevant observable data and $\Theta$ is the range of a possibly high-dimensional vector of parameter values. We diverge from common practice of using capital roman letters for random variables and small roman letters for values from these random variables, because parameter vectors are traditionally written as $\theta$ and the small letter $\textrm{d}$ (albeit non-italic) is reserved fro differentials.]

$$P(\theta \mid D) = \frac{P(D \mid \theta) \ P(\theta)}{P(D)}$$

The ingredients of this equation are:

- the **posterior distribution** $P(\theta \mid D)$ specifying our beliefs about how
  likely each value of $\theta$ is given fixed $k$;
- the **likelihood function** $P(D \mid \theta)$ specifying how likely each observation
  of $k$ is for a fixed $\theta$ (here given by the binomial distribution);
- the **prior distribution** $P(\theta)$ specifying our initial (aka.~*a priori*)
  beliefs about how likely each value of $\theta$ might be;
- the **marginal likelihood** $P(D) = \int P(D \mid \theta) \ P(\theta) \ \text{d}\theta$
  specifying how likely an observation of $k$ is under our prior beliefs about $\theta$.

A frequently used shorthand notation for probabilities is this:

$$\underbrace{P(\theta \, | \, D)}_{posterior} \propto \underbrace{P(\theta)}_{prior} \ \underbrace{P(D \, | \, \theta)}_{likelihood}$$

where the "proportional to" sign $\propto$ indicates that the probabilities on the LHS are defined in terms of the quantity on the RHS after normalization. So, if $F \colon x \mapsto \mathbb{R}^+$ is a positive function of non-normalized probabilities, $P(x) \propto F(x)$ is equivalent to $P(x) = \frac{F(x)}{\sum_{x'} F(x')}$.

This notation makes particularly clear that the posterior distribution is a "mix" of prior and likelihood. Let's explore this first, before worrying how to compute posteriors concretely.

### The effects of prior and likelihood on the posterior

We consider the case of flipping a coin with unknown bias $\theta$ a total of $N$ times and observing $k$ heads (= successes). This is modeled with the  **Binomial Model** (see Section \@ref(Chap-03-03-models-examples)), using priors expressed with a [Beta distribution](app-91-distributions-beta), giving us a model specification as: 

$$ 
\begin{aligned}
k & \sim \text{Binomial}(N, \theta) \\ 
\theta & \sim \text{Beta}(\alpha, \beta)
\end{aligned}
$$

<div style = "float:right; width:8%;">
<img src="visuals/skull_king.png" alt="badge-data-wrangling">  
</div>  

To study the impact of the likelihood function, we compare two data sets. The first one is the contrived "24/7" example where $N = 24$ and $k = 7$. The second example uses a much larger naturalistic data set stemming from the [King of France](app-93-data-sets-king-of-france) example, namely $k = 109$ for $N=311$. These numbers are the count of "true" responses for all conditions except for Condition 1, which did not involve a presupposition. 


```{r, echo = F}
data_KoF_cleaned <- read_csv('data_sets/king-of-france_data_cleaned.csv',
                             col_types = cols(
                               .default = col_character(),
                               submission_id = col_double(),
                               vignette = col_double(),
                               background_correct = col_logical(),
                               RT = col_double(),
                               age = col_double(),
                               item_version = col_double(),
                               correct_answer = col_logical(),
                               response = col_logical(),
                               timeSpent = col_double(),
                               trial_number = col_double(),
                               proportion_correct = col_double(),
                               outlier_subject = col_logical()
                             ))
```

```{r, eval = F}
data_KoF_cleaned <- read_csv(url('https://raw.githubusercontent.com/michael-franke/intro-data-analysis/master/data_sets/king-of-france_data_cleaned.csv'))
```

```{r}
data_KoF_cleaned %>% 
  filter(condition != "Condition 1") %>% 
  group_by(response) %>% 
  dplyr::count()
```


We can plot the likelihood function for both data sets like so:

```{r, echo = F, fig.cap = "Likelihood for the two examples of bimoial data."}
lh_tibble <- 
  tibble(
  theta = seq(0,1, length.out = 401),
  `24/5` = dbinom(7, 24, theta),
  `King of France` = dbinom(109, 311, theta)
) 

lh_tibble %>% 
  pivot_longer(
    cols = -1,
    names_to = "example",
    values_to = "likelihood"
  ) %>% 
  ggplot(aes(x = theta, y = likelihood, color = example)) +
  geom_line(size = 3) +
  facet_wrap(.~example, nrow = 2, scales = "free") +
  guides(color = F) +
    labs(
    x = latex2exp::TeX("Coin bias $\\theta$"),
    y = latex2exp::TeX("Prior probability $P_{M_i}(\\theta)$"),
    title = latex2exp::TeX("Binomial likelihood function for different data sets.")
  )
```


Picking up the example from Section \@ref(Chap-03-02-models-priors), we will consider the four types of priors show below:

```{r ch-03-03-estimation-types-of-priors, echo = F, fig.cap = "Examples of different kinds of Bayesian priors for the Binomial Model."}
prior_tibble <- tibble(
  theta = seq(0, 1, length.out = 401),
  objective = dbeta(theta, 0.5, 0.5),
  uninformative = dbeta(theta, 1, 1),
  `weakly informative` = dbeta(theta, 5, 2),
  `strongly informative` = dbeta(theta, 50, 20)
) 

prior_tibble %>% 
    pivot_longer(
    cols = -1,
    names_to = "prior_type",
    values_to = "prior"
  ) %>% 
  ggplot(aes(x = theta, y = prior)) +
  geom_line(size = 2) +
  facet_wrap(~ prior_type, ncol = 2, scales = "free") +
  labs(
    x = latex2exp::TeX("Coin bias $\\theta$"),
    y = latex2exp::TeX("Prior probability $P_{M_i}(\\theta)$"),
    title = latex2exp::TeX("Different kinds of priors over bias $\\theta$ (Binomial Model family).")
  ) 

```

Combining the four different priors and the two different data sets, we see that the posterior is indeed a mix of prior and likelihood. In particular we see that the strong informative prior has only little effect if there are many data points (the KoF data).

```{r ch-03-03-estimation-posteriors-comparison, echo = F, fig.cap = "Posterior beliefs over coin biases under different priors and different data sets."}
full_join(lh_tibble, prior_tibble, by = 'theta') %>% 
    pivot_longer(
    cols = 2:3,
    names_to = "example",
    values_to = "likelihood"
  ) %>% 
  pivot_longer(
    cols = 2:5,
    names_to = "prior_type",
    values_to = "prior"
  ) %>% 
  mutate(
    posterior = likelihood * prior
  ) %>%  
  filter( posterior != Inf) %>% 
  group_by( example, prior_type) %>% 
  mutate(
    posterior = posterior/sum(posterior)
  ) %>% 
  ggplot(aes(x = theta, y = posterior, color = example)) +
  geom_line(size = 2) +
  guides(color = F) +
  facet_grid(example ~ prior_type, scales = "free") +
  labs(
    x = latex2exp::TeX("Coin bias $\\theta$"),
    y = latex2exp::TeX("Posterior probability $P_{M_i}(\\theta \\, | \\, D)$"),
    title = latex2exp::TeX("Posterior beliefs in $\\theta$ for different priors and data.")
  ) 
```

### Posterior means and credible intervals

Let's consider the "24/7" example with a flat prior again, concisely repeated in Figure \@ref(fig:ch-03-03-estimation-24-7-overview).

```{r ch-03-03-estimation-24-7-overview, echo = F, fig.cap = "Prior (uninformative), likelihood and posterior for the 24/7 example.", fig.height = 12}

prior_plot_24_7 <- 
  tibble(
    theta = seq(0.01,1, by = 0.01),
    prior = dbeta(seq(0.01,1, by = 0.01), 1, 1 )
  ) %>% 
  ggplot(aes(x = theta, y = prior)) + 
  xlim(0,1) + 
  geom_line(color = project_colors[1], size = 2) + 
  labs(
    x = latex2exp::TeX("Coin bias $\\theta$"),
    y = latex2exp::TeX("Prior probability $P_{M}(\\theta)$"),
    title = "Prior"
  )

lh_plot_24_7 <- 
  tibble(
    theta = seq(0.01,1, by = 0.01),
    lh = dbinom(x = 7, size = 24, prob = theta)
  ) %>% 
  ggplot(aes(x = theta, y = lh)) + 
  xlim(0,1) + 
  geom_line(color = project_colors[2], size = 2) + 
  labs(
    x = latex2exp::TeX("Coin bias $\\theta$"),
    y = latex2exp::TeX("Likelihood $P_{M}(D \\, | \\, \\theta)$"),
    title = "Likelihood"
  )

hdi = HDInterval::hdi(qbeta , shape1 = 8 , shape2 = 18 )
hdiData <- tibble(
  theta = rep(hdi, each = 2),
  post = c(0,dbeta(hdi, 8, 18), 0)
)
expData <- tibble(
  theta = c(8/26,8/26),
  post = c(0,dbeta(8/26, 8, 18 ))
)

posterior_plot_24_7 <- 
  tibble(
    theta = seq(0.01,1, by = 0.01),
    posterior = dbeta(seq(0.01,1, by = 0.01), 8, 18 )
  ) %>% 
  ggplot(aes(x = theta, y = posterior)) + 
  xlim(0,1) + 
  labs(
    x = latex2exp::TeX("Coin bias $\\theta$"),
    y = latex2exp::TeX("Posterior probability $P_{M}(\\theta \\, | \\, D)$"),
    title = "Posterior"
  ) +
  geom_line(data = hdiData, aes(x = theta, y = post), color = "firebrick", size = 1.5) +
  geom_label(x = 0.7, y = 0.5, label = "Cred.Int.: 0.14 - 0.48", color = "firebrick", size = 5) +
  geom_line(data = expData, aes(x = theta, y = post), color = "darkblue", size = 1.5) +
  geom_label(x = 0.52, y = dbeta(8/26, 8, 18 ), label = "expectation: 0.308", color = "darkblue", size = 5) +
  geom_line(color = "black", size = 2)

cowplot::plot_grid(
  prior_plot_24_7,
  lh_plot_24_7,
  posterior_plot_24_7,
  ncol = 1
)
```


The posterior probability distribution in Figure \@ref(fig:ch-03-03-estimation-24-7-overview) contains rich information. It specifies how likely each value of $\theta$ is, obtained by updating the original prior beliefs with the observed data. Such rich information is difficult to process and communicate in language. It is therefore convenient to have conventional means of summarizing the rich information carried in a probability distribution like in Figure \@ref(fig:ch-03-03-estimation-24-7-overview). Customarily, we summarize in terms of a point-estimate and/or an interval estimate. The *point estimate* gives information about a "best value", i.e., a salient point, such as the expectation (in Bayesian approaches) or the most likely value (in frequentist approaches (see below)). The *interval estimate* gives, usually, an indication of how closely other *good values* are scattered around the *best value*.

A common Bayesian point estimate of coin bias parameter $\theta$ is **the mean of the posterior distribution**. It gives the value of $\theta$ which we would expect to see, when basing out expectations on the posterior distribution:
$$
\begin{aligned}
  \mathbb{E}_{P(\theta \mid D)} = \int \theta \ P(\theta \mid D) \ \text{d}\theta
\end{aligned}
$$
If we start with flat beliefs, the expected value of $\theta$ after $k$ successes in $N$ flips
can be calculated rather easily as $\frac{k+1}{n+2}$.^[This is also known as *Laplace's
  rule*, or the *rule of succession*.] For our example case, we calculate the expected value of
$\theta$ as $\frac{8}{26} \approx 0.308$ (see also Figure \@ref(fig:ch-03-03-estimation-24-7-overview)).

```{block2, type="infobox"}
**Remark: Maximum _a posteriori_** Another salient point-estimate to summarize a Bayesian posterior distribution is the MAP (maximum _a posteriori_). The map is the parameter value (tuple) that maximizes the posterior distribution:
  
$$ \text{MAP}(P(\theta \mid D)) =  \arg \max_\theta P(\theta \mid D) $$
  
While the mean of the posterior is "holistic" in the sense that it depends on the whole distribution, the MAP does not. The mean is therefore more faithful to the Bayesian ideal of taking the full posterior distribution into account. Moreover, depending on how Bayesian posteriors are computed / approximated, the estimation of a mean can be more reliable than that of a MAP.
```

A common Bayesian interval estimate of coin bias parameter $\theta$ is a **credible interval**.^[Also frequently called "highest-density intervals", even when we are dealing not with density but probability mass.] An interval $[l;u]$ is a $\gamma\%$ credible interval for a random variable $X$ if two conditions hold, namely
$$
\begin{aligned}
  P(l \le X \le u) = \frac{\gamma}{100}
\end{aligned}
$$
and, secondly, for every $x \in[l;u]$ and $x' \not \in[l;u]$ we have $P(X=x) > P(X = x')$. Intuitively, a $95\%$ credible interval gives the range of values in which we believe with
relatively high certainty that the true value resides. Figure \@ref(fig:ch-03-03-estimation-24-7-overview) indicates the $95\%$ credible interval, based on the posterior distribution $P(\theta \mid D)$ of $\theta$, for the 24/7 example.^[Not all random variables have a credible interval for a given $\gamma$, according to this definition. A bimodal distribution might not, for example. A bi-modal distribution has two regions of high probability. We can therefore generalize the concept to a finite set of disjoint convex *credible regions*, all of which have the second property of the definition above and all of which conjointly are realized with $\gamma\%$ probability. Unfortunately, common parlor uses the term "credible interval" to refer to credible regions as well. The same disaster occurs with alternative terms, such as "$\gamma\%$ highest-density intervals", which also often refers to what should better be called "highest-density regions".]

### Computing Bayesian posteriors with conjugate priors

Bayesian posterior distributions can be hard to compute. Usually, the prior $P(\theta)$ is easy to compute (otherwise we might choose a different one for practicality). Usually, the likelihood function $P(D \mid \theta)$ is also fast to compute. Everything seems innocuous when we just write:

$$\underbrace{P(\theta \, | \, D)}_{posterior} \propto \underbrace{P(\theta)}_{prior} \ \underbrace{P(D \, | \, \theta)}_{likelihood}$$

But the real pain is the normalizing constant, i.e., the marginalize likelihood a.k.a. the "integral of doom", which to compute can be intractable, especially if the parameter space is large and not well-behaved:

$$P(D) = \int P(D \mid \theta) \ P(\theta) \ \text{d}\theta$$

Section \@ref(Ch-03-03-estimation-algorithms) will therefore enlarge on methods to compute or approximate the posterior distribution efficiently.

Fortunately, computing Bayesian posterior distributions need not always be intractable. If the prior and the likelihood function cooperate, so to speak, the computation of the posterior can be as simple as sleep. The nature of the data often prescribes which likelihood function is plausible. But we have more wiggle room in the choice of the priors. If prior $P(\theta)$ and posterior $P(\theta \, | \, D)$ are of the same family, i.e., if they are the same kind of distribution albeit possibly with different parameterizations, we say that they **conjugate**. In that case, the prior $P(\theta)$ is called **conjugate prior** for the likelihood function $P(D \, | \, \theta)$. from which the posterior $P(\theta \, | \, D)$ is derived.

```{theorem, "Conjugacy-beta-binomial"}
The Beta distribution is the conjugate prior of binomial likelihood. For $\theta \sim \text{Beta}(a,b)$ as prior and data $k$ and $N$, the posterior is $\theta \sim \text{Beta}(a+k, b+ N-k)$.
```


```{proof}
By construction, the posterior is:
$$P(\theta \mid \langle{k, n \rangle}) \propto \text{Binomial}(k ; n, \theta) \ \text{Beta}(\theta \, | \, a, b) $$


We extend the RHS by definitions:

$$
\begin{aligned} 
\text{Binomial}(k ; n, \theta) \ \text{Beta}(\theta \, | \, a, b) & = \theta^{k} \, (1-\theta)^{n-k} \, \theta^{a-1} \, (1-\theta)^{b-1}  \\  & = \theta^{k + a - 1} \, (1-\theta)^{n-k +b -1}
\end{aligned}
$$

This latter expression is the non-normalized Beta-distribution for parameters $a + k$ and $b + N - k$, so that we conclude with what was to be shown:

$$
\begin{aligned} 
P(\theta \mid \tuple{k, n}) & = \text{Beta}(\theta \, | \, a + k, b+ N-k)
\end{aligned}
$$
```

&nbsp;

### Sequential updating

Ancient wisdom has coined the widely popular proverb: "Today's posterior is tomorrow's prior." Suppose we collected data from an experiment, like $k = 7$ in $N = 24$. Using uninformative priors at the outset, our posterior belief after the experiment is $\theta \sim \text{Beta}(8,18)$. But now consider what happened at half-time. After half the experiment, we had $k = 2$ and $N = 12$, so our beliefs followed $\theta \sim \text{Beta}(3, 10)$ at this moment in time. But using these beliefs as priors, and observing the rest of the data would consequently result in updating the prior $\theta \sim \text{Beta}(3, 10)$ with another set of observations $k = 5$ and $N = 12$, giving us the same posterior belief as what we would have gotten if we updated in one swoop. Figure \@ref(fig:ch-03-03-estimation-sequential-updates) shows steps through belief space, starting uninformed, and observing one piece of data at a time (going right for each outcome of heads, down for each outcome of tails).

```{r ch-03-03-estimation-sequential-updates, echo = F, fig.cap = "Beta distributions for different parameters. Starting from an uninformative prior (top left), we arrive at the posterior distribution in the bottom left, in any sequence of sequentially updating with the data.", fig.height = 8}
expand_grid(
    theta = seq(0,1,length.out = 41),
    a = 1:7,
    b = 1:4
  ) %>% 
  as_tibble() %>% 
  mutate(
    probability = dbeta(theta, a, b),
    parameters = str_c("a=", a, " b=", b)
  ) %>% 
  ggplot(aes(x = theta, y = probability)) +
  geom_line() +
  facet_wrap(~parameters, scales = "free", nrow = 7) +
  theme(
    axis.title.x=element_blank(),
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank(),
    axis.title.y=element_blank(),
    axis.text.y=element_blank(),
    axis.ticks.y=element_blank()
  )
```


This sequential updating is not a peculiarity of the Beta-Binomial case or of conjugacy. It holds in general for Bayesian inference. Sequential updating is a very intuitive property, but it is not shared by all other forms of inference from data. That Bayesian inference is sequential and commutative follows from commutativity of multiplication of likelihoods (and the definition of Bayes rule).

```{theorem, "Sequential-update"}
Bayesian posterior inference is sequential and commutative in the sense that for a data set $D$ which is comprised of two mutually exclusive subsets $D_1$ and $D_2$ such that $D_1 \cup D_2 = D$,we have:

$$ P(\theta \mid D ) \propto P(\theta \mid D_1) \ P(D_2 \mid \theta) $$  

```

```{proof}
$$
\begin{aligned}
P(\theta \mid D) & = \frac{P(\theta) \ P(D \mid \theta)}{ \int P(\theta') \ P(D \mid \theta') \text{d}\theta'} \\
& = \frac{P(\theta) \ P(D_1 \mid \theta) \ P(D_2 \mid \theta)}{ \int P(\theta') \ P(D_1 \mid \theta') \ P(D_2 \mid \theta') \text{d}\theta'} & \text{[from multiplicativity of likelihood]} \\
& = \frac{P(\theta) \ P(D_1 \mid \theta) \ P(D_2 \mid \theta)}{ \frac{k}{k} \int P(\theta') \ P(D_1 \mid \theta') \ P(D_2 \mid \theta') \text{d}\theta'} & \text{[for random positive k]} \\
& = \frac{\frac{P(\theta) \ P(D_1 \mid \theta)}{k} \ P(D_2 \mid \theta)}{\int \frac{P(\theta') \ P(D_1 \mid \theta')}{k} \ P(D_2 \mid \theta') \text{d}\theta'} & \text{[rules of integration; basic calculus]} \\
& = \frac{P(\theta \mid D_1) \ P(D_2 \mid \theta)}{\int P(\theta' \mid D_1) \ P(D_2 \mid \theta') \text{d}\theta'} & \text{[Bayes rule with } k = \int P(\theta) P(D_1 \mid \theta) \text{d}\theta ]\\
\end{aligned}
$$
```


&nbsp;


## A frequentist approach to parameter estimation {#ch-03-03-estimation-frequentist}



We pick up the "24/7" example again. As before, the goal is to draw inferences about the latent bias $\theta \in [0;1]$. Being a frequentist, we do that based only on likelihood function $P(k \mid \theta)$, given here by the binomial distribution as before. We will derive, from $P(k \mid \theta)$ alone, plausible point and interval estimates. There are several constructions for both point and interval estimates. We here only look at what seem to be the single most prominent exemplars in each category.

### Maximum likelihood estimate

The **maximum likelihood estimate (MLE)** is a point estimate based on the likelihood function alone. It specifies the value of $\theta$ for which the observed data is most likely. We often use the notation $\hat{\theta}$ to denote the MLE of $\theta$:

$$
\begin{aligned}
  \hat{\theta} = \arg \max_{\theta} P(d \mid \theta)
\end{aligned}
$$

For the binomial likelihood function, the maximum likelihood estimate is easy to calculate as $\frac{k}{N}$, yielding
$\frac{7}{24} \approx 0.292$ for the running example. Figure \@ref(fig:ch-03-03-estimation-MLE) shows a graph of the non-normalized likelihood function and indicates the maximum likelihood estimate (the value that maximizes the curve).

```{r ch-03-03-estimation-MLE, echo = F, fig.cap = "Non-normalized likelihood function for the observation of $k=7$ successes in $N=24$ flips, including maximum likelihood estimate."}
plotdata_24_7_mle <- tibble(
  theta = seq(0.01,1, by = 0.01),
  lh = dbinom(x = 7, size = 24, prob = theta)
  )
expData_27_7_mle <- tibble(
  theta = c(7/24,7/24),
  lh = c(0,dbinom(x=7,size=24,prob=7/24))
)
ggplot(plotdata_24_7_mle, aes(x = theta, y = lh)) + xlim(0,1) +
  geom_label(x = 0.52, y = dbinom(x=7,size=24,prob=7/24), label = "MLE: 0.292", color = project_colors[1], size = 5) +
  geom_line(data = expData_27_7_mle, aes(x = theta, y = lh), color = project_colors[1], size = 1.25) +
  geom_line(color = "black", size = 2) + ylab("likelihood")
```



### Confidence intervals

The most commonly used interval estimate in frequentist analyses are *confidence intervals*. Although (frequentist) confidence intervals *can* coincide with (subjectivist) credible intervals in specific cases, they generally do not. And even when confidence and credible values yield the same numerical results, these notions are fundamentally different and ought not to be confused.

Let's look at credible intervals to establish the proper contrast. Recall that part of the definition of a credible interval for a posterior distribution over $\theta$, captured here notationally in terms a random variable $\Theta$, was the probability $P(l \le \Theta \le u)$ that the value realized by random variable $\Theta$ lies in the interval $[l;u]$. This statement makes no sense to the frequentist. There cannot be any non-trivial value for $P(l \le \Theta \le u)$. The true value of $\theta$ is either in the interval $[l;u]$ or it is not. To speak of a probability that $\theta$ is in $[l;u]$ is to appeal to an ill-formed concept of probability which the frequentist denies.

In order to give an interval estimate nonetheless, the frequentist appeals to probabilities that she can accept: probabilities derived from (hypothetical) repetitions of a genuine random event with objectively observable outcomes. Let $\mathcal{D}$ be the random variable that captures the probability with which data $\mathcal{D}=D$ is realized. We obtain a pair of derived random variables $X_l$ and $X_u$ from a pair of functions $g_{l,u} \colon d \mapsto \mathbb{R}$. A **$\gamma\%$ confidence interval** for observed data $D_{\text{obs}}$ is the interval $[g_l(D_{\text{obs}}), g_u(D_{\text{obs}})]$ whenever functions $g_{l,u}$ are constructed in such a way that

$$
\begin{aligned}
  P(X_l \le \theta_{\text{true}} \le X_u) = \frac{\gamma}{100}
\end{aligned}
$$

where $\theta_{\text{true}}$ is the unknown but fixed true value of $\theta$. In more intuitive words, a confidence interval is the outcome of a special construction (functions $g_{l,u}$) such that, when applying this procedure repeatedly to outcomes of the assumed data-generating process, the true value of parameter $\theta$ will lie inside of the computed confidence interval in exactly $\gamma$\% of the cases.

It is easier to think of the definition of a confidence interval in terms of computer code and sampling (see Figure \@ref(fig:03-03-estimation-confidence-interval-scheme)). Suppose Grandma gives you computer code, a `magic_function` which takes as input data observations, and returns an interval estimate for the parameter of interest. We sample a value for the parameter of interest repeatedly, and consider it the "true parameter" for the time being. For each sampled "true parameter", we generate data repeatedly. We apply Grandma's `magic_function`, obtain an interval estimate and check if the true value that triggered the whole process is included in the interval. Grandma's `magic_function` is a $\gamma\%$ confidence interval if the proportion of inclusions (the check marks in Figure \@ref(fig:03-03-estimation-confidence-interval-scheme)) is $\gamma\%$.

```{r 03-03-estimation-confidence-interval-scheme, echo = F, fig.cap="Schematic representation of what a confidence interval does: think of it as a magic function which returns intervals that contain the true value in $\\gamma$ percent of the cases."}
knitr::include_graphics("visuals/confidence-interval.png")
```

In some complex cases, the frequentist analyst relies on functions $g_{l}$ and $g_{u}$ that are easy to compute but only approximately satisfy the condition $P(X_l \le \theta_{\text{true}} \le X_u) = \frac{\gamma}{100}$. For example, we might use an
asymptotically correct calculation, based on the observation that, if $n$ grows to infinity, the binomial distribution approximates a normal distribution. We can then calculate a confidence interval, *as if* our binomial distribution actually was a normal distribution. If $n$ is not large enough, this will be increasingly imprecise. Rules of thumb are used to decide how big $n$ has to be to involve at best a tolerable amount of imprecision (see the Info Box below). 

For our running example ($k = 7$, $n=24$), the rule of thumb mentioned in the Info Box below recommends *not* using the asymptotic calculation. If we did nonetheless, we would get a confidence interval of $[0.110; 0.474]$. For the binomial distribution also a more reliable calculation exist, which yields $[0.126; 0.511]$ for the running example. (We can use numeric simulation to explore how good/bad a particular approximate calculation is, as shown in the next section.) The more reliable construction, the so-called *exact method*, implemented in the function `binom.confint` of R package `binom`, revolves around the close relationship between confidence intervals and $p$-values. (To foreshadow a later discussion: the exact $\gamma\%$ confidence interval is the set of all parameter values for which an exact (binomial) test does not yield a significant test result as the level of $\alpha = 1-\frac{\gamma}{100}$.)


```{block, type='infobox'}
**Asymptotic approximation of a binomial confidence interval using a normal distribution.**

Let $X$ be the random variable that determines the binomial distribution, i.e., the probability of seeing $k$ successes in $n$ flips. For large $n$, $X$ approximates a normal distribution with a mean $\mu = n \ \theta$ and a standard deviation of    $\sigma = \sqrt{n \ \theta \ (1 - \theta)}$. The random variable $U$:
$$
  U = \frac{X - \mu}{\sigma} = \frac{X - n \  \theta}{\sqrt{n \  \theta \  (1-\theta)}}
$$
Let $\hat{P}$ be the random variable that captures the distribution of our maximum likelihood estimates for an observed outcome $k$:
$$
  \hat{P} = \frac{X}{n}
$$
Since $X = \hat{P} \  n$ we obtain:
$$
  U = \frac{\hat{P} \  n - n \  \theta}{\sqrt{n \  \theta \  (1-\theta)}}
$$
We now look at the probability that $U$ is realized to lie in a symmetric interval $[-c,c]$, centered around zero --- a probability which we require to match our confidence level:
$$
  P(-c \le U \le c) = \frac{\gamma}{100}
$$
We now expand the definition of $U$ in terms of $\hat{P}$, equate $\hat{P}$ with the current best estimate $\hat{p} = \frac{k}{n}$ based on the observed $k$ and rearrange terms, yielding the asymptotic approximation of a binomial confidence interval:
$$  
  \left [ \hat{p} - \frac{c}{n} \  \sqrt{n \  \hat{p} \  (1-\hat{p})} ; \ \ 
       \hat{p} + \frac{c}{n} \  \sqrt{n \  \hat{p} \  (1-\hat{p})} \right ]
$$
This approximation is conventionally considered precise enough when the following *rule of thumb* is met:

$$
n \ \hat{p} \ (1 - \hat{p}) > 9
$$
```


## Addressing point-valued hypotheses with parameter estimation {#ch-03-03-estimation-testing}

Using interval-based estimates, we can address research questions formulated as point-valued hypotheses about a parameter of interest. Let $\Theta$ be the parameter space of a model $M$. We are interested in some component $\Theta_i$ and our hypothesis is $\Theta_i = \theta^*_i$ for some specific value $\theta^*_i$. A simple (but crude and controversial) way of addressing this point-valued hypothesis based on observed data $D$ is to look at whether $\theta^*_i$ lies inside the interval-estimate for parameter $\Theta_i$ based on observed data $D$. This would apply for frequentist confidence intervals and Bayesian credible intervals alike. 

@kruschke2015 extends this approach to addressing point-valued hypothesis. He argues that we should *not* be concerned with point-valued hypotheses, but rather with intervals constructed around the point-value of interest. Kruschke therefore suggests to look at a **region of practical equivalence** (ROPE), usually defined by some $\epsilon$-region around $\theta^*_i$:

$$\text{ROPE}(\theta^*_i) = [\theta^*_i- \epsilon, \theta^*_i+ \epsilon]$$

The choice of $\epsilon$ is context-dependent and requires an understanding of the scale at which parameter values $\Theta_i$ differ. If the parameter of interest is, for example, the difference $\delta$ in the means of reaction times, like in the Mental Chronometry example, this parameter is intuitively interpretable. We can say, for instance, that an $\epsilon$-region of $\pm 5\text{ms}$ is really so short that any value in $[-5\text{ms}; 5\text{ms}]$ would be regarded as identical to $0$ for all practical purposes, because of what we know about reaction times and their potential differences. However, with parameters that are less clearly anchored to a concrete physical measurement about which we have solid distributional knowledge and/or reliable intuitions, fixing the size of the ROPE can be more difficult. For the bias of a coin flip, for instance, which we want to test at the point value $\theta^* = 0.5$ (testing the coin for fairness), we might want to consider a ROPE like $[0.49; 0.51]$, although this choice may be less objectively defensible without previous experimental evidence from similar situations. 

@kruschke2015 advances the ROPE approach for Bayesian hypothesis testing, based on parameter estimation and credible intervals. The rationale for using a ROPE could, in principle, also be extended to frequentist approaches, using confidence intervals instead of credible intervals It would, however, undermine the frequentist rationale for confidence intervals to use a ROPE, because we would lose the tight hand on error control, which is built into frequentist testing, and also confidence intervals. (More on this in Chapter \@ref(ch-03-05-hypothesis-testing)). That is why the frequentist approach does not use ROPEs, or equivalently only considers $\epsilon = 0$.

In Kruschke's ROPE-based approach where $\epsilon \ge 0$, the decision about a point-valued hypothesis becomes ternary. If $[l;u]$ is an interval-based estimate of parameter $\Theta_i$ and $[\theta^*_i - \epsilon; \theta^*_i + \epsilon]$ is the ROPE around the point-value of interest, then we would:

- **accept** the point-valued hypothesis iff $[l;u]$ is contained entirely in $[\theta^*_i - \epsilon; \theta^*_i + \epsilon]$;
- **reject** the point-valued hypothesis iff $[l;u]$ and $[\theta^*_i - \epsilon; \theta^*_i + \epsilon]$ have no overlap; and
- **withhold judgement** otherwise.

Consider the 24/7 example, where the point-valued hypothesis of interest is $\theta^* = 0.5$ (testing the coin for fairness) and the ROPE is $[0.49; 0.51]$ ($\epsilon = 0.1$, arbitrarily set here). The point- and interval-estimates for Bayesian and frequentist approaches are as follows:

```{r}
estimates_24_7 <- tibble(
  `lower_Bayes` = HDInterval::hdi(function(x) qbeta(x, 8,18))[1],
  `point_Bayes` = 8/25,
  `upper_Bayes` = HDInterval::hdi(function(x) qbeta(x, 8,18))[2],
  `lower_frequentist`  = binom::binom.exact(7,24)$lower,
  `point_frequentist`  = 7/24,
  `upper_frequentist`  = binom::binom.exact(7,24)$upper
) %>% 
  pivot_longer(
    everything(),
    names_pattern = "(.*)_(.*)",
    names_to = c(".value", "approach")
  )
estimates_24_7
```

Figure \@ref(fig:ch-03-03-estimation-ROPE-24-7) shows these estimates next to the ROPE. We see that the Bayesian 95% credible interval has no overlap with the ROPE, so that we would *reject* the null-hypothesis of $\theta^* = 0.5$ by the ROPE+estimation logic of statistical decision making. If we were to illegitimately (!) apply this approach to frequentist confidence intervals, we would *accept* this point-valued hypothesis instead. However, as stressed above, this is *not* an accepted move in frequentist statistics. The frequentists would *not* consider a ROPE with $\epsilon > 0$. The frequentist would also not accept the point-valued hypothesis in case the critical value is included in the confidence interval. They would merely withhold judgement, i.e., *not reject* the point hypothesis. (More on this in Chapter \@ref(ch-03-05-hypothesis-testing).)

```{r ch-03-03-estimation-ROPE-24-7, fig.cap = "Comparing point- and interval-estimates (Bayesian credible intervals and frequentist confidence intervals) against a ROPE of $[0.49; 0.51]$ (red shaded area) around the point-valued hypothesis of interest is $\\theta^* = 0.5$.", echo = F}
estimates_24_7 %>% 
  ggplot(aes(x = approach, y = point, ymin = lower, ymax = upper, color = approach)) +
  geom_point(size = 4, color = "black") + 
  geom_rect(
    aes(xmin = 0.5, xmax = 2.5, ymin = 0.49, ymax = 0.51), 
    fill = "firebrick", 
    color = "gray", alpha = 0.3
  ) +
    geom_rect(
    aes(xmin = 0.5, xmax = 2.5, ymin = 0.49999, ymax = 0.50001), 
    fill = "firebrick", 
    color = "firebrick", alpha = 1
  ) +
  # geom_hline(aes(yintercept = 0.5), color = "firebrick") +
  geom_linerange(size = 1.5) + 
  geom_point(size = 4, color = "black") + 
  coord_flip() + 
  ylim(c(0.1 ,0.55)) +
  guides(color = "none") +
  labs(
    y = latex2exp::TeX("Coin bias parameter $\\theta$"),
    x = ""
  )
```


## Comparing Bayesian and frequentist estimates {#ch-03-03-estimation-comparison}

For Bayesians point-valued and interval-based estimates are just summary statistics to efficiently communicate about or reason with the main thing: the full posterior distribution. For the frequentist, the point-valued and interval-based estimates might be all there is. Computing a full posterior can be very hard. Computing point-estimates is usually much simpler. Yet, all the trouble of having to specify priors, and having to calculate a much more complex mathematical object, can pay off. An example which is intuitive enough is that of a likelihood function in a multi-dimensional parameter space where there is an infinite collection of parameter values that maximize the likelihood function (think of plateau). Asking a godly oracle for the (actually: an) MLE can be disastrously misleading. The full posterior will show the quirkiness.^[We will see such an example later for the case of linear regression with correlated independent variables, so-called collinearity.]

Practical issues aside, there are also conceptual arguments that can be pinned against each other. Suppose you do not know the bias of a coin, you flip it once and it lands heads. The case in mathematical notation: $k=1$, $N=1$. As a frequentist, your "best" estimate of the coin's bias is that it is 100% rigged: it will *never* land tails. As a Bayesian, with uninformed priors, your "best" estimate is, following Laplace rule, $\frac{k+1}{N+2} = \frac{2}{3}$. Notice that, apparently, there might be different notions of what counts as "best" in place. Still, the frequentist "best" estimate seems rather extreme.

What about interval-ranged estimates? Which is the better tool, confidence intervals or credible intervals? -- This is harder to answer. Numerical simulations can help answer these questions.^[Even if the math seems daunting, this method is much more tangible and applicable and requires only basic programming experience.] The idea is simple but immensely powerful. We simulate, repeatedly, a ground-truth and synthetic results for fictitious experiments, and then we apply the statistical tests/procedures to these fictitious data sets. Since we know the ground-truth, we can check which tests/procedures got it right.

Let's look at a simulation, comparing credible intervals to confidence intervals, the latter of which calculated by asymptotic approximation or the so-called exact method. To do so, we repeatedly sample a ground-truth (e.g., a known coin bias $\theta_{\text{true}}$) from a flat distribution over $[0;1]$.^[This is already not innocuous. We are fixing, as it were, an assumption about how likely ground-truths should actually occur in the real world.]. We then simulate an experiment in a synthetic world with $\theta_{\text{true}}$, using a fixed value of $n$, here taken from the set $n \in \set{10, 25, 100, 1000}$. We then construct a confidence interval (either approximately or precisely) and a 95% credible interval; for each of the three interval estimates. We check whether the ground-truth $\theta_{\text{true}}$ is *not* included in any given interval estimate. We calculate the mean number of times such non-inclusion (errors!) happen for each kind of interval estimate. The coe below implements this and the figure below shows the results, based on 10,000 samples of $\theta_{\text{true}}$.

```{r}
# how many "true" thetas to sample
n_samples <- 10000 
# sample a "true" theta
theta_true <- runif(n=n_samples)
# create data frame to store results in
results <- expand.grid(
  theta_true = theta_true,
  n_flips = c(10,25,100, 1000)
) %>% 
  as_tibble() %>% 
  mutate(
    outcome = 0,
    norm_approx = 0,
    exact = 0,
    Bayes_HDI = 0
  )
  
for (i in 1:nrow(results)) {
  
  # sample fictitious experimental outcome for current true theta
  results$outcome[i] <- rbinom(
    n = 1, 
    size = results$n_flips[i], 
    prob = results$theta_true[i]
  )
  
  # get CI based on asymptotic Gaussian
  norm_approx_CI <- binom::binom.confint(
    results$outcome[i], 
    results$n_flips[i], 
    method = "asymptotic"
  )
  results$norm_approx[i] <- !(
    norm_approx_CI$lower <= results$theta_true[i] && 
      norm_approx_CI$upper >= results$theta_true[i]
    )
  
  # get CI based on exact method
  exact_CI <- binom::binom.confint(
    results$outcome[i], 
    results$n_flips[i], 
    method = "exact"
  )
  results$exact[i] <- !(
    exact_CI$lower <= results$theta_true[i] && 
      exact_CI$upper >= results$theta_true[i]
  )
  
  # get 95% HDI (flat priors)
  Bayes_HDI <- binom::binom.bayes(
    results$outcome[i], 
    results$n_flips[i], 
    type = "highest", 
    prior.shape1 = 1, 
    prior.shape2 = 1
  )
  results$Bayes_HDI[i] <- !(
    Bayes_HDI$lower <= results$theta_true[i] && 
      Bayes_HDI$upper >= results$theta_true[i]
  )
}

results %>% 
  gather(key = "method", "Type_1", norm_approx, exact, Bayes_HDI) %>% 
  group_by(method, n_flips) %>% 
  dplyr::summarize(avg_type_1 = mean(Type_1)) %>% 
  ungroup() %>% 
  mutate(
    method = factor(
      method, 
      ordered = T, 
      levels = c("norm_approx", "Bayes_HDI", "exact")
    )
  ) %>% 
  ggplot(aes(x = as.factor(n_flips), y = avg_type_1, color = method)) + 
  geom_point() + geom_line(aes(group = method)) +
  xlab("number of flips per experiment") +
  ylab("proportion of exclusions of true theta")


```


## Algorithms for parameter estimation {#Ch-03-03-estimation-algorithms}

Statistical methods are also dictated by practicality. If there is no computer at hand, a statistical model or procedure that is difficult or impossible to compute by hand is not useful. We must understand classic notions and procedures in statistics also from this perspective of computability. In reverse, we must understand recent changes in statistical practice likewise as a natural reaction to advances in computability.

### Optimizing functions

Computing the maximum or minimum of a function, such as an MLE or MAP estimate, is a common problem. R has a built-in function `optim` that is useful for finding the the minimum of a function. (If a maximum is needed, just multiply by $-1$ and search the minimum with `optim`.)

We can use the `otpim` function to retrieve an MLE for the three free parameters of a simple linear regression of `average_price` based on `total_volume_sold` in the [avocado data](app-93-data-sets-avocado), like so:

```{r}
# function for the negative log-likelihood of the given
# data and fixed parameter values
nll = function(y, x, beta_0, beta_1, sd) {
  # negative sigma is logically impossible
  if (sd <= 0) {return( Inf )}
  # predicted values
  yPred = beta_0 + x * beta_1
  # negative log-likelihood of each data point 
  nll = -dnorm(y, mean=yPred, sd=sd, log = T)
  # sum over all observations
  sum(nll)
}
fit_lh = optim(
  # initial parameter values
  par = c(1.5, 0, 0.5),
  # function to optimize
  fn = function(par) {
    with(avocado_data, 
         nll(average_price, total_volume_sold,
             par[1], par[2], par[3])
    )
  }
)
fit_lh$par
```

This result tells us that the best fitting parameter triple has an intercept of $\beta_0 \approx 1.42$, a slope $\beta_1 \approx -2.47$ and a standard deviation $\sigma \approx 0.39$. We can compare these values with a built-in function for linear regression models (which, however, does not return an estimate of $\sigma$ (see Chapter \@ref(Chap-04-01-simple-linear-regression) for more information)):

```{r}
lm(average_price ~ total_volume_sold, avocado_data)$coef
```

### Approximating posterior distributions

There are several methods of computing approximations of Bayesian posteriors. **Variational inference**, for example, hinges on the fact that under very general conditions Bayesian posterior distributions are well approximated by (multi-variate) normal distributions. The more data, the better the approximation. We can then reduce approximation of a Bayesian posterior to a problem of optimizing parameter values: we simply look for the parameter values that yield the "best" parametric approximation to the Bayesian posterior. (Here, "best" is usually expressed in terms of minimizing a measure of divergence between probability distributions, such as [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback–Leibler_divergence).) Another prominent method of approximating Bayesian posteriors is [rejection sampling](https://en.wikipedia.org/wiki/Rejection_sampling).

The most prominent class of methods to approximate Bayesian posteriors are Markov Chain Monte Carlo methods. We will describe the most basic version of such MCMC algorithms below. For the purposes of this class it suffices to accept that there are black boxes (with some knobs for fine-tuning) that, if you supply a model description, priors and data, will return samples from the posterior distribution.

#### Of apples and trees: Markov Chain Monte Carlo sampling

<div style = "float:right; width:20%;">
<img src="visuals/apples.png" alt="apples">  
</div>  

Beginning of each summer, Nature sends out the Children to distribute the apples among the trees. It is custom that bigger trees ought to receive more apples. Indeed, every tree is supposed to receive apples in proportion to how many leaves it has. If Giant George (an apple tree!) has twice as many leaves as Thin Finn (another apple tree!), Giant George is to receive twice as many apples as Thin Finn. This means that if there are $n_a$ apples to distribute in total, and $L(t)$ is the number of leaves of tree $t$, every tree should receive $A(t)$ apples, where:

$$ A(t) = \frac{L(t)}{\sum_{t'} L(t')} \ n_a $$

The trouble is that not even Nature knows the number of leaves of all the trees. Nature does not care about numbers. The Children, however, can count. But they cannot keep in mind the number of leaves for many trees for a long time. And no single Child could ever visit all the trees before the winter. This is why the Children distribute apples in a way that approximates Nature's will. The more apples to distribute, the better the approximation. Nature is generally fine with approximate but practical solutions.

When a Child visits a tree, it affectionately hangs an apple into its branches. It also writes down the name of the tree in a list next to the number of the apple it has just delivered. It then looks around and selects a random tree in the neighborhood. If the current tree $t_c$ where the Child is at present has fewer leaves than this other tree $t_o$, i.e., if $L(t_c) < L(t_o)$, the Child visits $t_o$. If instead $L(t_c) \ge L(t_o)$ the child flips a coin and visits $t_o$ with a probability proportional to $\frac{L(t_o)}{L(t_c)}$. In other words, the Child will always visits a tree with more trees, and it will visit a tree with fewer leaves depending on the proportion of leaves. 

When a large number of apples are distributed, and Nature looks at the list of trees each Child has visited, this list of tree names is a set of **representative samples** from the probability distribution:

$$P(t) \propto L(t)$$

These samples were obtained without knowledge of the normalizing constant. The Children only had $L(t)$ at their disposal. When trees are parameter tuples $\theta$ and the number of leaves is the product $P(D \mid \theta) \ P(\theta)$, the Children would deliver samples from the posterior distribution *without* knowledge of the normalizing constant (a.k.a. the integral of doom).

The sequence of trees visited by a single Child is a **sample chain**. Usually, Nature sends out at least 2-4 Children. The first tree a Child visits is the **initialization of the chain**. Sometimes Nature select initial trees strategically for each Child. Sometimes Nature lets randomness rule. In any case, a Child might be quiet far away from the meadow with lush apple trees, the so-called **critical region** (where to dwell makes most sense). It might take many tree hops before a Child reaches this meadow. Nature therefore allows each Child to hop from tree to tree for a certain time, the **warm-up period**, before the Children start distributing apples and taking notes. If each Child only records every $k$-th tree it visits, Nature calls $k$ a **thinning factor**. Thinning generally reduces **autocorrelation** (think: the amount to which subsequent samples do not carry independent information about the distribution). Since every next hop depends on the current tree (and only on the current tree), the whole process is a **Markov process**. It is light on memory and parallelisable but also affected by autocorrelation. Since we are using samples, a so-called **Monte Carlo method**, the whole affair is a **Markov Chain Monte Carlo** algorithm. It is one of many. It's called **Metropolis Hastings**. More complex MCMC algorithms exist. One class of such MCMC algorithms is called **Hamiltonian Monte Carlo** and these approaches use gradients to optimize the **proposal function**, i.e., the choice of the next tree to consider going to. They use the warm-up period to initialize certain tuning parameters, making them much faster and more reliable (at least if the distribution of leaves among neighboring trees is well-behaved). 

How could Nature be sure that the plan succeeded? If not even Nature knows the distribution $P(t)$, how can we be sure that the Children's list gives representative samples to work with? - Certainty is petty. Reduction of uncertainty is key. Since we send out several Children in parallel, and since each Child distributed many apples, we can compare the list of trees delivered by each Child (= the set of samples in each chain). We can use statistics and ask: is it plausible that the set of samples in each chain has been generated from the same probability distribution? - The answer to this question can help reduce uncertainty about the quality of the sampling process.

## Probabilistic modeling with `greta` {#ch-03-03-estimation-greta}

There are a number of software solutions for Bayesian posterior approximation, all of which implement a form of MCMC sampling, and most of which also realize at least one other form of parameter estimation. Many of these use a special language to define the model, and rely on a different programming language (like R, python, Julia etc.) to communicate with the program that does the sampling. Some options are:

- [WinBUGS](https://www.mrc-bsu.cam.ac.uk/software/bugs/the-bugs-project-winbugs/): a classic which has grown out of use a bit
- [JAGS](http://mcmc-jags.sourceforge.net): another classic
- [Stan](https://mc-stan.org): strongly developed current workhorse
- [WebPPL](http://webppl.org): light-weight, browser-based full probabilistic programming language
- [pyro](http://pyro.ai): for probabilistic (deep) machine learning, based on pytorch
- [greta](https://greta-stats.org): R-only probabilistic modeling package, based on Python and tensorflow

We will be using `greta` to look at explicitly formulated models. Later, when focusing on regression models, we will use an R package called `brms`, which relies on Stan, but we will not actively engage with Stan in this course.

### Basics of `greta`

In order to approximate a posterior distribution over parameters for a model, given some data, using an MCMC algorithm, we need to specify the model for the sampler. In particular we must tell it about (i) the parameters, (ii) their priors, (iii) the likelihood function. The R-package `greta` allows us to specify this model inside of an R script in a syntax that looks like we are using regular R functions, even if in fact we are not. 

For more information on `greta`, see the [greta starting guide](https://greta-stats.org/articles/get_started.html).

### Binomial Model

Figure \@ref(fig:ch-03-03-Binomial-Model-repeated) shows the Binomial model for coin flips, as discussed before. We are going to implement it in `greta`.

```{r ch-03-03-Binomial-Model-repeated, echo = F, fig.cap="The Binomial Model (repeated from before).", out.width = '40%'}
knitr::include_graphics("visuals/binomial-model.png")
```

We data from the [King of France example](app-93-data-sets-king-of-france), where we are interested in the number $k = 109$ of "true" responses to sentences with a false presupposition over all $N = 311$ relevant observations. The research question of interest is whether the idea that the parameter $\theta$ which models to overall disposition to answer "true" for these sentences could plausibly be fixed to $\theta = 0.5$. We declare the relevant counts using the `greta` function `as_data`. In this way all subsequent calculations with `k` and `N` will make it clear that we are defining `greta`-objects as part of a (so far) static model definition.


```{r, eval = F}
# greta data 
k <- as_data(109)
N <- as_data(311)
```

Next, we need to tell the model what the prior for the latent parameter `theta` is. We use an uninformative Beta distribution here, $\theta \sim \text{Beta}(1,1)$. This is realized with the function `beta` from the `greta` package. Notice that `beta` is *not* a function defined in R!

```{r, eval = F}
# coin bias & prior (here: uninformative)
theta   <- beta(1,1)
```

Finally, we tell `greta` about the likelihood of the data. This uses the `greta` functions `distribution` and `binomial`. By using `distribution` for the already defined and given data in variable `k` we inform `greta` that this line is (part of) the definition of the likelihood function.

```{r, eval = F}
# likelihood of data given theta
distribution(k) <- binomial(N, theta)
```

It remains to tell `greta` that the model definition is done and which parameters the MCMC sampler should return information about (here: `theta`).

```{r, eval = F}
# declare the greta model
m <- model(theta)
```

We can then call an the `greta` function `mcmcm` which draws samples using an MCMC algorithm (the default is Hamiltonian Monte Carlo). We can specify additional parameters to tweak the sampler, such as the length of the warm-up period, the number of samples, chains, etc.

```{r, eval = F}
# take 4 chains of 1000 samples
draws <- greta::mcmc(
  model = m, 
  n_samples = 1000,
  warmup = 1000,
  chains = 4
)
```

```{r, echo = F}
# saveRDS(draws, 'models_greta/binomial_draws.rds')
draws <- readRDS('models_greta/binomial_draws.rds')
```

The resulting `draws` object is a special kind of object, an `MCMC.list` as defined in the `coda` package. This is not very important for us, however, we simply transform the samples to a tidy representation, using the function `ggs` from the `ggmcmc` package:

```{r}
# cast results (type 'mcmc.list') into tidy tibble
tidy_draws = ggmcmc::ggs(draws)
tidy_draws
```

We can use these samples to compute Bayesian point- and interval estimates, for example:

```{r, eval = T}
# obtain Bayesian point and interval estimates
Bayes_estimates <- tidy_draws %>% 
  group_by(Parameter) %>%
  summarise(
    '|95%' = HDInterval::hdi(value)[1],
    mean = mean(value),
    '95|%' = HDInterval::hdi(value)[2]
  )
Bayes_estimates
```

Using the (controversial) method of inspecting posterior estimates, we would conclude that $\theta = 0$ is not an *a posteriori* credible value for the inclination to judge the truth of sentences with a false presupposition. 

Figure \@ref(fig:ch-03-03-binomial-posterior) moreover shows a density plot derived from the MCMC samples, together with the estimated 95% HDI and the true posterior distribution (in back), as derived by conjugacy.

```{r ch-03-03-binomial-posterior, echo = F, fig.cap = "Posterior over coin bias $\\theta$ given $k=109$ and $N=311$ approximated by samples from `greta`, with estimated 95% credible interval (red area). The black curve shows the true posterior, derived through conjugacy."}
# get density estimates from samples
dens <- filter(tidy_draws, Parameter == "theta") %>% pull(value) %>% 
  density()
# plot estimated density with true posterior
tibble(
  parameter = dens$x,
  density = dens$y
) %>% 
  ggplot(aes(x = parameter, y = density)) +
  geom_line(color = "firebrick") +
  geom_area(aes(x = ifelse(parameter > Bayes_estimates[1,2] %>% as.numeric & parameter < Bayes_estimates[1,4] %>% as.numeric , parameter, 0)),
            fill = "firebrick", alpha = 0.5) +
  ylim(0, max(dens$y)) +
  xlim(min(dens$x), max(dens$x)) +
  geom_line(
    data = tibble(
      parameter = seq(0,1, length.out = 401),
      density = dbeta(parameter, 109, 311-109)
    ),
    color = "black"
  ) + 
  ylim(c(0,15.6)) + 
  labs(
    x = latex2exp::TeX("Parameter $\\theta$")
  )
```

### T-Test Model for Mental Chronometry

<div style = "float:right; width:15%;">
<img src="visuals/badge-mental-chronometry.png" alt="badge-mental-chronometry">  
</div>  

We will use the [Mental Chronometry](app-93-data-sets-mental-chronometry) data to compare the reaction times in the "go/No-go" condition to the reaction times in the "discrimination" condition. To do this, we implement a T-Test model by hand in `greta`.

First we read in the data and get some handy summary statistics:


```{r}
mc_data_cleaned <- read_csv('data_sets/mental-chrono-data_cleaned.csv',
                            col_types = cols(
                              submission_id = col_double(),
                              trial_number = col_double(),
                              block = col_character(),
                              stimulus = col_character(),
                              RT = col_double(),
                              handedness = col_character(),
                              gender = col_character(),
                              total_time_spent = col_double(),
                              comments = col_character(),
                              mean_C = col_double(),
                              sd_C = col_double(),
                              trial_type = col_character(),
                              trial = col_double()
                            ))
means_and_diffs <- mc_data_cleaned %>%
  filter(block != "reaction") %>% 
  group_by(block) %>% 
  summarise(
    mean_RT = mean(RT)
  ) %>% 
  pivot_wider(
    names_from = block,
    values_from = mean_RT
  ) %>% 
  mutate(
    `discr - gng` = discrimination - goNoGo
  )
means_and_diffs
```

The model we will use for this situation is the T-Test model shown in Figure \@ref(fig:ch-03-03-T-Test-Model-Difference), repeated from the previous Chapter. We use the model which explicitly codes the difference between means (the variable $\delta$) to directly address the question of whether $\delta = 0$ is a plausible point-value for this parameter.

```{r ch-03-03-T-Test-Model-Difference, echo = F, out.width = '70%', fig.cap="A T-Test Model where one group is the default and the difference between group means is explicitly coded as a parameter."}
knitr::include_graphics("visuals/t-test-model-difference.png")
```

We extract the relevant data and declare it as a `greta` object:

```{r, eval = F}
# isolate data vectors
RT_goNoGo <- mc_data_cleaned %>% filter(block == "goNoGo") %>% pull(RT)
RT_discrm <- mc_data_cleaned %>% filter(block == "discrimination") %>% pull(RT)
# declare as greta data arrays
y0 <- as_data(RT_goNoGo)
y1 <- as_data(RT_discrm)
```


We then define the model, using weakly informative, partly regularizing priors, i.e., priors that are informed by the data to ensure swift convergence (expecting values of `mean_0` to lie in plausible region), but that are not very biased to allow a large impact of the likelihood function (using relatively large standard deviations).


```{r, eval = F}
# priors 
mean_0   <- normal(430, 50)
delta    <- normal(0, 100)
sigma    <- normal(100, 10, truncation = c(0, Inf))
# derived prameters
mean_1   <- mean_0 + delta
# likelihood
distribution(y0) <- normal(mean_0, sigma)
distribution(y1) <- normal(mean_1, sigma)
# model 
m <- model(mean_0, mean_1, delta, sigma)## --- sampling ---
draws <- greta::mcmc(m, warmup = 4000, n_samples = 6000, thin = 2)
```

```{r, echo = F}
# saveRDS(draws, '../models_greta/ttest_draws.rds')
draws <- readRDS('models_greta/ttest_draws.rds')
```

Bayesian point- and interval-estimates can be calculated from the posterior samples, including 

```{r, echo = T}
tidy_draws = ggmcmc::ggs(draws)
Bayes_estimates <- tidy_draws %>% 
  group_by(Parameter) %>%
  summarise(
    '|95%' = HDInterval::hdi(value)[1],
    mean = mean(value),
    '95|%' = HDInterval::hdi(value)[2]
  )
Bayes_estimates
```

The Bayesian point-estimates for means and the difference correspond closely to the summary statistics we derived previously:

```{r}
means_and_diffs
```

But we also now get indications of credible ranges of parameter values. Most interestingly, we obtain a 95% credible interval for the $\delta$ parameter, the difference between the means, which quite clearly does not include the case $\delta = 0$. The lower bound of the estimated 95% credible interval is more than 40 ms. We could conclude from this that, given this data set and the model used here, it is plausible that the difference in mean reaction times between the "discrimination" condition and the "go/no-go" condition is at least 40ms.

The plot below shows the density estimated from the posterior samples of $\delta$, together with the estimated 95% credible interval. 

```{r}
dens <- filter(tidy_draws, Parameter == "delta") %>% pull(value) %>% 
  density()

tibble(
  delta = dens$x,
  density = dens$y
) %>% 
  ggplot(aes(x = delta, y = density)) +
  geom_line() +
  geom_area(aes(x = ifelse(
    delta > Bayes_estimates[1,2] %>% as.numeric & 
      delta < Bayes_estimates[1,4] %>% as.numeric , 
    delta, 0)),
    fill = "firebrick", alpha = 0.5) +
  ylim(0, max(dens$y)) +
  xlim(min(dens$x), max(dens$x))
```

The actual posterior is multi-dimensional, and it always pays to inspect the full joint-posterior distribution so as not to miss any unexpected dependencies that might indicate sub-optimal inference or modeling. The `mcmc_pairs` function from the `bayesplot` package plots samples individually for each pair of parameters. Doing this we see the obvious (and perfectly fine) linear relation between estimates of `mean_0` and `delta`: the lower `mean_0` is estimated, the higher `delta` needs to be to yield a value of `mean_` that explains the data well.

```{r}
bayesplot::mcmc_pairs(draws)
```

