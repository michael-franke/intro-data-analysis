
# Common probability distributions {#app-91-distributions}

This chapter summarizes common probability distributions that occur at central places in this book.

## Selected continuous distributions of random variables

### Normal distribution {#app-91-distributions-normal}

One of the most important distribution families is the *Gaussian* or *normal family* because it fits many natural phenomena. Furthermore, the sampling distributions of many estimators depend on the normal distribution either because they are derived from normally distributed random variables or because they can be asymptotically approximated by a normal distribution for large samples (*Central limit theorem*). 

Distributions of the normal family are symmetric with range $(-\infty,+\infty)$ and have two parameters $\mu$ and $\sigma$, respectively referred to as the *mean* and the *standard deviation* of the normal random variable. These parameters are examples of *location* and *scale* parameters. The normal distribution is located at $\mu$, and the choice of $\sigma$ scales its width. The distribution is symmetric, with most observations lying around the central peak $\mu$ and more extreme values being further away depending on $\sigma$. 

$$X \sim Normal(\mu,\sigma) \ \ \text{, or alternatively written as: } \ \ X \sim \mathcal{N}(\mu,\sigma) $$

Figure \@ref(fig:ch-app-01-normal-distribution-density) shows the probability density function of three normally distributed random variables with different parameters. Figure \@ref(fig:ch-app-01-normal-distribution-cumulative) shows the corresponding cumulative function of the three normal distributions.

```{r, ch-app-01-normal-distribution-density, fig.cap = "Examples of a probability density function of the normal distribution. Numbers in legend represent parameter pairs $(\\mu, \\sigma)$.", echo=F}
rv_normal <- tibble(
  x = seq(from = -15, to = 15, by = .01),
  y1 = dnorm(x),
  y2 = dnorm(x, mean = 2, sd = 2),
  y3 = dnorm(x, mean = -2, sd = 3)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(0,1)",
                          parameter == "y2" ~ "(2,2)",
                          parameter == "y3" ~ "(-2,3)")
  )

ggplot(rv_normal, aes(x, y, color = parameter)) +
  geom_line(size = 2) +
  labs(color = "X ~ Normal", y = "Density")
```

```{r, ch-app-01-normal-distribution-cumulative, fig.cap = "The cumulative distribution functions of the normal distributions corresponding to the previous probability density functions.", echo=F}
rv_normal %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)/sum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_line(size = 2) +
    labs(color = "X ~ Normal", y = "y")
```

**Probability density function**

$$f(x)=\frac{1}{\sigma\sqrt{2\pi}}\exp\left(-0.5\left(\frac{x-\mu}{\sigma}\right)^2\right)$$

**Cumulative distribution function**

$$F(x)=\int_{-\inf}^{x}f(t)dt$$

**Expected value** $E(X)=\mu$

**Variance** $Var(X)=\sigma^2$

**Deviation and Coverage**
The normal distribution is often associated with the *68-95-99.7 rule*. The values refer to the probability of a random data point landing within *one*, *two* or *three* standard deviations of the mean (Figure \@ref(fig:ch-app-01-normal-distribution-coverage) depicts these three intervals). For example, about 68% of the values drawn from a normal distribution are within one standard deviation $\sigma$ away from the mean $\mu$.

* $P(\mu-\sigma \leq X \leq \mu+\sigma) = 0.6827$ 
* $P(\mu-2\sigma \leq X \leq \mu+2\sigma) = 0.9545$ 
* $P(\mu-3\sigma \leq X \leq \mu+3\sigma) = 0.9973$ 

```{r, ch-app-01-normal-distribution-coverage, fig.cap = "The coverage of a normal distribution.", echo=F}
# plot normal distribution with intervals
ggplot(rv_normal, aes(x = x)) +
  # plot area under the curve
  stat_function(fun = dnorm, args = list(mean = 0, sd = 2),
                geom = "area",
                fill = project_colors[1],
                xlim = c(-6, 6)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 2),
                geom = "area",
                fill = project_colors[2],
                xlim = c(-4, 4)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 2),
                geom = "area",
                fill = project_colors[3],
                xlim = c(-2, 2)) +
  # plot the curve
  stat_function(fun = dnorm, args = list(mean = 0, sd = 2),
                geom = "line",
                xlim = c(-10, 10),
                size = 2) +
  # scale x-axis
  xlim(-10, 10) +
  # label x-axis
  xlab("X") +
  # label ticks of x-axis
  scale_x_continuous(breaks = c(-6,-4,-2,0,2,4,6), 
                     labels = c(expression(-3~sigma),expression(-2~sigma),
                              expression(-sigma),"0",expression(sigma),
                              expression(2~sigma),expression(3~sigma)))
```

**Z-transformation / standardization**

A special case of normally distributed random variables is the *standard normal* distributed variable with $\mu=0$ and $\sigma=1$: $Y\sim Normal(0,1)$. Each normal distribution $X$ can be converted into a standard normal distribution $Z$ by *z-transformation* (see equation below):

$$Z=\frac{X-\mu}{\sigma}$$

The advantage of standardization is that values from different scales can be compared because they become *scale-independent* by z-transformation. 

**Alternative parameterization**

Often a normal distribution is parameterized in terms of its mean $\mu$ and variance $\sigma^2$. This is clear, from writing $X\sim Normal(\mu, \sigma^2)$, instead of $X\sim Normal(\mu, \sigma)$. 

**Linear transformations**

1. If normal random variable $X\sim Normal(\mu, \sigma^2)$ is linearly transformed by $Y=a*X+b$, then the new random variable $Y$ is again normally distributed with $Y \sim Normal(a\mu+b,a^2\sigma^2)$. 
2. Are $X\sim Normal(\mu_x, \sigma^2)$ and $Y\sim Normal(\mu_y, \sigma^2)$ normally distributed and independent, then their sum is again normally distributed with $X+Y \sim Normal(\mu_x+\mu_y, \sigma_x^2+\sigma_y^2)$. 

#### Hands-on

```{r, ch-app-01-normal-distribution-shiny, echo=FALSE, eval=FALSE, fig.cap="App taken from http://www.artofstat.com/webapps.html [@klingenberg2017]"}
knitr::include_app("https://istats.shinyapps.io/NormalDist/", height = "800px")
```

Here's WebPPL code to explore the effect of different parameter values on a normal distribution:

<pre class="webppl">
var mu = 2;            // mean
var sigma = 3;         // standard deviation
var n_samples = 30000; // number of samples used for approximation
///fold:
viz(repeat(n_samples, function(x) {gaussian({mu: mu, sigma: sigma})}));
///
</pre>

<pre class=" CodeMirror-line " role="presentation">
</pre>

<script>
// find all <pre> elements and set up the editor on them
var preEls = Array.prototype.slice.call(document.getElementsByClassName("webppl"));
preEls.map(function(el) { console.log(el); editor.setup(el, {language: 'webppl'}); });
</script>

### Chi-squared distribution {#app-91-distributions-chi2}

The $\chi^2$-distribution is widely used in hypothesis testing in inferential statistics because many test statistics are approximately distributed as $\chi^2$-distribution. 

The $\chi^2$-distribution is directly related to the standard normal distribution: The sum of the squares of $n$ independent and standard normally distributed random variables $X_1,X_2,...,X_n$ is distributed according to a $\chi^2$-distribution with $n$ *degrees of freedom*:

$$Y=X_1^2+X_2^2+...+X_n^2.$$

The $\chi^2$-distribution is a skewed probability distribution with range $[0,+\infty)$ and only one parameter $n$, the *degrees of freedom* (if $n=1$, then the range is $(0,+\infty)$): 

$$X\sim \chi^2(n).$$

Figure \@ref(fig:ch-app-01-chi-squared-distribution-density) shows the probability density function of three $\chi^2$-distributed random variables with different values for the parameter. Notice that with increasing degrees of freedom, the $\chi^2$-distribution can be approximated by a normal distribution (for $n \geq 30$). Figure \@ref(fig:ch-app-01-chi-squared-distribution-cumulative) shows the corresponding cumulative function of the three $\chi^2$-density distributions.

```{r, ch-app-01-chi-squared-distribution-density, fig.cap = "Examples of a probability density function of the chi-squared distribution.", echo=F}
rv_chisq <- tibble(
  x = seq(from = 0, to = 20, by = .01),
  y1 = dchisq(x, df = 2),
  y2 = dchisq(x, df = 4),
  y3 = dchisq(x, df = 9)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "n=2",
                          parameter == "y2" ~ "n=4",
                          parameter == "y3" ~ "n=9")
  )

# dist plot
ggplot(rv_chisq, aes(x, y, color = parameter)) +
  geom_line(size = 2) +
  labs(color = "X ~ Chi-Squared", y = "Density")
```

```{r, ch-app-01-chi-squared-distribution-cumulative, fig.cap = "The cumulative distribution functions of the chi-squared distributions corresponding to the previous probability density functions.", echo=F}
rv_chisq %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)/sum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_line(size = 2) +
    labs(color = "X ~ Chi-Squared", y = "y")
```

**Probability density function**
$$f(x)=\begin{cases}\frac{x^{\frac{n}{2}-1}e^{-\frac{x}{2}}}{2^{\frac{n}{2}}\Gamma (\frac{n}{2})} &\textrm{ for }x>0,\\ 0 &\textrm{ otherwise.}\end{cases}$$
where $\Gamma (\frac{n}{2})$ denotes the gamma function.

**Cumulative distribution function**
$$F(x)=\frac{\gamma (\frac{n}{2},\frac{x}{2})}{\Gamma \frac{n}{2}},$$
with $\gamma(s,t)$ being the lower incomplete gamma function:
$$\gamma(s,t)=\int_0^t t^{s-1}e^{-t} dt.$$

**Expected value** $E(X)=n$

**Variance** $Var(X)=2n$

**Transformations**
The sum of two $\chi^2$-distributed random variables $X \sim \chi^2(m)$ and $Y \sim \chi^2(n)$ is again a $\chi^2$-distributed random variable with $X+Y=\chi^2(m+n)$.

#### Hands-on

```{r, ch-app-01-chi-squared-distribution-shiny, echo=FALSE, eval=FALSE, fig.cap="App taken from http://www.artofstat.com/webapps.html [@klingenberg2017]"}
knitr::include_app("https://istats.shinyapps.io/ChisqDist/", height = "800px")
```

Here's WebPPL code to explore the effect of different parameter values on a $\chi^2$-distribution:

<pre class="webppl">
var df = 1;            // degrees of freedom
var n_samples = 30000; // number of samples used for approximation
///fold:
var chisq = function(nu) {
  var y = sample(Gaussian({mu: 0, sigma: 1}));
  if (nu == 1) {
    return y*y;
  } else {
    return y*y+chisq(nu-1);
  }
}

viz(repeat(n_samples, function(x) {chisq(df)}));
///
</pre>

<pre class=" CodeMirror-line " role="presentation">
</pre>

<script>
// find all <pre> elements and set up the editor on them
var preEls = Array.prototype.slice.call(document.getElementsByClassName("webppl"));
preEls.map(function(el) { console.log(el); editor.setup(el, {language: 'webppl'}); });
</script>

### F-distribution {#app-91-distributions-F}

The F-distribution, named after R.A. Fisher, is particularly used in regression and variance analysis. It is defined by the ratio of two $\chi^2$-distributed random variables $X\sim \chi^2(m)$ and $Y\sim \chi^2(n)$, each divided by its degrees of freedom:

$$F=\frac{\frac{X}{m}}{\frac{Y}{n}}.$$
The F-distribution is a continuous skewed probability distribution with range $(0,+\infty)$ and two parameters $m$ and $n$, corresponding to the degrees of freedom of the two $\chi^2$-distributed random variables:

$$X \sim F(m,n).$$

Figure \@ref(fig:ch-app-01-F-distribution-density) shows the probability density function of three F-distributed random variables with different parameter values. For a small number of degrees of freedom, the density distribution is skewed to the left side. When the number increases, the density distribution gets more and more symmetric. Figure \@ref(fig:ch-app-01-F-distribution-cumulative) shows the corresponding cumulative function of the three density distributions.
 
```{r, ch-app-01-F-distribution-density, fig.cap = "Examples of a probability density function of the F-distribution. Pairs of numbers in the legend are parameters $(m,n)$.", echo=F}
rv_F <- tibble(
  x = seq(from = 0, to = 7, by = .01),
  y1 = df(x, df1 = 2, df2 = 4),
  y2 = df(x, df1 = 4, df2 = 6),
  y3 = df(x, df1 = 12, df2 = 12)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(2,4)",
                          parameter == "y2" ~ "(4,6)",
                          parameter == "y3" ~ "(12,12)")
  )

# dist plot
ggplot(rv_F, aes(x, y, color = parameter)) +
  geom_line(size = 2) +
  labs(color = "X ~ F", y = "Density")
```

```{r, ch-app-01-F-distribution-cumulative, fig.cap = "The cumulative distribution functions of the F-distributions corresponding to the previous probability density functions.", echo=F}
rv_F %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)/sum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_line(size = 2) +
    labs(color = "X ~ F", y = "y")
```

**Probability density function**
$$F(x)=m^{\frac{m}{2}}n^{\frac{n}{2}} \cdot \frac{\Gamma (\frac{m+n}{2})}{\Gamma (\frac{m}{2})\Gamma (\frac{n}{2})} \cdot \frac{x^{\frac{m}{2}-1}}{(mx+n)^{\frac{m+n}{2}}} \textrm{ for } x>0,$$
where $\Gamma(x)$ denotes the gamma function.

**Cumulative distribution function**
$$F(x)=I\left(\frac{m \cdot x}{m \cdot x+n},\frac{m}{2},\frac{n}{2}\right),$$
with $I(z,a,b)$ being the regularized incomplete beta function:
$$I(z,a,b)=\frac{1}{B(a,b)} \cdot \int_0^z t^{a-1}(1-t)^{b-1} dt.$$

**Expected value** $E(X) = \frac{n}{n-2}$ (for $n \geq 3$)

**Variance** $Var(X) = \frac{2n^2(n+m-2)}{m(n-4)(n-2)^2}$ (for $n \geq 5$)

#### Hands-on

```{r, ch-app-01-F-distribution-shiny, echo=FALSE, eval=FALSE, fig.cap="App taken from http://www.artofstat.com/webapps.html [@klingenberg2017]"}
knitr::include_app("https://istats.shinyapps.io/FDist/", height = "800px")
```

Here's WebPPL code to explore the effect of different parameter values on an F-distribution:

<pre class="webppl">
var df1 = 12;           // degrees of freedom 1
var df2 = 12;           // degrees of freedom 2
var n_samples = 30000;  // number of samples used for approximation
///fold:
var chisq = function(nu) {
  var y = sample(Gaussian({mu: 0, sigma: 1}));
  if (nu == 1) {
    return y*y;
  } else {
    return y*y+chisq(nu-1);
  }
}

var F = function(nu1, nu2) {
  var X = chisq(nu1)/nu1;
  var Y = chisq(nu2)/nu2;
  return X/Y;
}

viz(repeat(n_samples, function(x) {F(df1, df2)}));
///
</pre>

<pre class=" CodeMirror-line " role="presentation">
</pre>

<script>
// find all <pre> elements and set up the editor on them
var preEls = Array.prototype.slice.call(document.getElementsByClassName("webppl"));
preEls.map(function(el) { console.log(el); editor.setup(el, {language: 'webppl'}); });
</script>

### Student's *t*-distribution {#app-91-distributions-students-t}

The Student's $t$-distribution, or just $t$-distribution for short, was discovered by William S. Gosset in 1908 [@vallverdu2015], who published his work under the pseudonym "Student". He worked at the Guinness factory and had to deal with the problem of small sample sizes, where using a normal distribution as an approximation can be too crude. To overcome this problem, Gosset conceived of the $t$-distribution. Accordingly, this distribution is used in particular when the sample size is small and the variance unknown, which is often the case in reality. Its shape resembles the normal bell shape and has a peak at zero, but the $t$-distribution is a bit lower and wider (bigger tails) than the normal distribution. 

The *standard $t$-distribution* consists of a standard-normally distributed random variable $X \sim \text{Normal}(0,1)$ and a $\chi^2$-distributed random variable $Y \sim \chi^2(n)$ ($X$ and $Y$ are independent):

$$T = \frac{X}{\sqrt{Y / n}}.$$
The $t$-distribution has the range $(-\infty,+\infty)$ and one parameter $\nu$, the degrees of freedom. The degrees of freedom can be calculated by the sample size $n$ minus one:
$$t \sim \text{Student-}t(\nu = n -1).$$

Figure \@ref(fig:ch-app-01-t-distribution-density) shows the probability density function of three $t$-distributed random variables with different parameters, and Figure \@ref(fig:ch-app-01-t-distribution-cumulative) shows the corresponding cumulative functions. Notice that for small degrees of freedom $\nu$, the $t$-distribution has bigger tails. This is because the $t$-distribution was specially designed to provide more conservative test results when analyzing small samples. When the degrees of freedom increase, the $t$-distribution approaches a normal distribution. For $\nu \geq 30$, this approximation is quite good. 
 
```{r, ch-app-01-t-distribution-density, fig.cap = "Examples of a probability density function of the $t$-distribution.", echo=F}
rv_student <- tibble(
  x = seq(from = -6, to = 6, by = .01),
  y1 = dt(x, df = 1),
  y2 = dt(x, df = 2),
  y3 = dt(x, df = 10)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "nu=1",
                          parameter == "y2" ~ "nu=2",
                          parameter == "y3" ~ "nu=10")
  )

# dist plot
ggplot(rv_student, aes(x, y, color = parameter)) +
  geom_line(size = 2) +
  labs(color = "X ~ t", y = "Density")
```

```{r, ch-app-01-t-distribution-cumulative, fig.cap = "The cumulative distribution functions of the $t$-distributions corresponding to the previous probability density functions.", echo=F}
rv_student %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)/sum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_line(size = 2) +
    labs(color = "X ~ t", y = "y")
```

**Probability density function**
$$ f(x, \nu)=\frac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\nu\pi} \cdot \Gamma(\frac{\nu}{2})}\left(1+\frac{x^2}{\nu}\right)^{-\frac{\nu+1}{2}},$$
with $\Gamma(x)$ denoting the gamma function.

**Cumulative distribution function**
$$F(x, \nu)=I\left(\frac{x+\sqrt{x^2+\nu}}{2\sqrt{x^2+\nu}},\frac{\nu}{2},\frac{\nu}{2}\right),$$
where $I(z,a,b)$ denotes the regularized incomplete beta function:
$$I(z,a,b)=\frac{1}{B(a,b)} \cdot \int_0^z t^{a-1}(1-t)^{b-1} \text{d}t.$$

**Expected value** $E(X) = 0$

**Variance** $Var(X) = \frac{n}{n-2}$ (for $n \geq 30$)

#### Hands-on

```{r, ch-app-01-t-distribution-shiny, echo=FALSE, eval=FALSE, fig.cap="App taken from http://www.artofstat.com/webapps.html [@klingenberg2017]", echo=F}
knitr::include_app("https://istats.shinyapps.io/tdist/", height = "800px")
```

Here's WebPPL code to explore the effect of different parameter values on a $t$-distribution:

<pre class="webppl">
var df = 3;            // degrees of freedom
var n_samples = 30000; // number of samples used for approximation
///fold:
var chisq = function(nu) {
  var y = sample(Gaussian({mu: 0, sigma: 1}));
  if (nu == 1) {
    return y*y;
  } else {
    return y*y+chisq(nu-1);
  }
}

var t = function(nu) {
  var X = sample(Gaussian({mu: 0, sigma: 1}));
  var Y = chisq(nu);
  return X/Math.sqrt(Y/nu);
}

viz(repeat(n_samples, function(x) {t(df)}));
///
</pre>

<pre class=" CodeMirror-line " role="presentation">
</pre>

<script>
// find all <pre> elements and set up the editor on them
var preEls = Array.prototype.slice.call(document.getElementsByClassName("webppl"));
preEls.map(function(el) { console.log(el); editor.setup(el, {language: 'webppl'}); });
</script>

Beyond the standard $t$-distribution there are also generalized $t$-distributions taking three parameters $\nu$, $\mu$ and $\sigma$, where the latter two are just the mean and the standard deviations, similar to the case of the normal distribution.

### Beta distribution {#app-91-distributions-beta}

The beta distribution creates a continuous distribution of numbers between 0 and 1. Therefore, this distribution is useful if the uncertain quantity is bounded by 0 and 1 (or 100%), is continuous, and has a single mode. In Bayesian Data Analysis, the beta distribution has a special standing as prior distribution for a [Bernoulli](#app-91-distributions-bernoulli) or [binomial](#app-91-distributions-binomial) likelihood. The reason for this is that a combination of a beta prior and a Bernoulli (or binomial) likelihood results in a posterior distribution with the same form as the beta distribution. Such priors are referred to as *conjugate priors* (see Chapter \@ref(ch-03-04-parameter-estimation-conjugacy)).

A beta distribution has two parameters $a$ and $b$ (sometimes also represented in Greek letters $\alpha$ and $\beta$):

$$X \sim Beta(a,b).$$

The two parameters can be interpreted as the number of observations made, such that: $n=a+b$. If $a$ and $b$ get bigger, the beta distribution gets narrower. If only $a$ gets bigger, the distribution moves rightward, and if only $b$ gets bigger, the distribution moves leftward. As the parameters define the shape of the distribution, they are also called *shape parameters*. A Beta(1,1) is equivalent to a uniform distribution. Figure \@ref(fig:ch-app-01-beta-distribution-density) shows the probability density function of four beta distributed random variables with different parameter values. Figure \@ref(fig:ch-app-01-beta-distribution-cumulative) shows the corresponding cumulative functions.

```{r, ch-app-01-beta-distribution-density, fig.cap = "Examples of a probability density function of the beta distribution. Pairs of numbers in the legend represent parameters $(a, b)$.", echo=F}
rv_beta <- tibble(
  x = seq(from = 0, to = 1, by = .01),
  y1 = dbeta(x, shape1 = 1, shape2 = 1),
  y2 = dbeta(x, shape1 = 4, shape2 = 4),
  y3 = dbeta(x, shape1 = 4, shape2 = 2),
  y4 = dbeta(x, shape1 = 2, shape2 = 4)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(1,1)",
                          parameter == "y2" ~ "(4,4)",
                          parameter == "y3" ~ "(4,2)",
                          parameter == "y4" ~ "(2,4)")
  )

# dist plot
ggplot(rv_beta, aes(x, y, color = parameter)) +
  geom_line(size = 2) +
  labs(color = "X ~ Beta", y = "Density")
```

```{r, ch-app-01-beta-distribution-cumulative, fig.cap = "The cumulative distribution functions of the beta distributions corresponding to the previous probability density functions.", echo=F}
rv_beta %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)/sum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_line(size = 2) +
    labs(color = "X ~ Beta", y = "y")
```

**Probability density function**

$$f(x)=\frac{\theta^{(a-1)} (1-\theta)^{(b-1)}}{B(a,b)},$$
where $B(a,b)$ is the beta function:

$$B(a,b)=\int^1_0 \theta^{(a-1)} (1-\theta)^{(b-1)}d\theta.$$

**Cumulative distribution function**

$$F(x)=\frac{B(x;a,b)}{B(a,b)},$$
where $B(x;a,b)$ is the incomplete beta function:

$$B(x;a,b)=\int^x_0 t^{(a-1)} (1-t)^{(b-1)} dt,$$

and $B(a,b)$ the (complete) beta function:

$$B(a,b)=\int^1_0 \theta^{(a-1)} (1-\theta)^{(b-1)}d\theta.$$

**Expected value** 

Mean: $E(X)=\frac{a}{a+b}$

Mode: $\omega=\frac{(a-1)}{a+b-2}$

**Variance** 

Variance: $Var(X)=\frac{ab}{(a+b)^2(a+b+1)}$

Concentration: $\kappa=a+b$ (related to variance such that the bigger $a$ and $b$ are, the narrower the distribution)

**Reparameterization of the beta distribution**
Sometimes it is helpful (and more intuitive) to write the beta distribution in terms of its mode $\omega$ and concentration $\kappa$ instead of $a$ and $b$:

$$Beta(a,b)=Beta(\omega(\kappa-2)+1, (1-\omega)(\kappa-2)+1), \textrm{ for } \kappa > 2.$$

#### Hands-on

Here's WebPPL code to explore the effect of different parameter values on a beta distribution:

<pre class="webppl">
var a = 2;             // shape parameter alpha
var b = 4;             // shape parameter beta
var n_samples = 30000; // number of samples used for approximation
///fold:
viz(repeat(n_samples, function(x) {beta({a: a, b: b})}));
///
</pre>

<pre class=" CodeMirror-line " role="presentation">
</pre>

<script>
// find all <pre> elements and set up the editor on them
var preEls = Array.prototype.slice.call(document.getElementsByClassName("webppl"));
preEls.map(function(el) { console.log(el); editor.setup(el, {language: 'webppl'}); });
</script>

### Uniform distribution

The (continuous) uniform distribution takes values within a specified range $a$ and $b$ that have constant probability. Due to its shape, the distribution is also sometimes called _rectangular distribution_. The uniform distribution is common for random number generation. In Bayesian Data Analysis, it is often used as prior distribution to express *ignorance*. This can be thought of in the following way: When different events are possible, but no (reliable) information exists about their probability of occurrence, the most conservative (and also intuitive) choice would be to assign probability in such a way that all events are equally likely to occur. The uniform distribution models this intuition and generates a completely random number in some interval $[a,b]$.

The distribution is specified by two parameters: the endpoints $a$ (minimum) and $b$ (maximum).

$$X \sim Uniform(a,b) \ \ \text{or alternativelly written as: } \ \ \mathcal{U}(a,b)$$

When $a=0$ and $b=1$, the distribution is referred to as *standard* uniform distribution. Figure \@ref(fig:ch-app-01-uniform-distribution-density) shows the probability density function of two uniformly distributed random variables with different parameter values. Figure \@ref(fig:ch-app-01-uniform-distribution-cumulative) shows the corresponding cumulative functions.

```{r, ch-app-01-uniform-distribution-density, fig.cap = "Examples of a probability density function of the uniform distribution. Pairs of numbers in the legend are parameter values $(a,b)$.", echo=F}
rv_unif <- tibble(
  x = seq(from = -.1, to = 4.2, by = .01),
  y1 = dunif(x),
  y2 = dunif(x, min = 2, max = 4)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = ifelse(parameter == "y1",
                       "(0,1)", "(2,4)")
  )

# dist plot
ggplot(rv_unif, aes(x, y, color = parameter)) +
  geom_line(size = 2) +
  labs(color = "X ~ Uniform", y = "Density")
```

```{r, ch-app-01-uniform-distribution-cumulative, fig.cap = "The cumulative distribution functions of the uniform distributions corresponding to the previous probability density functions.", echo=F}
rv_unif %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)/sum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_line(size = 2) +
    labs(color = "X ~ Uniform", y = "y")
```

**Probability density function**

$$f(x)=\begin{cases} \frac{1}{b-a} &\textrm{ for } x \in [a,b],\\0 &\textrm{ otherwise.}\end{cases}$$

**Cumulative distribution function**

$$F(x)=\begin{cases}0 & \textrm{ for } x<a,\\\frac{x-a}{b-a} &\textrm{ for } a\leq x < b,\\ 1 &\textrm{ for }x \geq b. \end{cases}$$

**Expected value** $E(X)=\frac{a+b}{2}$

**Variance**  $Var(X)=\frac{(b-a)^2}{12}$

#### Hands-on

Here's WebPPL code to explore the effect of different parameter values on a uniform distribution:

<pre class="webppl">
var a = 0;             // lower bound
var b = 1;             // upper bound (> a)
var n_samples = 30000; // number of samples used for approximation
///fold:
viz(repeat(n_samples, function(x) {uniform({a: a, b: b})}));
///
</pre>

<pre class=" CodeMirror-line " role="presentation">
</pre>

<script>
// find all <pre> elements and set up the editor on them
var preEls = Array.prototype.slice.call(document.getElementsByClassName("webppl"));
preEls.map(function(el) { console.log(el); editor.setup(el, {language: 'webppl'}); });
</script>

### Dirichlet distribution {#app-91-distributions-dirichlet}

The Dirichlet distribution is a multivariate generalization of the beta distribution: While the beta distribution is a distribution over binomials, the Dirichlet is a distribution over multinomials. 

It can be used in any situation where an entity has to necessarily fall into one of $n+1$ mutually exclusive subclasses, and the goal is to study the proportion of entities belonging to the different subclasses.

The Dirichlet distribution is commonly used as *prior distribution* in Bayesian statistics, as this family is a *conjugate prior* for the categorical distribution and the multinomial distribution.

The Dirichlet distribution $\mathcal{Dir}(\alpha)$ is a family of continuous multivariate probability distributions, parameterized by a vector $\alpha$ of positive reals. Thus, it is a distribution with $k$ positive parameters $\alpha^k$ with respect to a $k$-dimensional space.


$$X \sim \mathcal{Dirichlet}(\boldsymbol{\alpha})$$

The probability density function (see formula below) of the Dirichlet distribution for $k$ random variables is a $k-1$ dimensional probability *simplex* that exists on a $k$-dimensional space. How does the parameter $\alpha$ influence the Dirichlet distribution?

* Values of $\alpha_i<1$ can be thought of as anti-weight that pushes away $x_i$ toward extremes (see upper left panel of Figure \@ref(fig:ch-app-01-dirichlet-distribution-density)).
* If $\alpha_1=...=\alpha_k=1$, then the points are uniformly distributed (see upper right panel).
* Higher values of $\alpha_i$ lead to greater "weight" of $X_i$ and a greater amount of the total "mass" assigned to it (see lower left panel). 
* If all $\alpha_i$ are equal, the distribution is symmetric (see lower right panel for an asymmetric distribution).


```{r, ch-app-01-dirichlet-distribution-density, fig.cap = "Examples of a probability density function of the Dirichlet distribution with dimension $k$ for different parameter vectors $\\alpha$.", echo=F}
C <- matrix(c(10, 1, 2, 0.15, 10, 1, 10, 0.15, 10, 1, 5, 0.15), 4, 3)

f1 <- function(v) ddirichlet(v, C[1,])
f2 <- function(v) ddirichlet(v, C[2,])
f3 <- function(v) ddirichlet(v, C[3,])
f4 <- function(v) ddirichlet(v, C[4,])

mesh1 <- simplex_mesh(.0025) %>% as_tibble()
mesh2 <- simplex_mesh(.0025) %>% as_tibble()
mesh3 <- simplex_mesh(.0025) %>% as_tibble()
mesh4 <- simplex_mesh(.0025) %>% as_tibble()

mesh1$f1 <- mesh1 %>% apply(1, function(v) f1(bary2simp(v)))
mesh2$f2 <- mesh2 %>% apply(1, function(v) f2(bary2simp(v)))
mesh3$f3 <- mesh3 %>% apply(1, function(v) f3(bary2simp(v)))
mesh4$f4 <- mesh4 %>% apply(1, function(v) f4(bary2simp(v)))

points <- map_df(seq(nrow(C)), function(m){
  rdirichlet(250, C[m,]) %>% 
    simp2bary() %>% 
    as_tibble() %>% 
    transmute(
      mesh = paste0("f", m),
      x = V1, 
      y = V2
    )
}) %>% 
    mutate(
      mesh = recode(mesh, f1 = "(10, 10, 10)",
                          f2 = "(1, 1, 1)",
                          f3 = "(2, 10, 5)",
                          f4 = "(.15, .15, .15)")
    )

meshes <- left_join(mesh1, mesh2, by = c("x", "y")) %>% 
    left_join(mesh3, by = c("x", "y")) %>% 
    left_join(mesh4, by = c("x", "y")) %>% 
    pivot_longer(starts_with("f"), names_to = "mesh") %>% 
    mutate(
      mesh = recode(mesh, f1 = "(10, 10, 10)",
                          f2 = "(1, 1, 1)",
                          f3 = "(2, 10, 5)",
                          f4 = "(.15, .15, .15)")
    )

ggplot(meshes, aes(x, y)) +
  geom_raster(aes(fill = value), show.legend = FALSE) +
  coord_equal(xlim = c(0,1), ylim = c(0, .85)) +
  geom_point(data = points, color = "orange", size = .3) +
  facet_wrap(~mesh)
```

**Probability density function**

$$f(x)=\frac{\Gamma\left(\sum_{i=1}^{n+1} \alpha_i\right)}{\prod_{i=1}^{n+1}\Gamma(\alpha_i)}\prod_{i=1}^{n+1}p_i^{\alpha_i-1},$$
with $\Gamma(x)$ denoting the gamma function and 

$$p_i=\frac{X_i}{\sum_{j=1}^{n+1}X_j}, 1\leq i\leq n,$$

where $X_1,X_2,...,X_{n+1}$ are independent gamma random variables with $X_i \sim Gamma(\alpha_i,1)$.

**Expected value** $E(p_i)=\frac{\alpha_i}{t}, \textrm{ with } t=\sum_{i=1}^{n+1}\alpha_i$

**Variance** $Var(p_i)=\frac{\alpha_i(t-\alpha_i)}{t^2(t+1)}, \textrm{ with } t=\sum_{i=1}^{n+1}\alpha_i$

#### Hands-on

Here's WebPPL code to explore the effect of different parameter values on a Dirichlet distribution:

<pre class="webppl">
var alpha = Vector([1, 1, 5]);  // concentration parameter
var n_samples = 1000;           // number of samples used for approximation
///fold:
var model = function() {
  var dir_sample = dirichlet({alpha: alpha})
  return({"x_1" : dir_sample.data["0"], "x_2" : dir_sample.data["1"]})
}
viz(Infer({method : "rejection", samples: n_samples}, model))
///
</pre>

<pre class=" CodeMirror-line " role="presentation">
</pre>

<script>
// find all <pre> elements and set up the editor on them
var preEls = Array.prototype.slice.call(document.getElementsByClassName("webppl"));
preEls.map(function(el) { console.log(el); editor.setup(el, {language: 'webppl'}); });
</script>

## Selected discrete distributions of random variables 

### Binomial distribution {#app-91-distributions-binomial}

The binomial distribution is a useful model for binary decisions where the outcome is a choice between two alternatives (e.g., Yes/No, Left/Right, Present/Absent, Heads/Tails, ...). The two outcomes are coded as $0$ (failure) and $1$ (success). Consequently, let the probability of occurrence of the outcome "success" be $p$, then the probability of occurrence of "failure" is $1-p$. 
Consider a coin-flip experiment with the outcomes "heads" or "tails". If we flip a coin repeatedly, e.g., 30 times, the successive trials are independent of each other and the probability $p$ is constant, then the resulting binomial distribution is a discrete random variable with outcomes $\{0,1,2,...,30\}$.  
The binomial distribution has two parameters "size" and "prob", often denoted as $n$ and $p$, respectively. The "size" parameter refers to the number of trials and "prob" to the probability of success:
$$X \sim Binomial(n,p).$$

Figure \@ref(fig:ch-app-01-binomial-distribution-mass) shows the probability mass function of three binomially distributed random variables with different parameter values. As stated above, $p$ refers to the probability of success. The higher this probability, the more often we will observe the outcome coded with "1". Therefore, the distribution tends toward the right side and vice-versa. The distribution gets more symmetric if the parameter $p$ approximates 0.5. Figure \@ref(fig:ch-app-01-binomial-distribution-cumulative) shows the corresponding cumulative functions.

```{r, ch-app-01-binomial-distribution-mass, fig.cap = "Examples of a probability mass function of the binomial distribution. Numbers in the legend are pairs of parameters $(n, p)$.", echo=F}
# how many trials
trials = 30

rv_binom <- tibble(
  x = seq(0, trials),
  y1 = dbinom(x, size = trials, p = 0.2),
  y2 = dbinom(x, size = trials, p = 0.5),
  y3 = dbinom(x, size = trials, p = 0.8)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(30,0.2)",
                          parameter == "y2" ~ "(30,0.5)",
                          parameter == "y3" ~ "(30,0.8)")
  )

# dist plot
ggplot(rv_binom, aes(x, y, fill = parameter)) +
  geom_col(position = "identity", alpha = 0.8) +
  labs(fill = "X ~ Binomial", y = "Probability")
```


```{r, ch-app-01-binomial-distribution-cumulative, fig.cap = "The cumulative distribution functions of the binomial distributions corresponding to the previous probability mass functions.", echo=F}
rv_binom %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)/sum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_step(size = 2) +
    labs(color = "X ~ Binomial", y = "y")
```

**Probability mass function**

$$f(x)=\binom{n}{x}p^x(1-p)^{n-x},$$
where $\binom{n}{x}$ is the binomial coefficient.

**Cumulative function**

$$F(x)=\sum_{k=0}^{x}\binom{n}{k}p^k(1-p)^{n-k}$$

**Expected value** $E(X)=n \cdot p$

**Variance** $Var(X)=n \cdot p \cdot (1-p)$

#### Hands-on

```{r, ch-app-01-binomial-distribution-shiny, echo=FALSE, eval=FALSE, fig.cap="App taken from http://www.artofstat.com/webapps.html [@klingenberg2017]", echo=F}
knitr::include_app("https://istats.shinyapps.io/BinomialDist/", height = "800px")
```

Here's WebPPL code to explore the effect of different parameter values on a binomial distribution:

<pre class="webppl">
var p = 0.5;           // probability of success
var n = 4;             // number of trials (>= 1)
var n_samples = 30000; // number of samples used for approximation
///fold:
viz(repeat(n_samples, function(x) {binomial({p: p, n: n})}));
///
</pre>

<pre class=" CodeMirror-line " role="presentation">
</pre>

<script>
// find all <pre> elements and set up the editor on them
var preEls = Array.prototype.slice.call(document.getElementsByClassName("webppl"));
preEls.map(function(el) { console.log(el); editor.setup(el, {language: 'webppl'}); });
</script>

### Multinomial distribution {#app-91-distributions-multinomial}

The multinomial distribution is a generalization of the binomial distribution to the case of $n$ repeated trials: While the binomial distribution can have two outcomes, the multinomial distribution can have multiple outcomes.
Consider an experiment where each trial can result in any of $k$ possible outcomes with a probability $p_i$, where $i=1,2,...,k$, with $\sum_{i=1}^kp_i=1$. For $n$ repeated trials, let $k_i$ denote the number of times $X=x_i$ was observed, where $i=1,2,...,m$. It follows that $\sum_{i=1}^m k_i=n$.

**Probability mass function**

The probability of observing a vector of outcomes $\mathbf{k}=[k_1,...,k_m]^T$ is

$$f(\mathbf{k}|\mathbf{p})=\binom{n}{k_1\cdot k_2 \cdot...\cdot k_m} \prod_{i=1}^m p_i^{k_i},$$

where $\binom{n}{k_1\cdot k_2 \cdot...\cdot k_m}$ is the multinomial coefficient: $$\binom{n}{k_1\cdot k_2 \cdot...\cdot k_m}=\frac{n!}{k_1!\cdot k_2! \cdot...\cdot k_m!}.$$ 
It is a generalization of the binomial coefficient $\binom{n}{k}$.

**Expected value:** $E(X)=n\cdot p_i$

**Variance:** $Var(X)=n\cdot p_i\cdot (1-p_i)$

#### Hands-on

Here's WebPPL code to explore the effect of different parameter values on a multinomial distribution:

<pre class="webppl">
var ps = [0.25, 0.25, 0.25, 0.25];  // probabilities
var n = 4;                          // number of trials (>= 1)
var n_samples = 30000;              // number of samples used for approximation
///fold:
viz.hist(repeat(n_samples, function(x) {multinomial({ps: ps, n: n})}));
///
</pre>

<pre class=" CodeMirror-line " role="presentation">
</pre>

<script>
// find all <pre> elements and set up the editor on them
var preEls = Array.prototype.slice.call(document.getElementsByClassName("webppl"));
preEls.map(function(el) { console.log(el); editor.setup(el, {language: 'webppl'}); });
</script>

### Bernoulli distribution {#app-91-distributions-bernoulli}

The Bernoulli distribution is a special case of the binomial distribution with $size = 1$. The outcome of a Bernoulli random variable is therefore either 0 or 1. Apart from that, the same information holds as for the binomial distribution.
As the "size" parameter is now negligible, the Bernoulli distribution has only one parameter, the probability of success $p$:
$$X \sim Bern(p).$$
Figure \@ref(fig:ch-app-01-bernoulli-distribution-mass) shows the probability mass function of three Bernoulli distributed random variables with different parameters. Figure \@ref(fig:ch-app-01-bernoulli-distribution-cumulative) shows the corresponding cumulative distributions.

```{r, ch-app-01-bernoulli-distribution-mass, fig.cap = "Examples of a probability mass function of the Bernoulli distribution.", echo=F}

rv_bern <- tibble(
  x = seq(from = 0, to = 1),
  y1 = dbern(x, prob = 0.2),
  y2 = dbern(x, prob = 0.5),
  y3 = dbern(x, prob = 0.8)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "p=0.2",
                          parameter == "y2" ~ "p=0.5",
                          parameter == "y3" ~ "p=0.8")
  )

# dist plot
ggplot(rv_bern, aes(x, y, fill = parameter)) +
  geom_col(position = "dodge", color = "white") +
  labs(fill = "X ~ Bernoulli", y = "Probability") +
  scale_x_continuous(breaks = c(0.0,1.0), labels = c("0","1"), limits = c(-0.5,1.5))
```

```{r, ch-app-01-bernoulli-distribution-cumulative, fig.cap = "The cumulative distribution functions of the Bernoulli distributions corresponding to the previous probability mass functions.", echo=F}

rv_bern %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)/sum(y),
    cum_y2 = cumsum(y)/sum(y)
  ) %>% 
  add_column(
    x2 = c(1,1,1,1.5,1.5,1.5)
  ) %>%  
  ungroup() %>% 

ggplot(aes(x, cum_y, color = parameter)) +
  geom_segment(aes(xend = x2, yend = cum_y2), size = 1.5, linetype = "dashed") +
  geom_segment(aes(x = -0.5, y = 0,xend = 0.0, yend = 0), size = 1.5, linetype = "dashed") +
  geom_point(aes(x, cum_y), size = 4) +
  labs(color = "X ~ Bernoulli", y = "y") +
  scale_x_continuous(breaks = c(0.0,1.0), labels = c("0","1"), limits = c(-0.5,1.5))
```

**Probability mass function**

$$f(x)=\begin{cases} p &\textrm{ if } x=1,\\ 1-p &\textrm{ if } x=0.\end{cases}$$

**Cumulative function**

$$F(x)=\begin{cases} 0 &\textrm{ if } x < 0, \\ 1-p &\textrm{ if } 0 \leq x <1,\\1 &\textrm{ if } x \geq 1.\end{cases}$$

**Expected value** $E(X)=p$

**Variance** $Var(X)=p \cdot (1-p)$

#### Hands-on

Here's WebPPL code to explore the effect of different parameter values on a Bernoulli distribution:

<pre class="webppl">
var p = 0.5;           // probability of success
var n_samples = 30000; // number of samples used for approximation
///fold:
viz(repeat(n_samples, function(x) {bernoulli({p: p})}));
///
</pre>

<pre class=" CodeMirror-line " role="presentation">
</pre>

<script>
// find all <pre> elements and set up the editor on them
var preEls = Array.prototype.slice.call(document.getElementsByClassName("webppl"));
preEls.map(function(el) { console.log(el); editor.setup(el, {language: 'webppl'}); });
</script>

### Categorical distribution {#app-91-distributions-categorical}

The categorical distribution is a generalization of the Bernoulli distribution for categorical random variables: While a Bernoulli distribution is a distribution over two alternatives, the categorical is a distribution over multiple alternatives. For a single trial (e.g., a single die roll), the categorical distribution is equal to the multinomial distribution.

The categorical distribution is parametrized by the probabilities assigned to each event. Let $p_i$ be the probability assigned to outcome $i$. The set of $p_i$'s are the parameters, constrained by $\sum_{i=1}^kp_i=1$.

$$X \sim Categorical(\mathbf{p})$$

```{r, ch-app-01-categorical-distribution-mass, fig.cap = "Examples of a probability mass function of the categorical distribution.", echo=F}

rv_cat <- tibble(
  x = seq(from = 0, to = 20),
  y1 = extraDistr::dcat(x, prob = c(0.2,0.3,0.4,0.1)),
  y2 = extraDistr::dcat(x, prob = c(0.1,0.1,0.5,0.3)),
  y3 = extraDistr::dcat(x, prob = c(0.4,0.3,0.1,0.2))
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(0.2,0.3,0.4,0.1)",
                          parameter == "y2" ~ "(0.1,0.1,0.5,0.3)",
                          parameter == "y3" ~ "(0.4,0.3,0.1,0.2)")
  )


# dist plot
ggplot(rv_cat, aes(x, y, fill = parameter)) +
  geom_col(position = "dodge", color = "white") +
  labs(fill = "X ~ Categorical", y = "Probability") +
  scale_x_continuous(limits = c(0,5), breaks = c(1,2,3,4))
```

```{r, ch-app-01-categorical-distribution-cumulative, fig.cap = "The cumulative distribution functions of the categorical distributions corresponding to the previous probability mass functions.", echo=F}

rv_cat %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)/sum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_step(size = 2) +
    labs(color = "X ~ Categorical", y = "y") +
    scale_x_continuous(limits = c(0,5), breaks = c(1,2,3,4))
```

**Probability mass function**

$$f(x|\mathbf{p})=\prod_{i=1}^kp_i^{\{x=i\}},$$

where $\{x=i\}$ evaluates to 1 if $x=i$, otherwise 0 and $\mathbf{p}={p_1,...,p_k}$, where $p_i$ is the probability of seeing event $i$.

**Expected Value** $E(\mathbf{x})=\mathbf{p}$

**Variance** $Var(\mathbf{x})=\mathbf{p}\cdot(1-\mathbf{p})$

#### Hands-on

Here's WebPPL code to explore the effect of different parameter values on a categorical distribution:

<pre class="webppl">
var ps = [0.5, 0.25, 0.25];  // probabilities
var vs = [1, 2, 3];          // categories
var n_samples = 30000;       // number of samples used for approximation
///fold:
viz(repeat(n_samples, function(x) {categorical({ps: ps, vs: vs})}));
///
</pre>

<pre class=" CodeMirror-line " role="presentation">
</pre>

<script>
// find all <pre> elements and set up the editor on them
var preEls = Array.prototype.slice.call(document.getElementsByClassName("webppl"));
preEls.map(function(el) { console.log(el); editor.setup(el, {language: 'webppl'}); });
</script>

### Beta-Binomial distribution {#app-91-distributions-beta-binomial}

As the name already indicates, the beta-binomial distribution is a mixture of a binomial and beta distribution. Remember, a binomial distribution is useful to model a binary choice with outcomes "0" and "1". The binomial distribution has two parameters $p$ and $n$, denoting the probability of success ("1") and the number of trials, respectively. Furthermore, we assume that the successive trials are independent and $p$ is constant. In a beta-binomial distribution, $p$ is not anymore assumed to be constant (or fixed) but changes from trial to trial. Thus, a further assumption about the distribution of $p$ is made, and here the beta distribution comes into play: the probability $p$ is assumed to be randomly drawn from a beta distribution with parameters $a$ and $b$. 
Therefore, the beta-binomial distribution has three parameters $n$, $a$ and $b$:
$$X \sim BetaBinom(n,a,b).$$
For large values of a and b, the distribution approaches a binomial distribution. When $a=1$ and $b=1$, the distribution equals a discrete uniform distribution from 0 to $n$. When $n = 1$, the distribution equals a Bernoulli distribution.

Figure \@ref(fig:ch-app-01-betabinom-distribution-mass) shows the probability mass function of three beta-binomial distributed random variables with different parameter values. Figure \@ref(fig:ch-app-01-betabinom-distribution-cumulative) shows the corresponding cumulative distributions.

```{r, ch-app-01-betabinom-distribution-mass, fig.cap = "Examples of a probability mass function of the beta-binomial distribution. Triples of numbers in the legend represent parameter values $(n,a,b)$.", echo=F}
# how many trials
trials = 30

rv_betabinom <- tibble(
  x = seq(from = 0, to = trials),
  y1 = dbbinom(x, size = trials, alpha = 4, beta = 4),
  y2 = dbbinom(x, size = trials, alpha = 2, beta = 4),
  y3 = dbbinom(x, size = trials, alpha = 1, beta = 1)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(30,4,4)",
                          parameter == "y2" ~ "(30,2,4)",
                          parameter == "y3" ~ "(30,1,1)")
  )

# dist plot
ggplot(rv_betabinom, aes(x, y, fill = parameter)) +
  geom_col(position = "identity", alpha = 0.7) +
  labs(fill = "X ~ Beta-Binomial", y = "Probability")
```

```{r, ch-app-01-betabinom-distribution-cumulative, fig.cap = "The cumulative distribution functions of the beta-binomial distributions corresponding to the previous probability mass functions.", echo=F}
rv_betabinom %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)/sum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_step(size = 2) +
    labs(color = "X ~ Beta-Binomial", y = "y") 
```

**Probability mass function**

$$f(x)=\binom{n}{x} \frac{B(a+x,b+n-x)}{B(a,b)},$$

where $\binom{n}{x}$ is the binomial coefficient and $B(x)$ is the beta function (see beta distribution).

**Cumulative function**

$$F(x)=\begin{cases} 0 &\textrm{ if } x<0,\\ \binom{n}{x} \frac{B(a+x,b+n-x)}{B(a,b)} {}_3F_2(n,a,b) &\textrm{ if } 0 \leq x < n,\\ 1 &\textrm{ if } x \geq n. \end{cases}$$
where ${}_3F_2(n,a,b)$ is the generalized hypergeometric function. 

**Expected value** $E(X)=n \frac{a}{a+b}$

**Variance** $Var(X)=n \frac{ab}{(a+b)^2} \frac{a+b+n}{a+b+1}$

#### Hands-on

Here's WebPPL code to explore the effect of different parameter values on a beta-binomial distribution:

<pre class="webppl">
var a = 1;              // shape parameter alpha
var b = 1;              // shape parameter beta
var n = 10;             // number of trials (>= 1)
var n_samples = 30000;  // number of samples used for approximation
///fold:
viz(repeat(n_samples, function(x) {binomial({n: n, p: beta(a, b)})}));
///
</pre>

<pre class=" CodeMirror-line " role="presentation">
</pre>

<script>
// find all <pre> elements and set up the editor on them
var preEls = Array.prototype.slice.call(document.getElementsByClassName("webppl"));
preEls.map(function(el) { console.log(el); editor.setup(el, {language: 'webppl'}); });
</script>

### Poisson distribution

A Poisson distributed random variable represents the number of events occurring in a given *time interval*. The Poisson distribution is a limiting case of the binomial distribution when the number of trials becomes very large and the probability of success is small (e.g., the number of car accidents in Osnabrueck in the next month, the number of typing errors on a page, the number of interruptions generated by a CPU during T seconds, etc.).
Events described by a Poisson distribution must fulfill the following conditions: they occur in non-overlapping intervals, they do not occur simultaneously, and each event occurs at a constant rate. 

The Poisson distribution has one parameter, the rate $\lambda$, sometimes also referred to as *intensity*:

$$X \sim Poisson(\lambda).$$
 
The parameter $\lambda$ can be thought of as the expected number of events in the time interval. Consequently, changing the rate parameter changes the probability of seeing different numbers of events in one interval. Figure \@ref(fig:ch-app-01-poisson-distribution-mass) shows the probability mass function of three Poisson distributed random variables with different parameter values. Notice that the higher $\lambda$, the more symmetrical the distribution gets. In fact, the Poisson distribution can be approximated by a normal distribution for a rate parameter of $\geq$ 10. Figure \@ref(fig:ch-app-01-poisson-distribution-cumulative) shows the corresponding cumulative distributions.

```{r, ch-app-01-poisson-distribution-mass, fig.cap = "Examples of a probability mass function of the Poisson distribution.", echo=F}

rv_pois <- tibble(
  x = seq(from = 0, to = 30, by = 1),
  y1 = dpois(x, lambda = 2),
  y2 = dpois(x, lambda = 8),
  y3 = dpois(x, lambda = 15)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "lambda=2",
                          parameter == "y2" ~ "lambda=8",
                          parameter == "y3" ~ "lambda=15")
  )

# dist plot
ggplot(rv_pois, aes(x, y, fill = parameter)) +
  geom_col(alpha = 0.7, position = "identity") +
  labs(fill = "X ~ Poisson", y = "Density") 
```


```{r, ch-app-01-poisson-distribution-cumulative, fig.cap = "The cumulative distribution functions of the Poisson distributions corresponding to the previous probability mass functions.", echo=F}
# cumdist plot
rv_pois %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)/sum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_step(size = 2) +
    labs(color = "X ~ Poisson", y = "y")
```

**Probability mass function**

$$f(x)=\frac{\lambda^x}{x!}e^{-\lambda}$$

**Cumulative function**

$$F(x)=\sum_{k=0}^{x}\frac{\lambda^k}{k!}e^{-\lambda}$$

**Expected value** $E(X)= \lambda$

**Variance** $Var(X)=\lambda$

#### Hands-on

```{r, ch-app-01-poisson-distribution-shiny, echo=FALSE, eval=FALSE, fig.cap="App taken from http://www.artofstat.com/webapps.html [@klingenberg2017]", echo=F}
knitr::include_app("https://istats.shinyapps.io/PoissonDist/", height = "800px")
```

Here's WebPPL code to explore the effect of different parameter values on a Poisson distribution:

<pre class="webppl">
var lambda = 5;        // rate parameter
var n_samples = 30000; // number of samples used for approximation
///fold:
viz(repeat(n_samples, function(x) {poisson({mu: lambda})}));
///
</pre>

<pre class=" CodeMirror-line " role="presentation">
</pre>

<script>
// find all <pre> elements and set up the editor on them
var preEls = Array.prototype.slice.call(document.getElementsByClassName("webppl"));
preEls.map(function(el) { console.log(el); editor.setup(el, {language: 'webppl'}); });
</script>

<!-- ## Understanding distributions as random variables -->

<!-- ```{r, ch-app-01-random-variables-normal-distribution, fig.cap = "Transformation of a normally distributed random variable by addition and linear transformation.", echo=F} -->

<!-- rv_normal_transform = tibble( -->
<!--   x_axis = seq(from = -5, to = 5, by = .01), -->
<!--   rv_norm_x = dnorm(x = x_axis, mean = 1, sd = 2), -->
<!--   rv_norm_y = dnorm(x = x_axis, mean = 1.75, sd = 2.75), -->
<!--   rv_linear_transform = 3*rv_norm_x+0.25, -->
<!--   rv_summ_transform = rv_norm_x + rv_norm_y -->
<!-- ) %>%  -->
<!--   pivot_longer(cols = starts_with("rv"), -->
<!--                names_to  = "random_variables", -->
<!--                values_to = "y") %>%  -->
<!--   mutate( -->
<!--     random_variables = case_when(random_variables == "rv_norm_x" ~ "X ~ N(1,2)", -->
<!--                           random_variables == "rv_norm_y" ~ "Y ~ N(1.75,2.75)", -->
<!--                           random_variables == "rv_linear_transform" ~ "3*X+0.25~ N(3*1+0.25,3^2*2^2)", -->
<!--                           random_variables == "rv_summ_transform" ~ "X+Y ~ N(1+1.75,2^2+2.75^2)" -->
<!--                           ) -->
<!--   ) -->

<!-- # dist plot -->
<!-- ggplot(rv_normal_transform, aes(x_axis, y, color = random_variables)) + -->
<!--   geom_line(size = 2) + -->
<!--   guides(color = guide_legend(nrow = 2)) + -->
<!--   theme(legend.text = element_text(size = 9)) + -->
<!--   labs(color = "X,Y ~ Normal", y = "Density", x = "x") -->
<!-- ``` -->

<!-- ```{r, ch-app-01-random-variables-chi-distribution, fig.cap = "Create chi-squared distributed random variable by summation of standard normal random variables.", echo=F} -->

<!-- rv_norm_transform = tibble( -->
<!--   rv_norm_X = rnorm(n = 1e6), -->
<!--   rv_norm_Y = rnorm(n = 1e6), -->
<!--   rv_norm_Z = rnorm(n = 1e6), -->
<!--   rv_summ_transform_1 = rv_norm_X^2+rv_norm_Y^2, -->
<!--   rv_summ_transform_2 = rv_norm_X^2+rv_norm_Y^2+rv_norm_Z^2 -->
<!-- ) %>%  -->
<!--   pivot_longer(cols = starts_with("rv_summ"), -->
<!--                names_to  = "random_variables", -->
<!--                values_to = "y") %>%  -->
<!--   mutate( -->
<!--     RVs = case_when(random_variables == "rv_summ_transform_1" ~ "X^2+Y^2", -->
<!--                     random_variables == "rv_summ_transform_2" ~ "X^2+Y^2+Z^2" -->
<!--     ) -->
<!--   ) -->

<!-- rv_chi_transform = tibble( -->
<!--   x_axis = seq(from = 0, to = 10, by = .01), -->
<!--   rv_chi_1 = dchisq(x = x_axis, df = 2), -->
<!--   rv_chi_2 = dchisq(x = x_axis, df = 3) -->
<!-- ) %>%  -->
<!--   pivot_longer(cols = starts_with("rv"), -->
<!--                names_to  = "random_variables", -->
<!--                values_to = "y") %>%  -->
<!--   mutate( -->
<!--     RVs = case_when(random_variables == "rv_chi_1" ~ "Chi(2)", -->
<!--                           random_variables == "rv_chi_2" ~ "Chi(3)" -->
<!--                           ) -->
<!--   ) -->

<!-- # dist plots -->
<!-- p1 <- ggplot() + -->
<!--   geom_line(rv_chi_transform, mapping = aes(x_axis, y, color = RVs), size = 2) + -->
<!--   labs(y = "Density", x = "x")  -->
  
<!-- p2 <- ggplot() + -->
<!--   geom_line(rv_norm_transform, mapping = aes(y, color = RVs), size = 2, stat = "density") + -->
<!--   xlim(0,10) + -->
<!--   ylim(0,0.5) + -->
<!--   labs(y = "Density", x = "x")  -->

<!-- grid.arrange(p1,p2, ncol = 2) -->
<!-- ``` -->

<!-- ```{r, ch-app-01-random-variables-t-distribution, fig.cap = "Create a t-distributed random variable by dividing a standard normal RV by a chi-squared RV.", echo=F} -->

<!-- rv_norm_chi_transform = tibble( -->
<!--   rv_norm_X = rnorm(n = 1e6), -->
<!--   rv_norm_Y = rnorm(n = 1e6), -->
<!--   rv_norm_Z = rnorm(n = 1e6), -->
<!--   rv_chi = rv_norm_Y^2+rv_norm_Z^2, -->
<!--   rv_transform_t = rv_norm_X/sqrt(rv_chi/2) -->
<!-- ) %>%  -->
<!--   pivot_longer(cols = starts_with("rv_transform"), -->
<!--                names_to  = "random_variables", -->
<!--                values_to = "y") %>%  -->
<!--   mutate( -->
<!--     RVs = case_when(random_variables == "rv_transform_t" ~ "X/sqrt(chi/df)") -->
<!--   ) -->

<!-- rv_student = tibble( -->
<!--   x_axis = seq(from = -5, to = 5, by = .01), -->
<!--   rv_t_1 = dt(x = x_axis, df = 2) -->
<!-- ) %>%  -->
<!--   pivot_longer(cols = starts_with("rv"), -->
<!--                names_to  = "random_variables", -->
<!--                values_to = "y") %>%  -->
<!--   mutate( -->
<!--     RVs = case_when(random_variables == "rv_t" ~ "t(2)") -->
<!--   ) -->

<!-- # dist plots -->
<!-- p1 <- ggplot() + -->
<!--   geom_line(rv_student, mapping = aes(x_axis, y), size = 2) + -->
<!--   ylim(0,0.4) + -->
<!--   labs(y = "Density", x = "x")  -->
  
  
<!-- p2 <- ggplot() + -->
<!--   geom_line(rv_norm_chi_transform, mapping = aes(y), size = 2, stat = "density") + -->
<!--   xlim(-5,5) + -->
<!--   ylim(0,0.4) + -->
<!--   labs(y = "Density", x = "x")  -->
  
<!-- grid.arrange(arrangeGrob(p1, top = "RV ~ t(2)"), arrangeGrob(p2, top = "RV ~ std.norm/sqrt(chi/2)"), ncol=2) -->
<!-- ``` -->

<!-- ```{r, ch-app-01-random-variables-F-distribution, fig.cap = "Create an F-distributed random variable by dividing a chi-squared RV by another (independent) chi-squared RV.", echo=F} -->

<!-- rv_norm_chi_transform = tibble( -->
<!--   rv_norm_V = rnorm(n = 1e6), -->
<!--   rv_norm_W = rnorm(n = 1e6), -->
<!--   rv_norm_X = rnorm(n = 1e6), -->
<!--   rv_norm_Y = rnorm(n = 1e6), -->
<!--   rv_norm_Z = rnorm(n = 1e6), -->
<!--   rv_chi_1 = rv_norm_V^2+rv_norm_W^2, -->
<!--   rv_chi_2 = rv_norm_X^2+rv_norm_Y^2+rv_norm_Z^2, -->
<!--   rv_transform_F = (rv_chi_1/2)/(rv_chi_2/3) -->
<!-- ) %>%  -->
<!--   pivot_longer(cols = starts_with("rv_transform"), -->
<!--                names_to  = "random_variables", -->
<!--                values_to = "y") %>%  -->
<!--   mutate( -->
<!--     RVs = case_when(random_variables == "rv_transform_F" ~ "(rv_chi_1/2)/(rv_chi_2/3)") -->
<!--   ) -->

<!-- rv_fisher = tibble( -->
<!--   x_axis = seq(from = 0, to = 20, by = .01), -->
<!--   rv_F = df(x = x_axis, df1 = 2, df2 = 3) -->
<!-- ) %>%  -->
<!--   pivot_longer(cols = starts_with("rv"), -->
<!--                names_to  = "random_variables", -->
<!--                values_to = "y") %>%  -->
<!--   mutate( -->
<!--     RVs = case_when(random_variables == "rv_F" ~ "F(2,3)") -->
<!--   ) -->

<!-- # dist plots -->
<!-- p1 <- ggplot() + -->
<!--   geom_line(rv_fisher, mapping = aes(x_axis, y), size = 2) + -->
<!--   ylim(0,1) + -->
<!--   labs(y = "Density", x = "x")  -->
  
  
<!-- p2 <- ggplot() + -->
<!--   geom_line(rv_norm_chi_transform, mapping = aes(y), size = 2, stat = "density") + -->
<!--   xlim(0,20) + -->
<!--   ylim(0,1) + -->
<!--   labs(y = "Density", x = "x")  -->
  
<!-- grid.arrange(arrangeGrob(p1, top = "RV ~ F(2,3)"), arrangeGrob(p2, top = "RV ~ (rv_chi_1/2)/(rv_chi_2/3)"), ncol = 2) -->
<!-- ``` -->
