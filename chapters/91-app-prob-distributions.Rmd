
# Common probability distributions {#app-91-distributions}

This chapter summarizes common probability distributions, which occur at central places in this book.

## Selected continuous distributions of random variables

### Normal distribution {#app-91-distributions-normal}

One of the most important distribution families is the *gaussian* or *normal family* because it fits many natural phenomena. Furthermore the sampling distributions of many estimators depend on the normal distribution. On the one hand because they are derived from normally distributed random variables or on the other hand because they can be asymptotically approximated by a normal distribution for large samples (*Central limit theorem*). 

Distributions of the normal family are symmetric with range $(-\infty,+\infty)$ and have two parameters $\mu$ and $\sigma$ that are referred to, respectively, as the *mean* and the *standard deviation* of the normal random variable. These parameters are examples of *location* and *scale* parameters. The normal distribution is located at $\mu$ and its width is scaled by choice of $\sigma$. The distribution is symmetric with most observations lying aroung the central peak $\mu$ and more extreme values are further away depending on $\sigma$. 

$$X\sim Normal(\mu,\sigma^2)$$

Fig.~\ref{fig:ch-app-01-normal-distribution-density} shows the probability density function of three normal distributed random variables with different parameters. Fig.~\ref{fig:ch-app-01-normal-distribution-cumulative} shows the corresponding cumulative function of the three normal distributions.

```{r, ch-app-01-normal-distribution-density, fig.cap = "Examples of probability density function of normal distributions."}
rv_normal <- tibble(
  x = seq(from = -15, to = 15, by = .01),
  y1 = dnorm(x),
  y2 = dnorm(x, mean = 2, sd = 2),
  y3 = dnorm(x, mean = -2, sd = 3)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(0,1)",
                          parameter == "y2" ~ "(2,2)",
                          parameter == "y3" ~ "(-2,3)")
  )

ggplot(rv_normal, aes(x, y, color = parameter)) +
  geom_line(size = 2) +
  labs(color = "X ~ Normal", y = "Density")
```

```{r, ch-app-01-normal-distribution-cumulative, fig.cap = "Examples of the cumulative distribution function of normal distributions corresponding to the previous probability density functions."}
rv_normal %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)/sum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_line(size = 2) +
    labs(color = "X ~ Normal", y = "y")
```

A special case of normal distributed random variables is the *standard normal* distributed variable with $\mu=0$ and $\sigma=1$: $Y\sim Normal(0,1)$. Each normal distribution can be converted into a standard normal distribution by *z-standardization* (see equation below). The advantage of standardization is that values from different scales can be compared, because they become *scale independent* by z-transformation. 

**Probability density function**

$$f(x)=\frac{1}{\sigma\sqrt{2\pi}}\exp\left(-0.5\left(\frac{x-\mu}{\sigma}\right)^2\right)$$

**Cumulative distribution function**

$$F(x)=\int_{-\inf}^{x}f(t)dt$$

**Expected value** $E(X)=\mu$

**Variance** $Var(X)=\sigma^2$

**Z-transformation** $Z=\frac{X-\mu}{\sigma}$

**Deviation and *Coverage**
The normal distribution is often associated with the \emph{68-95-99.7 rule}. The values refer to the probability of a random data point landing within \emph{one}, \emph{two} or \emph{three} standard deviations of the mean (Fig.~\ref{fig:ch-app-01-normal-distribution-coverage} depicts these three intervals). For example, about 68% of values drawn from a normal distribution are within one standard deviation $\sigma$ away from the mean $\mu$.

* $P(\mu-\sigma \leq X \leq \mu+\sigma) = 0.6827$ 
* $P(\mu-2\sigma \leq X \leq \mu+2\sigma) = 0.9545$ 
* $P(\mu-3\sigma \leq X \leq \mu+3\sigma) = 0.9973$ 

```{r, ch-app-01-normal-distribution-coverage, fig.cap = "Coverage of normal distribution"}
# plot normal distribution with intervals
ggplot(NULL) +
  # plot area under the curve
  stat_function(fun = dnorm, args = list(mean = 0, sd = 2),
                geom = "area",
                fill = project_colors[1],
                xlim = c(-6, 6)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 2),
                geom = "area",
                fill = project_colors[2],
                xlim = c(-4, 4)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 2),
                geom = "area",
                fill = project_colors[3],
                xlim = c(-2, 2)) +
  # plot the curve
  stat_function(fun = dnorm, args = list(mean = 0, sd = 2),
                geom = "line",
                xlim = c(-10, 10),
                size = 2) +
  # scale x-axis
  xlim(-10, 10) +
  # label x-axis
  xlab("X") +
  # label ticks of x-axis
  scale_x_continuous(breaks = c(-6,-4,-2,0,2,4,6), 
                     labels = c(expression(-3~sigma),expression(-2~sigma),
                              expression(-sigma),"0",expression(sigma),
                              expression(2~sigma),expression(3~sigma)))
```

**Linear transformations**

1. If $X\sim Normal(\mu, \sigma^2)$ is linear transformed by $Y=a*X+b$, then the new random variable is again normal distributed with $Y \sim Normal(a\mu+b,a^2\sigma^2)$. 
2. Are $X\sim Normal(\mu_x, \sigma^2)$ and $Y\sim Normal(\mu_y, \sigma^2)$ normal distributed and independent, then their sum is again normal distributed with $X+Y \sim Normal(\mu_x+\mu_y, \sigma_x^2+\sigma_y^2)$. 

#### Hands On

```{r, ch-app-01-normal-distribution-shiny, fig.cap="App taken from http://www.artofstat.com/webapps.html [@klingenberg2017]"}
knitr::include_app("https://istats.shinyapps.io/NormalDist/", height = "800px")
```

### Chi-squared distribution {#app-91-distributions-chi2}

The $\chi^2$-distribution is widely used in hypothesis testing in inferential statistics, because many test statistics are approximately distributed as $\chi^2$-distribution. 

The $\chi^2$-distribution is directly related to the standard normal distribution: The sum of $n$ independent and standard normal distributed random variables $X_1,X_2,...,X_n$ is distributed according to a $\chi^2$ distribution with $n$ \emph{degrees of freedom}:

$$Y=X_1^2+X_2^2+...+X_n^2.$$

The $\chi^2$ distribution is a skew probability distribution with range $[0,+\infty)$ and only one parameter: $n$, the *degrees of freedom*: (If $n=1$, then $(0,+\infty)$.) 

$$X\sim \chi^2(n).$$

Fig.~\ref{fig:ch-app-01-chi-squared-distribution-density} shows the probability density function of three chi-squared distributed random variables with different values for the parameter. Notice, that with increasing degrees of freedom the chi-squared distribution approximates the normal distribution. For $n \geq 30$ the chi-squared distribution can be approximated by a normal distribution. Fig.~\ref{fig:ch-app-01-chi-squared-distribution-cumulative} shows the corresponding cumulative function of the three chi-squared density distributions.

```{r, ch-app-01-chi-squared-distribution-density, fig.cap = "Examples of probability density function of chi-squared distributions."}
rv_chisq <- tibble(
  x = seq(from = 0, to = 20, by = .01),
  y1 = dchisq(x, df = 2),
  y2 = dchisq(x, df = 4),
  y3 = dchisq(x, df = 9)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(2)",
                          parameter == "y2" ~ "(4)",
                          parameter == "y3" ~ "(9)")
  )

# dist plot
ggplot(rv_chisq, aes(x, y, color = parameter)) +
  geom_line(size = 2) +
  labs(color = "X ~ Chi-Squared", y = "Density")
```

```{r, ch-app-01-chi-squared-distribution-cumulative, fig.cap = "Examples of the cumulative distribution function of chi-squared distributions corresponding to the previous probability density functions."}
rv_chisq %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)/sum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_line(size = 2) +
    labs(color = "X ~ Chi-Squared", y = "y")
```

**Probability density function**
$$f(x)=\begin{cases}\frac{x^{\frac{n}{2}-1}e^{-\frac{x}{2}}}{2^{\frac{n}{2}}\Gamma (\frac{n}{2})} &\textrm{ for }x>0,\\ 0 &\textrm{ otherwise.}\end{cases}$$
Where $\Gamma (\frac{n}{2})$ denotes the Gamma function.

**Cumulative distribution function**
$$F(x)=\frac{\gamma (\frac{n}{2},\frac{x}{2})}{\Gamma \frac{n}{2},}$$
with $\gamma(s,t)$ being the lower incomplete gamma function:
$$\gamma(s,t)=\int_0^t t^{s-1}e^{-t} dt.$$

**Expected value** $E(X)=n$

**Variance** $Var(X)=2n$

**Transformations**
The sum of two $\chi^2$-distributed random variables $X \sim \chi^2(m)$ and $Y \sim \chi^2(n)$ is again a $chi^2$-distributed random variable $X+Y=\chi^2(m+n)$.

#### Hands On

```{r, ch-app-01-chi-squared-distribution-shiny, fig.cap="App taken from http://www.artofstat.com/webapps.html [@klingenberg2017]"}
knitr::include_app("https://istats.shinyapps.io/ChisqDist/", height = "800px")
```

### F distribution
The F distribution, named after R.A. Fisher, is used in particular in regression and variance analysis. It is defined by the ratio of two $chi^2$-distributed random variables $X\sim \chi^2(m)$ and $Y\sim \chi^2(n)$, each divided by its degree of freedom:

$$F=\frac{\frac{X}{m}}{\frac{Y}{n}}.$$
The F distribution is a continuous skew probability distribution with range $(0,+\infty)$ and two parameters $m$ and $n$, corresponding to the degrees of freedom of the two $chi^2$-distributed random variables:

$$X \sim F(m,n).$$

Fig.~\ref{fig:ch-app-01-F-distribution-density} shows the probability density function of three F distributed random variables with different parameter values. For a small number of degrees of freedom the density distribution is skewed to the left side. When the number increases, the density distribution gets more and more symmetric. Fig.~\ref{fig:ch-app-01-F-distribution-cumulative} shows the corresponding cumulative function of the three F density distributions.
 
```{r, ch-app-01-F-distribution-density, fig.cap = "Examples of probability density function of F distributions."}
rv_F <- tibble(
  x = seq(from = 0, to = 7, by = .01),
  y1 = df(x, df1 = 2, df2 = 4),
  y2 = df(x, df1 = 4, df2 = 6),
  y3 = df(x, df1 = 12, df2 = 12)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(2,4)",
                          parameter == "y2" ~ "(4,6)",
                          parameter == "y3" ~ "(12,12)")
  )

# dist plot
ggplot(rv_F, aes(x, y, color = parameter)) +
  geom_line(size = 2) +
  labs(color = "X ~ F", y = "Density")
```

```{r, ch-app-01-F-distribution-cumulative, fig.cap = "Examples of the cumulative distribution function of F distributions corresponding to the previous probability density functions."}
rv_F %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)/sum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_line(size = 2) +
    labs(color = "X ~ F", y = "y")
```

**Probability density function**
$$F(x)=m^{\frac{m}{2}}n^{\frac{n}{2}} \cdot \frac{\Gamma (\frac{m+n}{2})}{\Gamma (\frac{m}{2})\Gamma (\frac{n}{2})} \cdot \frac{x^{\frac{m}{2}-1}}{(mx+n)^{\frac{m+n}{2}}} \textrm{ for } x>0.$$
Where $\Gamma(x)$ denotes the gamma function.

**Cumulative distribution function**
$$F(x)=I\left(\frac{m \cdot x}{m \cdot x+n},\frac{m}{2},\frac{n}{2}\right),$$
with $I(z,a,b)$ being the regularized incomplete beta function:
$$I(z,a,b)=\frac{1}{B(a,b)} \cdot \int_0^z t^{a-1}(1-t)^{b-1} dt.$$

**Expected value** $E(X) = \frac{n}{n-2}$ (for $n \geq 3$)

**Variance** $Var(X) = \frac{2n^2(n+m-2)}{m(n-4)(n-2)^2}$ (for $n \geq 5$)

#### Hands On

```{r, ch-app-01-F-distribution-shiny, fig.cap="App taken from http://www.artofstat.com/webapps.html [@klingenberg2017]"}
knitr::include_app("https://istats.shinyapps.io/FDist/", height = "800px")
```

### Student's *t*-distribution   {#app-91-distributions-students-t}

The $t$ or Student's $t$ distribution was discovered by William S. Gosset in 1908 [@vallverdu2015], who published his work under the pseudonym "Student". He worked at the Guinness factory and had to deal with the problem of small sample sizes, where using a normal distribution as an approximation can be too crude. To overcome this problem Gosset conceived of the $t$ distribution. Accordingly, this distribution is used in particular when the sample size is small and the variance unknown, which is often the case in reality. Its shape ressembles the normal bell shape and has a peak at zero, but the $t$ distribution is a bit lower and wider (bigger tails) than the normal distribution. 

The $t$ distribution consists of a standard-normally distributed random variable $X \sim \text{Normal}(0,1)$ and a $\chi^2$-distributed random variable $Y \sim \chi^2(n)$ ($X$ and $Y$ are independent):

$$T = \frac{X}{\sqrt{Y / n}}.$$
The $t$ distribution has range $(-\infty,+\infty)$ and one parameter $\nu$, the degrees of freedom. The degrees of freedom can be calculated by the sample size $n$ minus one:
$$t \sim \text{Student-}t(\nu = n -1).$$

Fig.~\@ref(fig:ch-app-01-t-distribution-density) shows the probability density function of three $t$-distributed random variables with different parameters, and Fig.~\@ref(fig:ch-app-01-t-distribution-cumulative) shows the corresponding cumulative function of the three $t$ density distributions. Notice that for small degrees of freedom $\nu$, the $t$-distribution has bigger tails. This is because the $t$ distribution was specially designed to provide more conservative test results when analyzing small samples. When the degrees of freedom increases, the $t$-distribution approaches a normal distribution. For $\nu \geq 30$ this approximation is quite good. 
 
```{r, ch-app-01-t-distribution-density, fig.cap = "Examples of probability density functions of $t$ distribution."}
rv_student <- tibble(
  x = seq(from = -6, to = 6, by = .01),
  y1 = dt(x, df = 1),
  y2 = dt(x, df = 2),
  y3 = dt(x, df = 10)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(1)",
                          parameter == "y2" ~ "(2)",
                          parameter == "y3" ~ "(10)")
  )

# dist plot
ggplot(rv_student, aes(x, y, color = parameter)) +
  geom_line(size = 2) +
  labs(color = "X ~ t", y = "Density")
```

```{r, ch-app-01-t-distribution-cumulative, fig.cap = "Examples of the cumulative distribution function of $t$ distributions corresponding to the previous probability density functions."}
rv_student %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)/sum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_line(size = 2) +
    labs(color = "X ~ t", y = "y")
```

**Probability density function**
$$ f(x, \nu)=\frac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\nu\pi} \cdot \Gamma(\frac{\nu}{2})}\left(1+\frac{x^2}{\nu}\right)^{-\frac{\nu+1}{2}},$$
with $\Gamma(x)$ denoting the gamma function.

**Cumulative distribution function**
$$F(x, \nu)=I\left(\frac{x+\sqrt{x^2+\nu}}{2\sqrt{x^2+\nu}},\frac{\nu}{2},\frac{\nu}{2}\right),$$
where $I(z,a,b)$ denotes the regularized incomplete beta function:
$$I(z,a,b)=\frac{1}{B(a,b)} \cdot \int_0^z t^{a-1}(1-t)^{b-1} \text{d}t.$$

**Expected value** $E(X) = 0$

**Variance** $Var(X) = \frac{n}{n-2}$ (for $n \geq 30$)

#### Hands On

```{r, ch-app-01-t-distribution-shiny, fig.cap="App taken from http://www.artofstat.com/webapps.html [@klingenberg2017]"}
knitr::include_app("https://istats.shinyapps.io/tdist/", height = "800px")
```

### Beta distribution {#app-91-distributions-beta}

The beta distribution creates a continuous distribution of numbers between 0 and 1, therefore this distribution is useful if the uncertain quantity is bounded by 0 and 1 (or 100%), is continuous, and has a single mode. In Bayesian Data Analysis the beta distribution has a special standing as prior distribution for a bernoulli or binomial (see discrete distributions) likelihood. The reason for this is that a combination of a beta prior and a bernoulli (or binomial) liklihood results in a posterior distribution with the same form as the beta distribution. Such priors are referred to as *conjugate priors*.

A beta distribution has two parameters $a$ and $b$:
$$X \sim Beta(a,b).$$
The two parameters can be interpreted as the number of observations made, such that: $n=a+b$. If $a$ and $b$ get bigger, the beta distribution gets narrower. If only $a$ gets bigger the distribution moves rightward and if only $b$ gets bigger the distribution moves leftward. Thus, the parameters define the shape of the distribution, therefore they are also called *shape parameters*. A Beta(1,1) is equivalent to a uniform distribution. Fig.~\ref{fig:ch-app-01-beta-distribution-density} shows the probability density function of four beta distributed random variables with different parameter values. Fig.~\ref{fig:ch-app-01-beta-distribution-cumulative} shows the corresponding cumulative functions.

```{r, ch-app-01-beta-distribution-density, fig.cap = "Examples of probability density function of beta distributions."}
rv_beta <- tibble(
  x = seq(from = 0, to = 1, by = .01),
  y1 = dbeta(x, shape1 = 1, shape2 = 1),
  y2 = dbeta(x, shape1 = 4, shape2 = 4),
  y3 = dbeta(x, shape1 = 4, shape2 = 2),
  y4 = dbeta(x, shape1 = 2, shape2 = 4)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(1,1)",
                          parameter == "y2" ~ "(4,4)",
                          parameter == "y3" ~ "(4,2)",
                          parameter == "y4" ~ "(2,4)")
  )

# dist plot
ggplot(rv_beta, aes(x, y, color = parameter)) +
  geom_line(size = 2) +
  labs(color = "X ~ Beta", y = "Density")
```

```{r, ch-app-01-beta-distribution-cumulative, fig.cap = "Examples of the cumulative distribution function of beta distributions corresponding to the previous probability density functions."}
rv_beta %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)/sum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_line(size = 2) +
    labs(color = "X ~ Beta", y = "y")
```

**Probability density function**

$$f(x)=\frac{\theta^{(a-1)} (1-\theta)^{(b-1)}}{B(a,b)},$$
where $B(a,b)$ is the beta *function*:

$$B(a,b)=\int^1_0 \theta^{(a-1)} (1-\theta)^{(b-1)}d\theta.$$

**Cumulative distribution function**

$$F(x)=\frac{B(x;a,b)}{B(a,b)},$$
where $B(x;a,b)$ is the *incomplete beta function*:

$$B(x;a,b)=\int^x_0 t^{(a-1)} (1-t)^{(b-1)} dt,$$

and $B(a,b)$ the (complete) *beta function*

$$B(a,b)=\int^1_0 \theta^{(a-1)} (1-\theta)^{(b-1)}d\theta.$$

**Expected value** 
Mean: $E(X)=\frac{a}{a+b}$
Mode: $\omega=\frac{(a-1)}{a+b-2}$

**Variance** 
Variance: $Var(X)=\frac{ab}{(a+b)^2(a+b+1)}$
Concentration: $\kappa=a+b$ (related to variance such that, the bigger $a$ and $b$ are, the narrower the distribution)

**Reparameterization of the beta distribution**
Sometimes it is helpful (and more intuitive) to write the beta distribution in terms of its mode $\omega$ and concentration $\kappa$ instead of $a$ and $b$:

$$Beta(a,b)=Beta(\omega(\kappa-2)+1, (1-\omega)(\kappa-2)+1), \textrm{ for } \kappa > 2.$$

### Uniform distribution
The (continuous) uniform distribution takes values within a specified range $a$ and $b$ that have constant probability.  Sometimes the distribution is also called rectangular distribution, due to its shape of a rectangle. The uniform distribution is in particular common for random number generation. In Bayesian Data Analysis it is often used as prior distribution to express *ignorance*. This can be thought in the following way: When different events are possible but no (reliable) information exists about their probability of occurence, the most conservative (and also intuitive) choice would be to assign probability such that all events are equally likely to occur. The uniform distribution model this intuition, it generates a completely random number in some interval $[a,b)$.

The distribution is specified by two parameters: the end points $a$ (minimum) and $b$ (maximum): 
$$X \sim Unif(a,b).$$
When $a=0$ and $b=1$ the distribution is referred to as *standard* uniform distribution. Fig.~\ref{fig:ch-app-01-uniform-distribution-density} shows the probability density function of two uniform distributed random variables with different parameter values. Fig.~\ref{fig:ch-app-01-uniform-distribution-cumulative} shows the corresponding cumulative functions.

```{r, ch-app-01-uniform-distribution-density, fig.cap = "Examples of probability density function of uniform distributions."}
rv_unif <- tibble(
  x = seq(from = -.1, to = 4.2, by = .01),
  y1 = dunif(x),
  y2 = dunif(x, min = 2, max = 4)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = ifelse(parameter == "y1",
                       "(0,1)", "(2,4)")
  )

# dist plot
ggplot(rv_unif, aes(x, y, color = parameter)) +
  geom_line(size = 2) +
  labs(color = "X ~ Uniform", y = "Density")
```

```{r, ch-app-01-uniform-distribution-cumulative, fig.cap = "Examples of the cumulative distribution function of uniform distributions corresponding to the previous probability density functions."}
rv_unif %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)/sum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_line(size = 2) +
    labs(color = "X ~ Uniform", y = "y")
```

**Probability density function**

$$f(x)=\begin{cases} \frac{1}{b-a} &\textrm{ for } x \in [a,b],\\0 &\textrm{ otherwise.}\end{cases}$$

**Cumulative distribution function**

$$F(x)=\begin{cases}0 & \textrm{ for } x<a,\\\frac{x-a}{b-a} &\textrm{ for } a\leq x < b,\\ 1 &\textrm{ for }x \geq b. \end{cases}$$

**Expected value** $E(X)=\frac{a+b}{2}$

**Variance**  $Var(X)=\frac{(b-a)^2}{12}$

### Dirichlet distribution {#app-91-distributions-dirichlet}

The Dirichlet distribution is a multivariate generalisation of the Beta distribution: While Beta distribution is a distribution over binomials, the Dirichlet is a distribution over Multinomials. 

It can be used in any situation where an entity has to necessarily fall into one of $n+1$ mutually exclusive subclasses, and the goal is to study the proportion of entities belonging to the different subclasses.

The Dirichlet distributions are commonly used as *prior distributions* in Bayesian statistics, as this family is a *conjugate prior* for the categorical distribution and the multinomial distribution.

The Dirichlet distribution $\mathcal{Dir}(\alpha)$ is a family of continuous multivariate probability distributions, parameterized by a vector $\alpha$ of positive reals. Thus, it is a distribution with $k$ positive parameters $\alpha^k$ with respect to a $k$-dimensional space.


$$X \sim \mathcal{Dir}(\boldsymbol{\alpha})$$

The probability density function (see formula below) of the Dirichlet distribution for $k$ random variables is a $k-1$ dimensional probability *simplex* that exists on a $k$ dimensional space. How does the parameter $\alpha$ influence the Dirichlet distribution?

* Higher values of $\alpha_i$ lead to greater "weight" of $X_i$ and greater amount of the total "mass" assigned to it. (see plot 1)
* If $\alpha_1=...=\alpha_k=1$, then the points are uniformly distributed. (see plot 2)
* If all $\alpha_i$ are equal, the distribution is symmetric. (see plot 3 for asymmetric plot)
* Values of $\alpha_i<1$ can be thought as anti-weight that pushes away $x_i$ toward extremes. (see plot 4)


```{r, ch-app-01-dirichlet-distribution-density, fig.cap = "Examples of probability density function of dirichlet distributions."}
C <- matrix(c(10, 1, 2, 0.15, 10, 1, 10, 0.15, 10, 1, 5, 0.15), 4, 3)

f1 <- function(v) ddirichlet(v, C[1,])
f2 <- function(v) ddirichlet(v, C[2,])
f3 <- function(v) ddirichlet(v, C[3,])
f4 <- function(v) ddirichlet(v, C[4,])

mesh1 <- simplex_mesh(.0025) %>% as_tibble()
mesh2 <- simplex_mesh(.0025) %>% as_tibble()
mesh3 <- simplex_mesh(.0025) %>% as_tibble()
mesh4 <- simplex_mesh(.0025) %>% as_tibble()

mesh1$f1 <- mesh1 %>% apply(1, function(v) f1(bary2simp(v)))
mesh2$f2 <- mesh2 %>% apply(1, function(v) f2(bary2simp(v)))
mesh3$f3 <- mesh3 %>% apply(1, function(v) f3(bary2simp(v)))
mesh4$f4 <- mesh4 %>% apply(1, function(v) f4(bary2simp(v)))

points <- map_df(seq(nrow(C)), function(m){
  rdirichlet(250, C[m,]) %>% 
    simp2bary() %>% 
    as_tibble() %>% 
    transmute(
      mesh = paste0("f", m),
      x = V1, 
      y = V2
    )
}) %>% 
    mutate(
      mesh = recode(mesh, f1 = "(10, 10, 10)",
                          f2 = "(1, 1, 1)",
                          f3 = "(2, 10, 5)",
                          f4 = "(.15, .15, .15)")
    )

meshes <- left_join(mesh1, mesh2, by = c("x", "y")) %>% 
    left_join(mesh3, by = c("x", "y")) %>% 
    left_join(mesh4, by = c("x", "y")) %>% 
    pivot_longer(starts_with("f"), names_to = "mesh") %>% 
    mutate(
      mesh = recode(mesh, f1 = "(10, 10, 10)",
                          f2 = "(1, 1, 1)",
                          f3 = "(2, 10, 5)",
                          f4 = "(.15, .15, .15)")
    )

ggplot(meshes, aes(x, y)) +
  geom_raster(aes(fill = value), show.legend = FALSE) +
  coord_equal(xlim = c(0,1), ylim = c(0, .85)) +
  geom_point(data = points, color = "orange", size = .3) +
  facet_wrap(~mesh)
```

**Probability density function**

$$f(x)=\frac{\Gamma\left(\sum_{i=1}^{n+1} \alpha_i\right)}{\prod_{i=1}^{n+1}\Gamma(\alpha_i)}\prod_{i=1}^{n+1}p_i^{\alpha_i-1},$$
with *Gamma* denoting the gamma function and 

$$p_i=\frac{X_i}{\sum_{j=1}^{n+1}X_j}, 1\leq i\leq n,$$

where $X_1,X_2,...,X_{n+1}$ are independent Gamma random variables, with $X_i \sim G(\alpha_i,1)$.

**Expected value** $E(p_i)=\frac{\alpha_i}{t}, \textrm{ with } t=\sum_{i=1}^{n+1}\alpha_i$

**Variance** $Var(p_i)=\frac{\alpha_i(t-\alpha_i)}{t^2(t+1)}, \textrm{ with } t=\sum_{i=1}^{n+1}\alpha_i$

## Selected discrete distributions of random variables 

### Binomial distribution {#app-91-distributions-binomial}

The binomial distribution is a useful model for binary decisions where the outcome is a choice between two alternatives (e.g. Yes/No, Left/Right, Present/Absent, Head/Tail, ...). The two outcomes are coded as $0$ (failure) and $1$ (success). Consequently, let the probability of occurence of the outcome "success" be $p$, then the probability of occurence of "failure" is $1-p$. 
Consider a coin-flip experiment, with the outcomes "head" and "tail". If we flip a coin repeatedly, e.g. $30$ times, the successive trials are independent of each other and the probability $p$ is constant, then the resulting binomial distribution is a discrete random variable with outcomes $\{0,1,2,...,30\}$.  
The binomial distribution has two parameters "size" and "prob", often denoted as $n$ and $p$, respectively. The "size" refers to the number of trials and "prob" to the probability of success:
$$X \sim Binomial(n,p).$$

Fig.~\ref{fig:ch-app-01-binomial-distribution-mass} shows the probability mass function of three binomial distributed random variables with different parameter values. As stated above, $p$ refers to the probability of success. The higher this probability the more often we will observe the outcome coded with "1". Therefore the distribution tends toward the right side and vice-versa. The distribution gets more symmetrical if the parameter $p$ approximates 0.5. Fig.~\ref{fig:ch-app-01-binomial-distribution-cumulative} shows the corresponding cumulative functions.

```{r, ch-app-01-binomial-distribution-mass, fig.cap = "Examples of probability mass function of Binomial distributions."}
# how many trials
trials = 30

rv_binom <- tibble(
  x = seq(0, trials),
  y1 = dbinom(x, size = trials, p = 0.2),
  y2 = dbinom(x, size = trials, p = 0.5),
  y3 = dbinom(x, size = trials, p = 0.8)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(n,0.2)",
                          parameter == "y2" ~ "(n,0.5)",
                          parameter == "y3" ~ "(n,0.8)")
  )

# dist plot
ggplot(rv_binom, aes(x, y, fill = parameter)) +
  geom_col(position = "identity", alpha = 0.8) +
  labs(fill = "X ~ Binomial", y = "Probability")
```


```{r, ch-app-01-binomial-distribution-cumulative, fig.cap = "Examples of the cumulative distribution function of Binomial distributions corresponding to the previous probability mass functions."}
rv_binom %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)/sum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_step(size = 2) +
    labs(color = "X ~ Binomial", y = "y")
```

**Probability mass function**

$$f(x)=\binom{n}{x}p^x(1-p)^{n-x},$$
where $\binom{n}{x}$ is the binomial coefficient.

**Cumulative function**

$$F(x)=\sum_{k=0}^{x}\binom{n}{k}p^k(1-p)^{n-k}$$

**Expected value** $E(X)=n \cdot p$

**Variance** $Var(X)=n \cdot p \cdot (1-p)$

#### Hands On

```{r, ch-app-01-binomial-distribution-shiny, fig.cap="App taken from http://www.artofstat.com/webapps.html [@klingenberg2017]"}
knitr::include_app("https://istats.shinyapps.io/BinomialDist/", height = "800px")
```

### Multinomial distribution {#app-91-distributions-multinomial}

The Multinomial distribution is a generalization of the Binomial distribution to the case of $n$ repeated trials: While the Binomial distribution can have two outcomes, the Multinomial distribution can have multiple outcomes.
Consider an experiment where each trial can result in any of $k$ possible outcomes with a probability $p_i, \textrm{ where }(i=1,2,...,k)$, with $\sum_{i=1}^kp_i=1$. For $n$ repeated trials, let $k_i$ denote the number of times $X=x_i$ was observed, where $i=1,2,...,m$. It follows that $\sum_{i=1}^m k_i=n$.

**Probability mass function**

The probability of observing a vector of outcomes $\mathbf{k}=[k_1,...,k_m]^T$ is

$$f(\mathbf{k}|\mathbf{p})=\binom{n}{k_1\cdot k_2 \cdot...\cdot k_m} \prod_{i=1}^m p_i^{k_i},$$

where $\binom{n}{k_1\cdot k_2 \cdot...\cdot k_m}$ is the *multinomial coefficient*: $\binom{n}{k_1\cdot k_2 \cdot...\cdot k_m}=\frac{n!}{k_1!\cdot k_2! \cdot...\cdot k_m!}$. It is a generalization of the *binomial coefficient* $\binom{n}{k}$.

**Expected Values:** $E(X)=n\cdot p_i$

**Variance:** $Var(X)=n\cdot p_i\cdot (1-p_i)$

### Bernoulli distribution {#app-91-distributions-bernoulli}

The Bernoulli distribution is a special case of the binomial distribution with $size = 1$, therefore the outcome of a bernoulli random variable is either 0 or 1. Apart from that the same information holds as for the binomial distribution.
As the "size" parameter is now negligible, the bernoulli distribution has only one parameter, the probability of success $p$:
$$X \sim Bern(p).$$
Fig.~\ref{fig:ch-app-01-bernoulli-distribution-mass} shows the probability mass function of three bernoulli distributed random variables with different parameters. Fig.~\ref{fig:ch-app-01-bernoulli-distribution-cumulative} shows the corresponding cumulative distributions.

```{r, ch-app-01-bernoulli-distribution-mass, fig.cap = "Examples of probability mass function of Bernoulli distributions."}

rv_bern <- tibble(
  x = seq(from = 0, to = 1),
  y1 = dbern(x, prob = 0.2),
  y2 = dbern(x, prob = 0.5),
  y3 = dbern(x, prob = 0.8)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(0.2)",
                          parameter == "y2" ~ "(0.5)",
                          parameter == "y3" ~ "(0.8)")
  )

# dist plot
ggplot(rv_bern, aes(x, y, fill = parameter)) +
  geom_col(position = "dodge", color = "white") +
  labs(fill = "X ~ Bernoulli", y = "Probability") +
  scale_x_continuous(breaks = c(0.0,1.0), labels = c("0","1"), limits = c(-0.5,1.5))
```

```{r, ch-app-01-bernoulli-distribution-cumulative, fig.cap = "Examples of the cumulative distribution function of Bernoulli distributions corresponding to the previous probability mass functions."}

rv_bern %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)/sum(y),
    cum_y2 = cumsum(y)/sum(y)
  ) %>% 
  add_column(
    x2 = c(1,1,1,1.5,1.5,1.5)
  ) %>%  
  ungroup() %>% 

ggplot(aes(x, cum_y, color = parameter)) +
  geom_segment(aes(xend = x2, yend = cum_y2), size = 1.5, linetype = "dashed") +
  geom_segment(aes(x = -0.5, y = 0,xend = 0.0, yend = 0), size = 1.5, linetype = "dashed") +
  geom_point(aes(x, cum_y), size = 4) +
  labs(color = "X ~ Bernoulli", y = "y") +
   scale_x_continuous(breaks = c(0.0,1.0), labels = c("0","1"), limits = c(-0.5,1.5))
```

**Probability mass function**

$$f(x)=\begin{cases} p &\textrm{ if } x=1,\\ 1-p &\textrm{ if } x=0.\end{cases}$$

**Cumulative function**

$$F(x)=\begin{cases} 0 &\textrm{ if } x < 0, \\ 1-p &\textrm{ if } 0 \leq x <1,\\1 &\textrm{ if } x \geq 1.\end{cases}$$

**Expected value** $E(X)=p$

**Variance** $Var(X)=p \cdot (1-p)$

### Categorical distribution {#app-91-distributions-categorical}

The Categorical distribution, sometimes also referred to as *Multinoulli* distribution, is a generalization of the Bernoulli distribution for categorical random variables: While a Bernoulli distribution is a distribution over two alternatives, the Categorical is a distribution over multiple alternatives. For a single trial (e.g. a single die roll) the Categorical distribution is equal to the Multinomial distribution.

The Categorical distribution is parametrized by the probabilities assigned to each event. Let $p_i$ the probability assigned to outcome $i$. The set of $p_i$'s are the parameters, and are constrained by $\sum_{i=1}^kp_i=1$.

$$X \sim Cat(\mathbf{p})$$

```{r, ch-app-01-categorical-distribution-mass, fig.cap = "Examples of probability mass function of Categorical distributions."}

rv_cat <- tibble(
  x = seq(from = 0, to = 20),
  y1 = dcat(x, prob = c(0.2,0.3,0.4,0.1)),
  y2 = dcat(x, prob = c(0.1,0.1,0.5,0.3)),
  y3 = dcat(x, prob = c(0.4,0.3,0.1,0.2))
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(0.2,0.3,0.4,0.1)",
                          parameter == "y2" ~ "(0.1,0.1,0.5,0.3)",
                          parameter == "y3" ~ "(0.4,0.3,0.1,0.2)")
  )


# dist plot
ggplot(rv_cat, aes(x, y, fill = parameter)) +
  geom_col(position = "dodge", color = "white") +
  labs(fill = "X ~ Categorical", y = "Probability") +
  scale_x_continuous(limits = c(0,5), breaks = c(1,2,3,4))
```

```{r, ch-app-01-categorical-distribution-cumulative, fig.cap = "Examples of the cumulative distribution function of Categorical distributions corresponding to the previous probability mass functions."}

rv_cat %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)/sum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_step(size = 2) +
    labs(color = "X ~ Categorical", y = "y") +
    scale_x_continuous(limits = c(0,5), breaks = c(1,2,3,4))
```

**Probability mass function**

$$f(x|\mathbf{p})=\prod_{i=1}^kp_i^{\{x=i\}},$$

where $\{x=i\}$ evaluates to 1 if $x=i$, otherwise 0 and $\mathbf{p}={p_1,...,p_k}$ where $p_i$ is the probabability of seeing event $i$.

**Expected Value**: $E(\mathbf{x})=\mathbf{p}$

**Variance**: $Var(\mathbf{x})=\mathbf{p}\cdot(1-\mathbf{p})$

### Beta-Binomial distribution {#app-91-distributions-beta-binomial}

The beta-binomial distribution, as the name already indicates, is a mixture of a binomial and beta distribution. Remember, a binomial distribution is useful to model a binary choice with outcomes "0" and "1". The binomial distribution has two parameters $p$, the probability of success ("1"), and $n$, the number of trials. Furthermore we assume that the successive trials are independent and $p$ is constant. In a beta-binomial distribution $p$ is not anymore assumed to be constant (or fixed) but changes from trial to trial. Thus, a further assumption about the distribution of $p$ is made and here the beta distribution comes into play: the probability $p$ is assumed to be randomly drawn from a beta distribution with parameters $a$ and $b$. 
Therefore, the beta-binomial distribution has three parameters $n$, $a$ and $b$:
$$X \sim BetaBinom(n,a,b).$$
For large values of a and b the distribution approaches a binomial distribution. When $a=1$ and $b=1$ the distribution equals a discrete uniform distribution from 0 to $n$. When $n = 1$, the distribution equals a bernoulli distribution.

Fig.~\ref{fig:ch-app-01-betabinom-distribution-mass} shows the probability mass function of three beta-binomial distributed random variables with different parameter values. Fig.~\ref{fig:ch-app-01-betabinom-distribution-cumulative} shows the corresponding cumulative distributions.

```{r, ch-app-01-betabinom-distribution-mass, fig.cap = "Examples of probability mass function of Beta-Binomial distributions."}
# how many trials
trials = 30

rv_betabinom <- tibble(
  x = seq(from = 0, to = trials),
  y1 = dbbinom(x, size = trials, alpha = 4, beta = 4),
  y2 = dbbinom(x, size = trials, alpha = 2, beta = 4),
  y3 = dbbinom(x, size = trials, alpha = 1, beta = 1)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(n,4,4)",
                          parameter == "y2" ~ "(n,2,4)",
                          parameter == "y3" ~ "(n,1,1)")
  )

# dist plot
ggplot(rv_betabinom, aes(x, y, fill = parameter)) +
  geom_col(position = "identity", alpha = 0.7) +
  labs(fill = "X ~ Beta-Binomial", y = "Probability")
```

```{r, ch-app-01-betabinom-distribution-cumulative, fig.cap = "Examples of the cumulative distribution function of Beta-Binomial distributions corresponding to the previous probability mass functions."}
rv_betabinom %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)/sum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_step(size = 2) +
    labs(color = "X ~ Beta-Binomial", y = "y") 
```

**Probability mass function**

$$f(x)=\binom{n}{x} \frac{B(a+x,b+n-x)}{B(a,b)},$$

where $\binom{n}{x}$ is the binomial coefficient and $B(x)$ the beta *function* (see beta distribution).

**Cumulative function**

$$F(x)=\begin{cases} 0 &\textrm{ if } x<0,\\ \binom{n}{x} \frac{B(a+x,b+n-x)}{B(a,b)} {}_3F_2(n,a,b) &\textrm{ if } 0 \leq x < n,\\ 1 &\textrm{ if } x \geq n. \end{cases}$$
Where ${}_3F_2(n,a,b)$ is the generalized hypergeometric function. 

**Expected value** $E(X)=n \frac{a}{a+b}$

**Variance** $Var(X)=n \frac{ab}{(a+b)^2} \frac{a+b+n}{a+b+1}$

### Poisson distribution
A poisson distributed random variable represents the number of successes occurring in a given *time interval*. It gives the probability of a given number of events happening in a fixed interval of time. The poisson distribution is a limiting case of the binomial distribution when the number of trials becomes very large and the probability of success is small. For example the number of car accidents in Osnabrueck in the next month, the number of typing errors on a page, the number of interruptions generated by a CPU during T seconds, etc. 
Events described by a poisson distribution must fullfill the following conditions: they occur in non-overlapping intervals, they can not occur simultaneously and each event occurs at a constant rate. 

The poisson distribution has one parameter, the rate $\lambda$, sometimes also referred to as *intensity*:
$$X \sim Po(\lambda).$$
 
The parameter $\lambda$ can be thought of as the expected number of events in the time interval. Consequently, changing the rate parameter changes the probability of seeing different numbers of events in one interval. See Fig.~\ref{fig:ch-app-01-poisson-distribution-mass} for the probability mass function of three poisson distributed random variables with different parameter values. Notice, that the higher  $\lambda$ the more symmetrical gets the distribution. In fact, the poisson distribution can be approximated by a normal distribution for a rate paramter $\geq$ 10. Fig.~\ref{fig:ch-app-01-poisson-distribution-cumulative} shows the corresponding cumulative distributions.

```{r, ch-app-01-poisson-distribution-mass, fig.cap = "Examples of probability mass function of Poisson distributions."}

rv_pois <- tibble(
  x = seq(from = 0, to = 30, by = 1),
  y1 = dpois(x, lambda = 2),
  y2 = dpois(x, lambda = 8),
  y3 = dpois(x, lambda = 15)
) %>% 
  pivot_longer(cols = starts_with("y"),
               names_to  = "parameter",
               values_to = "y") %>% 
  mutate(
    parameter = case_when(parameter == "y1" ~ "(2)",
                          parameter == "y2" ~ "(8)",
                          parameter == "y3" ~ "(15)")
  )

# dist plot
ggplot(rv_pois, aes(x, y, fill = parameter)) +
  geom_col(alpha = 0.7, position = "identity") +
  labs(fill = "X ~ Poisson", y = "Density") 
```


```{r, ch-app-01-poisson-distribution-cumulative, fig.cap = "Examples of the cumulative distribution function of Poisson distributions corresponding to the previous probability mass functions."}
# cumdist plot
rv_pois %>% 
  group_by(parameter) %>% 
  mutate(
    cum_y = cumsum(y)/sum(y)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x, cum_y, color = parameter)) +
    geom_step(size = 2) +
    labs(color = "X ~ Poisson", y = "y")
```

**Probability mass function**

$$f(x)=\frac{\lambda^x}{x!}e^{-\lambda}$$

**Cumulative function**

$$F(x)=\sum_{k=0}^{x}\frac{\lambda^k}{k!}e^{-\lambda}$$

**Expected value** $E(X)= \lambda$

**Variance** $Var(X)=\lambda$

#### Hands On

```{r, ch-app-01-poisson-distribution-shiny, fig.cap="App taken from http://www.artofstat.com/webapps.html [@klingenberg2017]"}
knitr::include_app("https://istats.shinyapps.io/PoissonDist/", height = "800px")
```

## Understanding distributions as random variables

```{r, ch-app-01-random-variables-normal-distribution, fig.cap = "Transformation of a normal distributed random variable by addition and linear transformation."}

rv_normal_transform = tibble(
  x_axis = seq(from = -5, to = 5, by = .01),
  rv_norm_x = dnorm(x = x_axis, mean = 1, sd = 2),
  rv_norm_y = dnorm(x = x_axis, mean = 1.75, sd = 2.75),
  rv_linear_transform = 3*rv_norm_x+0.25,
  rv_summ_transform = rv_norm_x + rv_norm_y
) %>% 
  pivot_longer(cols = starts_with("rv"),
               names_to  = "random_variables",
               values_to = "y") %>% 
  mutate(
    random_variables = case_when(random_variables == "rv_norm_x" ~ "X ~ N(1,2)",
                          random_variables == "rv_norm_y" ~ "Y ~ N(1.75,2.75)",
                          random_variables == "rv_linear_transform" ~ "3*X+0.25~ N(3*1+0.25,3^2*2^2)",
                          random_variables == "rv_summ_transform" ~ "X+Y ~ N(1+1.75,2^2+2.75^2)"
                          )
  )

# dist plot
ggplot(rv_normal_transform, aes(x_axis, y, color = random_variables)) +
  geom_line(size = 2) +
  labs(color = "X,Y ~ Normal", y = "Density", x = "x") 
```

```{r, ch-app-01-random-variables-chi-distribution, fig.cap = "Create chi-distributed random variable by summation of standard normal random variables"}

rv_norm_transform = tibble(
  rv_norm_X = rnorm(n = 1e6),
  rv_norm_Y = rnorm(n = 1e6),
  rv_norm_Z = rnorm(n = 1e6),
  rv_summ_transform_1 = rv_norm_X^2+rv_norm_Y^2,
  rv_summ_transform_2 = rv_norm_X^2+rv_norm_Y^2+rv_norm_Z^2
) %>% 
  pivot_longer(cols = starts_with("rv_summ"),
               names_to  = "random_variables",
               values_to = "y") %>% 
  mutate(
    RVs = case_when(random_variables == "rv_summ_transform_1" ~ "X^2+Y^2",
                          random_variables == "rv_summ_transform_2" ~ "X^2+Y^2+Z^2"
    )
  )

rv_chi_transform = tibble(
  x_axis = seq(from = 0, to = 10, by = .01),
  rv_chi_1 = dchisq(x = x_axis, df = 2),
  rv_chi_2 = dchisq(x = x_axis, df = 3)
) %>% 
  pivot_longer(cols = starts_with("rv"),
               names_to  = "random_variables",
               values_to = "y") %>% 
  mutate(
    RVs = case_when(random_variables == "rv_chi_1" ~ "Chi(2)",
                          random_variables == "rv_chi_2" ~ "Chi(3)"
                          )
  )

# dist plots
p1 <- ggplot() +
  geom_line(rv_chi_transform, mapping = aes(x_axis, y, color = RVs), size = 2) +
  labs(y = "Density", x = "x") 
  
p2 <- ggplot() +
  geom_line(rv_norm_transform, mapping = aes(y, color = RVs), size = 2, stat = "density") +
  xlim(0,10) +
  ylim(0,0.5) +
  labs(y = "Density", x = "x") 

grid.arrange(p1,p2, ncol = 2)
```

```{r, ch-app-01-random-variables-t-distribution, fig.cap = "Create t-distributed random variable by divison of standard normal RV by chi-squared RV"}

rv_norm_chi_transform = tibble(
  rv_norm_X = rnorm(n = 1e6),
  rv_norm_Y = rnorm(n = 1e6),
  rv_norm_Z = rnorm(n = 1e6),
  rv_chi = rv_norm_Y^2+rv_norm_Z^2,
  rv_transform_t = rv_norm_X/sqrt(rv_chi/2)
) %>% 
  pivot_longer(cols = starts_with("rv_transform"),
               names_to  = "random_variables",
               values_to = "y") %>% 
  mutate(
    RVs = case_when(random_variables == "rv_transform_t" ~ "X/sqrt(chi/df)")
  )

rv_student = tibble(
  x_axis = seq(from = -5, to = 5, by = .01),
  rv_t_1 = dt(x = x_axis, df = 2)
) %>% 
  pivot_longer(cols = starts_with("rv"),
               names_to  = "random_variables",
               values_to = "y") %>% 
  mutate(
    RVs = case_when(random_variables == "rv_t" ~ "t(2)")
  )

# dist plots
p1 <- ggplot() +
  geom_line(rv_student, mapping = aes(x_axis, y), size = 2) +
  ylim(0,0.4) +
  labs(y = "Density", x = "x") 
  
  
p2 <- ggplot() +
  geom_line(rv_norm_chi_transform, mapping = aes(y), size = 2, stat = "density") +
  xlim(-5,5) +
  ylim(0,0.4) +
  labs(y = "Density", x = "x") 
  
grid.arrange(arrangeGrob(p1, top = "RV ~ t(2)"), arrangeGrob(p2, top = "RV ~ std.norm/sqrt(chi/2)"), ncol=2)
```

```{r, ch-app-01-random-variables-F-distribution, fig.cap = "Create F-distributed random variable by divison of chi-squared RV by (another independent) chi-squared RV"}

rv_norm_chi_transform = tibble(
  rv_norm_V = rnorm(n = 1e6),
  rv_norm_W = rnorm(n = 1e6),
  rv_norm_X = rnorm(n = 1e6),
  rv_norm_Y = rnorm(n = 1e6),
  rv_norm_Z = rnorm(n = 1e6),
  rv_chi_1 = rv_norm_V^2+rv_norm_W^2,
  rv_chi_2 = rv_norm_X^2+rv_norm_Y^2+rv_norm_Z^2,
  rv_transform_F = (rv_chi_1/2)/(rv_chi_2/3)
) %>% 
  pivot_longer(cols = starts_with("rv_transform"),
               names_to  = "random_variables",
               values_to = "y") %>% 
  mutate(
    RVs = case_when(random_variables == "rv_transform_F" ~ "(rv_chi_1/2)/(rv_chi_2/3)")
  )

rv_fisher = tibble(
  x_axis = seq(from = 0, to = 20, by = .01),
  rv_F = df(x = x_axis, df1 = 2, df2 = 3)
) %>% 
  pivot_longer(cols = starts_with("rv"),
               names_to  = "random_variables",
               values_to = "y") %>% 
  mutate(
    RVs = case_when(random_variables == "rv_F" ~ "F(2,3)")
  )

# dist plots
p1 <- ggplot() +
  geom_line(rv_fisher, mapping = aes(x_axis, y), size = 2) +
  ylim(0,1) +
  labs(y = "Density", x = "x") 
  
  
p2 <- ggplot() +
  geom_line(rv_norm_chi_transform, mapping = aes(y), size = 2, stat = "density") +
  xlim(0,20) +
  ylim(0,1) +
  labs(y = "Density", x = "x") 
  
grid.arrange(arrangeGrob(p1, top = "RV ~ F(2,3)"), arrangeGrob(p2, top = "RV ~ (rv_chi_1/2)/(rv_chi_2/3)"), ncol = 2)
```
