# Generalized linear model {#Chap-04-04-GLM}

<hr>

So far our regression models all had a metric dependent variable.
In this chapter we are going to look at the basic architecture of **generalized linear models** (GLM), a class of regression models that allow for different types of dependent variables, like binary, nominal, ordinal or count data.
The focus of this chapter is on the main idea of the generalization, and to showcase the GLM family with one concrete example, namely **logistic regression**, which is used for a binary dependent variable.


```{block, type='infobox'}
The learning goals for this chapter are:

- understand how to generalize linear regression modeling to other kinds of dependent variables
- be able to run and interpret a logistic regression model with `brms`
```

## Generalizing the linear regression model

The general architecture of a (Bayesian) generalized regression model is shown in Figure \@ref(fig:Chap-04-04-GLM-scheme).
Based on a predictor matrix $X$ and concrete values for the regression coefficients $\beta$, the heart of linear regression modeling is the **linear predictor term**:

$$
\xi = X \beta
$$

The linear predictor is transformed in some way or other by what is called a **link function** $LF$ to yield the **predicted central tendency**:

$$
\eta = LF(\xi, \theta_{LF})
$$

The link function (also sometimes specified as an *inverse* link function) may additionally have free model parameters $\theta_{LF}$.
The predicted central tendency $\eta$ then serves an argument in a **likelihood function** $LH$, which needs to be appropriate for the data to be explained and which, too, may have additional free model parameters $\theta_{LH}$:

$$
y \sim LH(\eta, \theta_{LH})
$$

```{r Chap-04-04-GLM-scheme, echo = F, fig.cap="Basic architecture of generalized linear regression models."}
knitr::include_graphics("visuals/glm_scheme/glm_scheme.png")
```


The standard linear regression, which was covered in the previous chapters, is subsumed under this GLM scheme.
To see this, consider the following representation of a (Bayesian) linear regression model:

$$
\begin{align*}
\beta, \sigma & \sim \text{some prior} \\
\xi  & = X \beta && \text{[linear predictor]} \\ 
\eta  & = \xi && \text{[predictor of central tendency]} \\ 
y & \sim \text{Normal}(\eta, \sigma) && \text{[likelihood]}  
\end{align*}
$$
So, the standard linear regression model is the special case of the GLM scheme of Figure \@ref(fig:Chap-04-04-GLM-scheme) in which (i) the (inverse) link function is the identity map, so that the linear predictor *is* the predicted central tendency $\xi = \eta$ and (ii) the likelihood function is the normal distribution (with additional parameter $\theta_{LF} = \sigma$).

The need to have other likelihood functions arises when the data $y$ to be predicted is not plausibly generated by a normal distribution.
Take for instance the binary outcome of a coin flip, where a Bernoulli distribution is the natural choice for the probability of a single outcome / data observation.
The Bernoulli distribution has a single parameter $\theta_c$ (the bias of the coin), and can be written as follows, where it is assumed that $y$ takes values 0 or 1, as usual:

$$
y \sim \text{Bernoulli}(\theta_c) = {\theta_c}^y (1- \theta_c)^{1-y}
$$
So, *if* we want a Bernoulli likelihood function (which makes perfect sense of binary outcomes) our prediction of central tendency $\eta$ should be a latent coin bias $\theta_c$, rather than the mean of a normal distribution.
Since the coin bias is bounded $\theta_c \in [0;1]$, but our linear predictor $\xi \in \mathbb{R}$ is not, a link function is needed which maps (in a suitable way) a real-valued linear predictor $\xi$ onto a bounded predictor of central tendency $\eta = \theta_c$.
Whence the need for a link function: we just need to make sure that the linear predictor (a nice, well-behaved construct) maps onto appropriate values that the likelihood function (whose nature is dictated by the (assumed) nature of the data) requires.
In the case of binary data, a good choice of a link function is the **logistic function**.
The next section looks at the resulting **logistic regression** model in more detail.

Other combinations of (inverse) link functions and likelihood functions give rise to other kinds of commonly used instances of generalized linear models.
Below is a concise list of some of the common instances:


| type of $y$ | (inverse) link function | likelihood function | 
|:---|:---:|:---:|
| metric |  $\eta = \xi$ | $y \sim \text{Normal}(\mu = \eta, \sigma)$
| binary | $\eta = \text{logistic}(\xi) = (1 + \exp(-\xi))^{-1}$ | $y \sim \text{Bernoulli}(\eta)$
| nominal | $\eta_k = \text{soft-max}(\xi_k, \lambda) \propto \exp(\lambda \xi_k)$ | $y \sim \text{Multinomial}({\eta})$
| ordinal | $\eta_k = \text{threshold-Phi}(\xi_k, \sigma, {\delta})$ | $y \sim \text{Multinomial}({\eta})$
| count | $\eta = \exp(\xi)$ | $y \sim \text{Poisson}(\eta)$

## Logistic regression

Suppose $y \in \{0,1\}^n$ is an $n$-placed vector of binary outcomes, and $X$ a predictor matrix for a linear regression model.
A Bayesian logistic regression model has the following form:

$$
\begin{align*}
\beta, \sigma & \sim \text{some prior} \\
\xi  & = X \beta                            && \text{[linear predictor]} \\   
\eta_i  & = \text{logistic}(\xi_i)          && \text{[predictor of central tendency]} \\  
y_i & \sim \text{Bernoulli}(\eta_i)         && \text{[likelihood]} \\ 
\end{align*}
$$
The logistic function used as a link function is a function in $\mathbb{R} \rightarrow [0;1]$, i.e., from the reals to the unit interval.
It is defined as:

$$\text{logistic}(\xi_i) = (1 + \exp(-\xi_i))^{-1}$$
It's shape (a sigmoid, or S-shaped curve) is this:

```{r, echo = F}
myFun1 = function(x) return( 1 / (1 + exp(- 1 * (x - 0))) )
ggplot(data.frame(x = c(-5,5)), aes(x)) +
         stat_function(fun = myFun1, color = project_colors[2], size = 2) +
  labs(label = "logistic function", x = latex2exp::TeX("$\\eta_i$"), y = latex2exp::TeX("logistic($\\eta_i$)"))
```  

<div style = "float:right; width:20%;">
<img src="visuals/badge-Simon-task.png" alt="badge model comparison">
</div>

We use the [Simon task data](#app-93-data-sets-simon-task) as an example application.
So far we only tested the first of two hypotheses about the Simon task data, namely the hypothesis relating to reaction times.
The second hypothesis which arose in the context of the Simon task refers to the accuracy of answers, i.e., the proportion of "correct" choices:

$$
\text{Accuracy}_{\text{correct},\ \text{congruent}} > \text{Accuracy}_{\text{correct},\ \text{incongruent}}
$$
Notice that `correctness` is a binary categorical variable.
Therefore, we use logistic regression to test this hypothesis.

Here is how to set up a logistic regression model with `brms`.
The only thing that is new here is that we specify explicitly the likelihood function and the (inverse!) link function.^[Notice that the logit function is the inverse of the logistic function.]
This is done using the syntax `family = bernoulli(link = "logit")`.
For later hypothesis testing we also use proper priors and take samples from the prior as well.

```{r, eval = F}
fit_brms_ST_Acc = brm(
  # regress 'correctness' against 'condition'
  formula = correctness ~ condition, 
  # specify link and likeihood function
  family = bernoulli(link = "logit"),
  # which data to use
  data = aida::data_ST %>% 
    # 'reorder' answer categories (making 'correct' the target to be explained)
    mutate(correctness = correctness == 'correct'),
  # weakly informative priors (slightly conservative)
  #   for `class = 'b'` (i.e., all slopes)
  prior = prior(student_t(1, 0, 2),  class = 'b'),
  # also collect samples from the prior (for point-valued testing)
  sample_prior = 'yes',
  # take more than the usual samples (for numerical stability of testing)
  iter = 20000
)
```

```{r echo = F, eval = F}
saveRDS(object = fit_brms_ST_Acc, file = "../models_brms/simon-task-Acc.rds")
```

```{r echo = F}
fit_brms_ST_Acc <- readRDS("models_brms/simon-task-Acc.rds")
```

The Bayesian summary statistics of the posterior samples of values for regression coefficients are:

```{r}
summary(fit_brms_ST_Acc)$fixed[,c("l-95% CI", "Estimate", "u-95% CI")]
```

What do these specific numerical estimates for coefficients mean?
The mean estimate for the linear predictor $\xi_\text{cong}$ for the "congruent" condition is roughly `r summary(fit_brms_ST_Acc)$fixed["Intercept","Estimate"] %>% round(3)`.
The mean estimate for the linear predictor $\xi_\text{inc}$ for the "incongruent" condition is roughly `r summary(fit_brms_ST_Acc)$fixed["Intercept","Estimate"] %>% round(3)` + `r summary(fit_brms_ST_Acc)$fixed["conditionincongruent","Estimate"] %>% round(3)`, so roughly `r (summary(fit_brms_ST_Acc)$fixed["Intercept","Estimate"] +  summary(fit_brms_ST_Acc)$fixed["conditionincongruent","Estimate"]) %>% round(3)`.
The central predictors corresponding to these linear predictors are:

```{r echo = F}
logistic <- function(x) {
  1 / (1 + exp(-x))
}
```

$$
\begin{align*}
\eta_\text{cong} & = \text{logistic}(`r summary(fit_brms_ST_Acc)$fixed["Intercept","Estimate"] %>% round(3)`) \approx `r logistic(summary(fit_brms_ST_Acc)$fixed["Intercept","Estimate"]) %>% round(3)` \\
\eta_\text{incon} & = \text{logistic}(`r (summary(fit_brms_ST_Acc)$fixed["Intercept","Estimate"] + summary(fit_brms_ST_Acc)$fixed["conditionincongruent","Estimate"]) %>% round(3)`) \approx `r logistic(summary(fit_brms_ST_Acc)$fixed["Intercept","Estimate"] + summary(fit_brms_ST_Acc)$fixed["conditionincongruent","Estimate"]) %>% round(3)`
\end{align*}
$$

These central estimates for the latent proportion of "correct" answers in each condition tightly match the empirically observed proportion of "correct" answers in the data:


```{r, echo = T}
proportions_correct_ST <- data_ST %>% 
  group_by(condition, correctness) %>% 
  dplyr::count() %>% 
  group_by(condition) %>% 
  mutate(proportion_correct = (n / sum(n)) %>% round(3)) %>% 
  filter( correctness == "correct") %>% 
  select(-n, -correctness) 
proportions_correct_ST
```

Testing hypothesis for a logistic regression model is the exact same as for a standard regression model.
And so, we find very strong support for hypothesis 2, suggesting that (given model and data), there is reason to believe that the accuracy in incongruent trials is lower than in congruent trials.

```{r}
brms::hypothesis(fit_brms_ST_Acc, "conditionincongruent < 0")
```





