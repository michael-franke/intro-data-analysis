# Bayesian parameter estimation {#ch-03-04-parameter-estimation}

<hr>

<div style = "float:right; width:40%;">
<img src="visuals/badge-estimation.png" alt="badge estimation">  
</div>  

Based on a model $M$ with parameters $\theta$, parameter estimation addresses the question of which values of $\theta$ are good estimates, given some data $D$. 
This chapter deals specifically with _Bayesian_ parameter estimation. 
Given a Bayesian model $M$, we can use Bayes rule to update prior beliefs about $\theta$ to obtain so-called _posterior beliefs_ $P_M(\theta \mid D)$, which represent the new beliefs after observing $D$ and updating in a conservative, rational manner based on the assumptions spelled out in $M$.
We will see two different methods of computing posterior distributions $P_M(\theta \mid D)$, a precise mathematical derivation with limited applicability in terms of so-called **conjugate priors**, and an efficient but approximate sampling method based on so-called **Markov Chain Monte Carlo algorithms**.
The chapter also introduces common **point-valued and interval-ranged estimates** for parameters, in particular the Bayesian measures of **posterior mean** and **credible intervals**.
We will also learn about the **posterior predictive distribution** and how to draw inferences about hypotheses about specific parameter values. 


```{block, type='infobox'}
The learning goals for this chapter are:

- understand how Bayes rule applies to parameter estimation
  - role of prior and likelihood
  - understand the notion of conjugate prior
- understand and compute point-valued and interval-range estimators
  - MLE, MAP, posterior mean
  - (Bayesian) credible intervals
- understand the basic ideas behind MCMC sampling algorithms 
- understand the notion of a _posterior predictive distribution_
- learn how to use Bayesian parameter inference to test hypotheses about parameter values

```

## Bayes rule for parameter estimation  {#ch-03-03-estimation-bayes}

### Definitions and terminology

Fix a Bayesian model $M$ with likelihood $P(D \mid \theta)$ for observed data $D$ and prior over parameters $P(\theta)$. We then update our prior beliefs $P(\theta)$ to obtain posterior beliefs by Bayes rule:^[Since parameter estimation is only about one model, it is harmless to omit the index $M$ in the probability notation.]

$$P(\theta \mid D) = \frac{P(D \mid \theta) \ P(\theta)}{P(D)}$$

The ingredients of this equation are:

- the **posterior distribution** $P(\theta \mid D)$  -  our posterior beliefs about how likely each value of $\theta$ is given $D$;
- the **likelihood function** $P(D \mid \theta)$  -  how likely each observation of $D$ is for a fixed $\theta$;
- the **prior distribution** $P(\theta)$  - our initial (*prior*) beliefs about how likely each value of $\theta$ might be;
- the **marginal likelihood** $P(D) = \int P(D \mid \theta) \ P(\theta) \ \text{d}\theta$ -  how likely an observation of $D$ is under our prior beliefs about $\theta$ (a.k.a., the prior predictive probability of $D$ according to $M$)

A frequently used shorthand notation for probabilities is this:

$$\underbrace{P(\theta \, | \, D)}_{posterior} \propto \underbrace{P(\theta)}_{prior} \ \underbrace{P(D \, | \, \theta)}_{likelihood}$$

where the "proportional to" sign $\propto$ indicates that the probabilities on the LHS are defined in terms of the quantity on the RHS after normalization. So, if $F \colon X \rightarrow \mathbb{R}^+$ is a positive function of non-normalized probabilities (assuming, for simplicity, finite $X$), $P(x) \propto F(x)$ is equivalent to $P(x) = \frac{F(x)}{\sum_{x' \in X} F(x')}$.

### The effects of prior and likelihood on the posterior

The shorthand notation for the posterior $P(\theta \, | \, D) \propto P(\theta) \ P(D \, | \, \theta)$ makes it particularly clear that the posterior distribution is a "mix" of prior and likelihood.
Let's first explore this "mixing property" of the posterior before worrying about how to compute posteriors concretely.

We consider the case of flipping a coin with unknown bias $\theta$ a total of $N$ times and observing $k$ heads (= successes). This is modeled with the  **Binomial Model** (see Section \@ref(Chap-03-03-models-general)), using priors expressed with a [Beta distribution](#app-91-distributions-beta), giving us a model specification as: 

$$ 
\begin{aligned}
k & \sim \text{Binomial}(N, \theta) \\ 
\theta & \sim \text{Beta}(a, b)
\end{aligned}
$$

<div style = "float:right; width:8%;">
<img src="visuals/skull_king.png" alt="badge-data-wrangling">  
</div>  

To study the impact of the likelihood function, we compare two data sets. The first one is the contrived "24/7" example where $N = 24$ and $k = 7$. The second example uses a much larger naturalistic data set stemming from the [King of France](#app-93-data-sets-king-of-france) example, namely $k = 109$ for $N = 311$. These numbers are the number of "true" responses and the total number of responses for all conditions except Condition 1, which did not involve a presupposition. 


```{r}
data_KoF_cleaned <- aida::data_KoF_cleaned
```

```{r}
data_KoF_cleaned %>% 
  filter(condition != "Condition 1") %>% 
  group_by(response) %>% 
  dplyr::count()
```


The likelihood function for both data sets is plotted in Figure \@ref(fig:ch-03-03-estimation-likelihood-functions). 
The most important thing to notice is that the more data we have (as in the KoF example), the narrower the range of parameter values that make the data likely.
Intuitively, this means that the more data we have, the more severely constrained the range of _a posteriori_ plausible parameter values will be, all else equal.


```{r ch-03-03-estimation-likelihood-functions, echo = F, fig.cap = "Likelihood for two examples of binomial data. The first example has $k = 7$ and $N = 24$. The second has $k = 109$ and $N = 311$."}
lh_tibble <- 
  tibble(
  theta = seq(0,1, length.out = 401),
  `24/7` = dbinom(7, 24, theta),
  `King of France` = dbinom(109, 311, theta)
) 

lh_tibble %>% 
  pivot_longer(
    cols = -1,
    names_to = "example",
    values_to = "likelihood"
  ) %>% 
  ggplot(aes(x = theta, y = likelihood, color = example)) +
  geom_line(size = 3) +
  facet_wrap(.~example, nrow = 2, scales = "free") +
  guides(color = F) +
    labs(
    x = latex2exp::TeX("Bias $\\theta$"),
    y = latex2exp::TeX("Likelihood $P(D | \\theta)$"),
    title = latex2exp::TeX("Binomial likelihood function for different data sets.")
  )
```



Picking up the example from Section \@ref(Chap-03-02-models-priors), we will consider the four types of priors show below in Figure \@ref(fig:ch-03-03-estimation-types-of-priors).
 
 
 ```{r ch-03-03-estimation-types-of-priors, echo = F, fig.cap = "Examples of different kinds of Bayesian priors for the Binomial Model."}
prior_tibble <- tibble(
  theta = seq(0, 1, length.out = 401),
  uninformative = dbeta(theta, 1, 1),
  `weakly inf.` = dbeta(theta, 5, 2),
  `strongly inf.` = dbeta(theta, 50, 20),
  `point-valued` = dbeta(theta, 50000, 20000)
)

prior_tibble %>% 
  pivot_longer(
    cols = -1,
    names_to = "prior_type",
    values_to = "prior"
  ) %>%
  mutate(
    prior_type = factor(prior_type, levels = c('uninformative', 'weakly inf.', 'strongly inf.', 'point-valued'))
  ) %>% 
  ggplot(aes(x = theta, y = prior)) +
  geom_line(size = 2) +
  facet_wrap(~ prior_type, ncol = 2, scales = "free") +
  labs(
    x = latex2exp::TeX("Bias $\\theta$"),
    y = latex2exp::TeX("Prior probability $P(\\theta)$"),
    title = latex2exp::TeX("Different kinds of priors over bias $\\theta$"),
    subtitle = latex2exp::TeX("Binomial Model family")
  ) 

```

Combining the four different priors and the two different data sets, we see that the posterior is indeed a mix of prior and likelihood. In particular, we see that the weakly informative prior has only little effect if there are many data points (the KoF data), but does affect the posterior of the 24/7 case (compared against the uninformative prior).

```{r ch-03-03-estimation-posteriors-comparison, echo = F, fig.cap = "Posterior beliefs over bias parameter $\\theta$ under different priors and different data sets. We see that strongly informative priors have more influence on the posterior than weakly informative priors, and that the influence of the prior is stronger for less data than for more."}
full_join(lh_tibble, prior_tibble, by = 'theta') %>% 
    pivot_longer(
    cols = 2:3,
    names_to = "example",
    values_to = "likelihood"
  ) %>% 
  pivot_longer(
    cols = 2:5,
    names_to = "prior_type",
    values_to = "prior"
  ) %>% 
  mutate(
    posterior = likelihood * prior
  ) %>%  
  filter( posterior != Inf) %>% 
  group_by( example, prior_type) %>% 
  mutate(
    posterior = posterior/sum(posterior)
  ) %>%
  mutate(
    prior_type = factor(prior_type, levels = c('uninformative', 'weakly inf.', 'strongly inf.', 'point-valued'))
  ) %>%
  ggplot(aes(x = theta, y = posterior, color = example)) +
  geom_line(size = 2) +
  guides(color = F) +
  facet_grid(prior_type ~ example, scales = "free") +
  theme(strip.text.y = element_text(size = 9, face = 'plain'))+
  labs(
    x = latex2exp::TeX("Bias $\\theta$"),
    y = latex2exp::TeX("Posterior probability $P(\\theta \\, | \\, D)$"),
    title = latex2exp::TeX("Posterior beliefs in $\\theta$ for different priors and data.")
  ) 
```

<div class = "exercises">
**Exercise 9.1**

a. Use the WebPPL code below to explore the effects of priors and different observations in the Binomial Model in order to be able to answer the questions in the second part below. Ask yourself how you need to change parameters in such a way as to:
  - make the contribution of the likelihood function stronger
  - make the prior more informative

<pre class="webppl">
// select your parameters here
var k = 7    // observed successes (heads)
var N = 24   // total flips of a coin
var a = 1    // first shape parameter of beta prior
var b = 1    // second shape parameter of beta prior
var n_samples = 50000  // number of samples for approximation
///fold:
display("Prior distribution")
var prior = function() {
  beta(a, b)
}
viz(Infer({method: "rejection", samples: n_samples}, prior))

display("\nPosterior distribution")
var posterior = function() {
  beta(k + a, N - k + b)
}
viz(Infer({method: "rejection", samples: n_samples}, posterior))
///
</pre>

<script>
// find all <pre> elements and set up the editor on them
var preEls = Array.prototype.slice.call(document.getElementsByClassName("webppl"));
preEls.map(function(el) { console.log(el); editor.setup(el, {language: 'webppl'}); });
</script> 

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

To make the influence of the likelihood function stronger, we need more data. Try increasing variables `N` and `k` without changing their ratio.

To make the prior more strongly informative, you should increase the shape parameters `a` and `b`. 

</div>
</div>

b. Based on your explorations of the WebPPL code, which of the following statements do you think is true?

1. The prior always influences the posterior more than the likelihood.

2. The less informative the prior, the more the posterior is influenced by it.

3. The posterior is more influenced by the likelihood the less informative the prior is.

4. The likelihood always influences the posterior more than the prior.

5. The likelihood has no influence on the posterior in case of a point-valued prior (assuming a single-parameter model).


<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">


1. False

2. False

3. True

4. False

5. True

</div>
</div>

</div>

### Computing Bayesian posteriors with conjugate priors {#ch-03-04-parameter-estimation-conjugacy}

Bayesian posterior distributions can be hard to compute. Almost always, the prior $P(\theta)$ is easy to compute (otherwise, we might choose a different one for practicality). Usually, the likelihood function $P(D \mid \theta)$ is also fast to compute. Everything seems innocuous when we just write:

$$\underbrace{P(\theta \, | \, D)}_{posterior} \propto \underbrace{P(\theta)}_{prior} \ \underbrace{P(D \, | \, \theta)}_{likelihood}$$

But the real pain is the normalizing constant, i.e., the marginalized likelihood a.k.a. the "integral of doom", which to compute can be intractable, especially if the parameter space is large and not well-behaved:

$$P(D) = \int P(D \mid \theta) \ P(\theta) \ \text{d}\theta$$

Section \@ref(Ch-03-03-estimation-algorithms) will, therefore, enlarge on methods to compute or approximate the posterior distribution efficiently.

Fortunately, the computation of Bayesian posterior distributions can be quite simple in special cases. If the prior and the likelihood function cooperate, so to speak, the computation of the posterior can be as simple as sleep. The nature of the data often prescribes which likelihood function is plausible. But we have more wiggle room in the choice of the priors. If prior $P(\theta)$ and posterior $P(\theta \, | \, D)$ are of the same family, i.e., if they are the same kind of distribution albeit possibly with different parameterizations, we say that they **conjugate**. In that case, the prior $P(\theta)$ is called **conjugate prior** for the likelihood function $P(D \, | \, \theta)$ from which the posterior $P(\theta \, | \, D)$ is derived.

<div class = "mathstuff">

```{theorem, "Conjugacy-beta-binomial"}
The Beta distribution is the conjugate prior of binomial likelihood. For $\theta \sim \text{Beta}(a,b)$ as prior and data $k$ and $N$, the posterior is $\theta \sim \text{Beta}(a+k, b+ N-k)$.
```


<div class="collapsibleProof">
<button class="trigger">Show proof.</button>
<div class="content">

```{proof}
By construction, the posterior is:
$$P(\theta \mid \langle{k, N \rangle}) \propto \text{Binomial}(k ; N, \theta) \ \text{Beta}(\theta \, | \, a, b) $$


We extend the RHS by definitions, while omitting the normalizing constants:

$$
\begin{aligned} 
\text{Binomial}(k ; N, \theta) \ \text{Beta}(\theta \, | \, a, b) & 
  \propto \theta^{k} \, (1-\theta)^{N-k} \, \theta^{a-1} \, (1-\theta)^{b-1}  \\  &
  = \theta^{k + a - 1} \, (1-\theta)^{N-k +b -1}
\end{aligned}
$$

This latter expression is the non-normalized Beta-distribution for parameters $a + k$ and $b + N - k$, so that we conclude with what was to be shown:

$$
\begin{aligned} 
P(\theta \mid \tuple{k, N}) & = \text{Beta}(\theta \, | \, a + k, b+ N-k)
\end{aligned}
$$
```

&nbsp;

</div>
</div>
</div>



<div class = "exercises">
**Exercise 9.2**  

a. Fill in the blanks in the code below to get a plot of the posterior distribution for the coin flip scenario with $k=20$, $N=24$, making use of conjugacy and starting with a uniform Beta prior.

```{r, eval=F}
theta = seq(0, 1, length.out = 401)

as_tibble(theta) %>%
  mutate(posterior =  ____ ) %>%
  ggplot(aes(___, posterior)) +
   geom_line()

```

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

```{r}
theta <- seq(0, 1, length.out = 401)

as_tibble(theta) %>%
  mutate(posterior = dbeta(theta, 21, 5)) %>%
  ggplot(aes(theta, posterior)) +
   geom_line()

```

</div>
</div>

b. Suppose that Jones flipped a coin with unknown bias 30 times. She observed 20 heads. She updates her beliefs rationally with Bayes rule. Her posterior beliefs have the form of a beta distribution with parameters $\alpha = 25$, $\beta = 15$. What distribution and what parameter values of that distribution capture Jones' prior beliefs before updating her beliefs with this data?

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

 $\text{Beta}(5,5)$

</div>
</div>

</div>

### Excursion: Sequential updating

Ancient wisdom has coined the widely popular proverb: "Today's posterior is tomorrow's prior." Suppose we collected data from an experiment, like $k = 7$ in $N = 24$. Using uninformative priors at the outset, our posterior belief after the experiment is $\theta \sim \text{Beta}(8,18)$. But now consider what happened at half-time. After half the experiment, we had $k = 2$ and $N = 12$, so our beliefs followed $\theta \sim \text{Beta}(3, 11)$ at this moment in time. But using these beliefs as priors, and observing the rest of the data would consequently result in updating the prior $\theta \sim \text{Beta}(3, 11)$ with another set of observations $k = 5$ and $N = 12$, giving us the same posterior belief as what we would have gotten if we updated in one swoop. Figure \@ref(fig:ch-03-03-estimation-sequential-updates) shows the steps through the belief space, starting uninformed and observing one piece of data at a time (going right for each outcome of heads, down for each outcome of tails).

```{r ch-03-03-estimation-sequential-updates, echo = F, fig.cap = "Beta distributions for different parameters. Starting from an uninformative prior (top left), we arrive at the posterior distribution in the bottom left, in any sequence of sequentially updating with the data.", fig.height = 8}
expand_grid(
    theta = seq(0,1,length.out = 41),
    a = 1:7,
    b = 1:4
  ) %>% 
  as_tibble() %>% 
  mutate(
    probability = dbeta(theta, a, b),
    parameters = str_c("a=", a, " b=", b)
  ) %>% 
  ggplot(aes(x = theta, y = probability)) +
  geom_line() +
  facet_wrap(~parameters, scales = "free", nrow = 7) +
  theme(
    axis.title.x=element_blank(),
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank(),
    axis.title.y=element_blank(),
    axis.text.y=element_blank(),
    axis.ticks.y=element_blank()
  )
```


This sequential updating is not a peculiarity of the Beta-Binomial case or of conjugacy. It holds in general for Bayesian inference. Sequential updating is a very intuitive property, but it is not shared by all other forms of inference from data. That Bayesian inference is sequential and commutative follows from the commutativity of multiplication of likelihoods (and the definition of Bayes rule).

<div class = "mathstuff">

```{theorem, "Sequential-update"}
Bayesian posterior inference is sequential and commutative in the sense that for a data set $D$ which is comprised of two mutually exclusive subsets $D_1$ and $D_2$ such that $D_1 \cup D_2 = D$, we have:

$$ P(\theta \mid D ) \propto P(\theta \mid D_1) \ P(D_2 \mid \theta) $$  

```

<div class="collapsibleProof">
<button class="trigger">Show proof.</button>
<div class="content">

```{proof}
$$
\begin{aligned}
P(\theta \mid D) & = \frac{P(\theta) \ P(D \mid \theta)}{ \int P(\theta') \ P(D \mid \theta') \text{d}\theta'} \\
& = \frac{P(\theta) \ P(D_1 \mid \theta) \ P(D_2 \mid \theta)}{ \int P(\theta') \ P(D_1 \mid \theta') \ P(D_2 \mid \theta') \text{d}\theta'} & \text{[from multiplicativity of likelihood]} \\
& = \frac{P(\theta) \ P(D_1 \mid \theta) \ P(D_2 \mid \theta)}{ \frac{k}{k} \int P(\theta') \ P(D_1 \mid \theta') \ P(D_2 \mid \theta') \text{d}\theta'} & \text{[for random positive k]} \\
& = \frac{\frac{P(\theta) \ P(D_1 \mid \theta)}{k} \ P(D_2 \mid \theta)}{\int \frac{P(\theta') \ P(D_1 \mid \theta')}{k} \ P(D_2 \mid \theta') \text{d}\theta'} & \text{[rules of integration; basic calculus]} \\
& = \frac{P(\theta \mid D_1) \ P(D_2 \mid \theta)}{\int P(\theta' \mid D_1) \ P(D_2 \mid \theta') \text{d}\theta'} & \text{[Bayes rule with } k = \int P(\theta) P(D_1 \mid \theta) \text{d}\theta ]\\
\end{aligned}
$$
```

&nbsp;

</div>
</div>
</div>


### Posterior predictive distribution

We already learned about the _prior predictive distribution_ of a model in Chapter \@ref(Chap-03-03-models-parameters-prior-predictive). Remember that the prior predictive distribution of a model $M$ captures how likely hypothetical data observations are from an _a priori_ point of view.
It was defined like this:

$$ 
\begin{aligned}
P_M(D_{\text{pred}})  & = \sum_{\theta} P_M(D_{\text{pred}} \mid \theta) \ P_M(\theta)  && \text{[discrete parameter space]} \\
P_M(D_{\text{pred}})  & = \int P_M(D_{\text{pred}} \mid \theta) \ P_M(\theta) \ \text{d}\theta  && \text{[continuous parameter space]}
\end{aligned}
$$

After updating beliefs about parameter values in the light of observed data $D_{\text{obs}}$, we can similarly define the **posterior predictive distribution**, which is analogous to the prior predictive distribution, except that it relies on the posterior over parameter values $P_{M(\theta \mid D_{\text{obs}})}$ instead of the prior $P_M(\theta)$:

$$ 
\begin{aligned}
P_M(D_{\text{pred}} \mid D_{\text{obs}})  & = \sum_{\theta} P_M(D_{\text{pred}} \mid \theta) \ P_M(\theta \mid D_{\text{obs}})  && \text{[discrete parameter space]} \\
P_M(D_{\text{pred}} \mid D_{\text{obs}})  & = \int P_M(D_{\text{pred}} \mid \theta) \ P_M(\theta \mid D_{\text{obs}}) \ \text{d}\theta  && \text{[continuous parameter space]}
\end{aligned}
$$
 

## Point-valued and interval-ranged estimates {#ch-03-04-parameter-estimation-points-intervals}

Let's consider the "24/7" example with a flat prior again, concisely repeated in Figure \@ref(fig:ch-03-03-estimation-24-7-overview).

```{r ch-03-03-estimation-24-7-overview, echo = F, fig.cap = "Prior (uninformative), likelihood and posterior for the 24/7 example.", fig.height = 8}

prior_plot_24_7 <- 
  tibble(
    theta = seq(0.01,1, by = 0.01),
    prior = dbeta(seq(0.01,1, by = 0.01), 1, 1 )
  ) %>% 
  ggplot(aes(x = theta, y = prior)) + 
  xlim(0,1) + 
  geom_line(color = project_colors[1], size = 2) + 
  labs(
    x = latex2exp::TeX("Bias $\\theta$"),
    y = latex2exp::TeX("Prior probability $P_{M}(\\theta)$"),
    title = "Prior"
  )

lh_plot_24_7 <- 
  tibble(
    theta = seq(0.01,1, by = 0.01),
    lh = dbinom(x = 7, size = 24, prob = theta)
  ) %>% 
  ggplot(aes(x = theta, y = lh)) + 
  xlim(0,1) + 
  geom_line(color = project_colors[2], size = 2) + 
  labs(
    x = latex2exp::TeX("Bias $\\theta$"),
    y = latex2exp::TeX("Likelihood $P_{M}(D \\, | \\, \\theta)$"),
    title = "Likelihood"
  )

hdi = HDInterval::hdi(qbeta , shape1 = 8 , shape2 = 18 )
hdiData <- tibble(
  theta = rep(hdi, each = 2),
  post = c(0,dbeta(hdi, 8, 18), 0)
)
expData <- tibble(
  theta = c(8/26,8/26),
  post = c(0,dbeta(8/26, 8, 18 ))
)

posterior_plot_24_7 <- 
  tibble(
    theta = seq(0.01,1, by = 0.01),
    posterior = dbeta(seq(0.01,1, by = 0.01), 8, 18 )
  ) %>% 
  ggplot(aes(x = theta, y = posterior)) + 
  xlim(0,1) + 
  labs(
    x = latex2exp::TeX("Bias $\\theta$"),
    y = latex2exp::TeX("Posterior probability $P_{M}(\\theta \\, | \\, D)$"),
    title = "Posterior"
  ) +
  geom_line(data = hdiData, aes(x = theta, y = post), color = "firebrick", size = 1.5) +
  geom_label(x = 0.7, y = 0.5, label = "Cred.Int.: 0.14 - 0.48", color = "firebrick", size = 5) +
  geom_line(data = expData, aes(x = theta, y = post), color = "darkblue", size = 1.5) +
  geom_label(x = 0.52, y = dbeta(8/26, 8, 18 ), label = "expectation: 0.308", color = "darkblue", size = 5) +
  geom_line(color = "black", size = 2)

cowplot::plot_grid(
  prior_plot_24_7,
  lh_plot_24_7,
  posterior_plot_24_7,
  ncol = 1
)
```

The posterior probability distribution in Figure \@ref(fig:ch-03-03-estimation-24-7-overview) contains rich information. It specifies how likely each value of $\theta$ is, obtained by updating the original prior beliefs with the observed data. Such rich information is difficult to process and communicate in natural language. It is therefore convenient to have conventional means of summarizing the rich information carried in a probability distribution like in Figure \@ref(fig:ch-03-03-estimation-24-7-overview). Customarily, we summarize in terms of a point-estimate and/or an interval estimate. The *point estimate* gives information about a "best value", i.e., a salient point, and the *interval estimate* gives, usually, an indication of how closely other "good values" are scattered around the "best value".
The most frequently used Bayesian point estimate is the mean of the posterior distribution, and the most frequently used Bayesian interval estimate is the credible interval.
We will introduce both below, alongside some alternatives (namely the _maximum a posteriori_, the _maximum likelihood estimate_ and the inner-quantile range).

### Point-valued estimates

A common Bayesian point estimate of parameter vector $\theta$ is **the mean of the posterior distribution** over $\theta$. It gives the value of $\theta$ which we would expect to see when basing out expectations on the posterior distribution:
$$
\begin{aligned}
  \mathbb{E}_{P(\theta \mid D)} = \int \theta \ P(\theta \mid D) \ \text{d}\theta
\end{aligned}
$$
Taking the Binomial Model as example, if we start with flat beliefs, the expected value of $\theta$ after $k$ successes in $N$ flips can be calculated rather easily as $\frac{k+1}{n+2}$.^[This is also known as *Laplace's rule* or the *rule of succession*.] For our example case, we calculate the expected value of $\theta$ as $\frac{8}{26} \approx 0.308$ (see also Figure \@ref(fig:ch-03-03-estimation-24-7-overview)).

Another salient point-estimate to summarize a Bayesian posterior distribution is the **maximum _a posteriori_**, or MAP, for short. The MAP is the parameter value (tuple) that maximizes the posterior distribution:
  
$$ \text{MAP}(P(\theta \mid D)) =  \arg \max_\theta P(\theta \mid D) $$
  
While the mean of the posterior is "holistic" in the sense that it depends on the whole distribution, the MAP does not. The mean is therefore more faithful to the Bayesian ideal of taking the full posterior distribution into account. Moreover, depending on how Bayesian posteriors are computed/approximated, the estimation of a mean can be more reliable than that of a MAP.

The **maximum likelihood estimate (MLE)** is a point estimate based on the likelihood function alone.
It specifies the value of $\theta$ for which the observed data is most likely. We often use the notation $\hat{\theta}$ to denote the MLE of $\theta$:

$$
\begin{aligned}
  \hat{\theta} = \arg \max_{\theta} P(D \mid \theta)
\end{aligned}
$$

By ignoring the prior information entirely, the MLE is not a Bayesian notion, but a frequentist one (more on this in later chapters).
For the binomial likelihood function, the maximum likelihood estimate is easy to calculate as $\frac{k}{N}$, yielding
$\frac{7}{24} \approx 0.292$ for the running example. Figure \@ref(fig:ch-03-03-estimation-MLE) shows a graph of the non-normalized likelihood function and indicates the maximum likelihood estimate (the value that maximizes the curve).

```{r ch-03-03-estimation-MLE, echo = F, fig.cap = "Non-normalized likelihood function for the observation of $k=7$ successes in $N=24$ flips, including maximum likelihood estimate."}
plotdata_24_7_mle <- tibble(
  theta = seq(0.01,1, by = 0.01),
  lh = dbinom(x = 7, size = 24, prob = theta)
  )
expData_27_7_mle <- tibble(
  theta = c(7/24,7/24),
  lh = c(0,dbinom(x=7,size=24,prob=7/24))
)
ggplot(plotdata_24_7_mle, aes(x = theta, y = lh)) + xlim(0,1) +
  geom_label(x = 0.52, y = dbinom(x=7,size=24,prob=7/24), label = "MLE: 0.292", color = project_colors[1], size = 5) +
  geom_line(data = expData_27_7_mle, aes(x = theta, y = lh), color = project_colors[1], size = 1.25) +
  geom_line(color = "black", size = 2) + ylab("likelihood")
```

<div class = "exercises">
**Exercise 9.3**

Can you think of a situation where MLE and MAP are the same? HINT: Think which prior eliminates the difference between them!

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

MLE is a special case of MAP with a uniform prior.

</div>
</div>
</div>

### Interval-ranged estimates

A common Bayesian interval estimate of the coin bias parameter $\theta$ is a **credible interval**.^[Also frequently called "highest-density intervals", even when we are dealing not with density but probability mass.] An interval $[l;u]$ is a $\gamma\%$ credible interval for a random variable $X$ if two conditions hold, namely
$$
\begin{aligned}
  P(l \le X \le u) = \frac{\gamma}{100}
\end{aligned}
$$
and, secondly, for every $x \in[l;u]$ and $x' \not \in[l;u]$ we have $P(X=x) > P(X = x')$. Intuitively, a $95\%$ credible interval gives the range of values in which we believe with
relatively high certainty that the true value resides. Figure \@ref(fig:ch-03-03-estimation-24-7-overview) indicates the $95\%$ credible interval, based on the posterior distribution $P(\theta \mid D)$ of $\theta$, for the 24/7 example.^[Not all random variables have a credible interval for a given $\gamma$, according to this definition. A bimodal distribution might not, for example. A bi-modal distribution has two regions of high probability. We can, therefore, generalize the concept to a finite set of disjoint convex *credible regions*, all of which have the second property of the definition above and all of which conjointly are realized with $\gamma\%$ probability. Unfortunately, common parlor uses the term "credible interval" to refer to credible regions as well. The same disaster occurs with alternative terms, such as "$\gamma\%$ highest-density intervals", which also often refers to what should better be called "highest-density regions".]

Instead of credible intervals, sometimes posteriors are also summarized in terms of the $\gamma\%$ inner-quantile region. This is the interval $[l;u]$ such that 

$$P(l \le X) = P(X \le u) =  0.5 \cdot (1 - \frac{\gamma}{100})$$

For example, a 95% inner-quantile region contains all values except the smallest and largest values what each comprise 2.5% of the probability mass/density.
The inner-quantile range is easier to compute and does not have trouble with multi-modality.
This is why it is frequently used to approximate Bayesian credible intervals. 
However, care must be taken because the inner-quantile range is not as intuitive a measure of the "best values" as credible intervals.
While credible intervals and inner quantile regions coincide for distributions with a symmetric distribution around a single maximum, and so tend to coincide for large sample size when posteriors tend to converge to normal distributions, there are cases of clear divergence. 
Figure \@ref(fig:ch-03-03-estimation-credible-interval-vs-interquantilerange) shows such a case.
While the inner-quantile region does not include the most likely values, the credible interval does.

```{r ch-03-03-estimation-credible-interval-vs-interquantilerange, echo = F, fig.cap = "Difference between a 95% credible interval and a 95% inner-quantile region."}

CI_of_beta <- c(0, qbeta(p = 0.6, 1, 3))
IQR_of_beta <- qbeta(p = c(0.3, 0.7), 1, 3)

tibble(
  x = seq(from = 0,to = 1, length.out = 10001),
  density = dbeta(x, 1, 3)
) %>%
ggplot(aes(x = x, y = density)) +
  geom_line() +
  geom_segment(aes(y = 0.2, yend = 0.2, x = IQR_of_beta[1], xend = IQR_of_beta[2]), color = project_colors[1], size = 2) +
  geom_segment(aes(y = 0.8, yend = 0.8, x = CI_of_beta[1], xend = CI_of_beta[2]), color = project_colors[2], size = 2) +
  annotate("text", x = mean(CI_of_beta), y = 0.91, label = "60% Credible interval", color = project_colors[2], size = 5) +
  annotate("text", x = mean(IQR_of_beta), y = 0.31, label = "60% inner-quant. region", color = project_colors[1], size = 5)


```

### Computing Bayesian estimates

As mentioned, the most common (and arguably best) summaries to report for a Bayesian posterior are the posterior mean and a credible interval. The `aida` package which accompanies this book has a convenience function called `aida::summarize_sample_vector()` that gives the mean and 95% credible interval for a vector of samples. 
You can use it like so:

```{r }
# take samples from a posterior (24/7 example with flat priors)
posterior_samples <- rbeta(100000, 8, 18)
# get summaries
aida::summarize_sample_vector(
  # vector of samples 
  samples = posterior_samples,
  # name of output column
  name = "theta"
)
```

### Excursion: Computing MLEs and MAPs in R

Computing the maximum or minimum of a function, such as an MLE or MAP estimate, is a common problem. R has a built-in function `optim` that is useful for finding the minimum of a function. (If a maximum is needed, just multiply by $-1$ and search the minimum with `optim`.)

We can use the `optim` function to retrieve an MLE for 24/7 data and the Binomial Model (with flat priors) using conjugacy like so:

```{r }
# perform optimization
MLE <- optim(
  # starting value for optimization
  par = 0.2,
  # funtion to minimize (= optimize)
  fn = function(par){
    -dbeta(par, 8, 18)
  },
  # method of optimization (for 1-d cases)
  method = "Brent",
  # lower and upper bound of possible parameter values
  lower = 0,
  upper = 1
)
# retrieve MLE
MLE$par
```

Indeed, the value obtained by computationally approximating the maximum likelihood estimate for this likelihood function coincides with the true value of $\frac{7}{24}$.

## Approximating the posterior {#Ch-03-03-estimation-algorithms}

There are several methods of computing approximations of Bayesian posteriors. **Variational inference**, for example, hinges on the fact that under very general conditions, Bayesian posterior distributions are well approximated by (multi-variate) normal distributions. The more data, the better the approximation. We can then reduce the approximation of a Bayesian posterior to a problem of optimizing parameter values: we simply look for the parameter values that yield the "best" parametric approximation to the Bayesian posterior. (Here, "best" is usually expressed in terms of minimizing a measure of divergence between probability distributions, such as [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullbackâ€“Leibler_divergence).) Another prominent method of approximating Bayesian posteriors is [rejection sampling](https://en.wikipedia.org/wiki/Rejection_sampling).

The most prominent class of methods to approximate Bayesian posteriors are Markov Chain Monte Carlo methods. We will describe the most basic version of such MCMC algorithms below. For most applications in the context of this introductory book, it suffices to accept that there are black boxes (with some knobs for fine-tuning) that, if you supply a model description, priors and data, will return samples from the posterior distribution.

### Of apples and trees: Markov Chain Monte Carlo sampling {#ch-03-03-MCMC}

<div style = "float:right; width:20%;">
<img src="visuals/apples.png" alt="apples">  
</div>  

Beginning of each summer, Nature sends out the Children to distribute the apples among the trees. It is custom that bigger trees ought to receive more apples. Indeed, every tree is supposed to receive apples in proportion to how many leaves it has. If Giant George (an apple tree!) has twice as many leaves as Thin Finn (another apple tree!), Giant George is to receive twice as many apples as Thin Finn. This means that if there are $n_a$ apples to distribute in total, and $L(t)$ is the number of leaves of tree $t$, every tree should receive $A(t)$ apples, where:

$$ A(t) = \frac{L(t)}{\sum_{t'} L(t')} \ n_a $$

The trouble is that Nature does not know the number of leaves of all the trees: Nature does not care about numbers. The Children, however, can count. But they cannot keep in mind the number of leaves for many trees for a long time. And no single Child could ever visit all the trees before the winter. This is why the Children distribute apples in a way that approximates Nature's will. The more apples to distribute, the better the approximation. Nature is generally fine with approximate but practical solutions.

When a Child visits a tree, it affectionately hangs an apple into its branches. It also writes down the name of the tree in a list next to the number of the apple it has just delivered. It then looks around and selects a random tree in the neighborhood. If the current tree $t_c$, where the Child is at present, has fewer leaves than this other tree $t_o$, i.e., if $L(t_c) < L(t_o)$, the Child visits $t_o$. If instead $L(t_c) \ge L(t_o)$, the Child flips a coin and visits $t_o$ with a probability proportional to $\frac{L(t_o)}{L(t_c)}$. In other words, the Child will always visit a tree with more leaves, and it will visit a tree with fewer leaves depending on the proportion of leaves. 

When a large number of apples are distributed, and Nature looks at the list of trees each Child has visited. This list of tree names is a set of **representative samples** from the probability distribution:

$$P(t) \propto L(t)$$

These samples were obtained without the knowledge of the normalizing constant. The Children only had $L(t)$ at their disposal. When trees are parameter tuples $\theta$ and the number of leaves is the product $P(D \mid \theta) \ P(\theta)$, the Children would deliver samples from the posterior distribution *without* knowledge of the normalizing constant (a.k.a. the integral of doom).

The sequence of trees visited by a single Child is a **sample chain**. Usually, Nature sends out at least 2-4 Children. The first tree a Child visits is the **initialization of the chain**. Sometimes Nature selects initial trees strategically for each Child. Sometimes Nature lets randomness rule. In any case, a Child might be quite far away from the meadow with lush apple trees, the so-called **critical region** (where to dwell makes the most sense). It might take many tree hops before a Child reaches this meadow. Nature, therefore, allows each Child to hop from tree to tree for a certain time, the **warm-up period**, before the Children start distributing apples and taking notes. If each Child only records every $k$-th tree it visits, Nature calls $k$ a **thinning factor**. Thinning generally reduces **autocorrelation** (think: the amount to which subsequent samples do not carry independent information about the distribution). Since every next hop depends on the current tree (and only on the current tree), the whole process is a **Markov process**. It is light on memory and parallelizable but also affected by autocorrelation. Since we are using samples, a so-called **Monte Carlo method**, the whole affair is a **Markov Chain Monte Carlo** algorithm. It is one of many. It's called **Metropolis-Hastings**. More complex MCMC algorithms exist. One class of such MCMC algorithms is called **Hamiltonian Monte Carlo**, and these approaches use gradients to optimize the **proposal function**, i.e., the choice of the next tree to consider going to. They use the warm-up period to initialize certain tuning parameters, making them much faster and more reliable (at least if the distribution of leaves among neighboring trees is well-behaved). 

How could Nature be sure that the plan succeeded? If not even Nature knows the distribution $P(t)$, how can we be sure that the Children's list gives representative samples to work with? Certainty is petty. The reduction of uncertainty is key! Since we send out several Children in parallel, and since each Child distributed many apples, we can compare the list of trees delivered by each Child (= the set of samples in each chain). For that purpose, we can use statistics and ask: is it plausible that the set of samples in each chain has been generated from the same probability distribution? - The answer to this question can help reduce uncertainty about the quality of the sampling process.

<style>
#target_box {
  width: 250px;
  height: 80px;
  border: 1px solid #aaaaaa;
  overflow: auto;
}
#start_box {
  width: 300px;
  height: auto;
  padding: 10px;
  margin: 2em 5em 0 0;
  float: right;
}
.drag {
  height: auto;
  padding: 10px;
  margin: 0 0 0 0;
}
</style>

<div class = "exercises">
**Exercise 9.6**

On the right, there is a shuffled list of the steps that occur in the MH algorithm. Bring the list in the right order by dragging each step to the corresponding box on the left.

<div id="start_box" ondrop="drop(event)" ondragover="allowDrop(event)">
  <p id="drag1" class="drag" draggable="true" ondragstart="drag(event)">If the new proposal has a higher posterior value than the most recent
  sample, then accept the new proposal.</p>
  <p id="drag2" class="drag" draggable="true" ondragstart="drag(event)">Generate a new value (proposal).</p>   
  <p id="drag3" class="drag" draggable="true" ondragstart="drag(event)">Set an initial value.</p>
  <p id="drag4" class="drag" draggable="true" ondragstart="drag(event)">Compare the posterior value of the new proposal and the height of the
  posterior at the previous step.</p>
  <p id="drag5" class="drag" draggable="true" ondragstart="drag(event)">Choose to accept or reject the new proposal concerning the computed
  proportion.</p>  
  <p id="drag6" class="drag" draggable="true" ondragstart="drag(event)">If the new proposal has a lower posterior value than the most recent
  sample, compute the proportion of the posterior value of the new proposal and the height of the posterior at the previous step.</p>
</div>

<p><b>Step 1:</b></p>
<div id="target_box" ondrop="drop(event)" ondragover="allowDrop(event)"></div>

<p><b>Step 2:</b></p>
<div id="target_box" ondrop="drop(event)" ondragover="allowDrop(event)"></div>

<p><b>Step 3:</b></p>
<div id="target_box" ondrop="drop(event)" ondragover="allowDrop(event)"></div>

<p><b>Step 4:</b></p>
<div id="target_box" ondrop="drop(event)" ondragover="allowDrop(event)"></div>

<p><b>Step 5:</b></p>
<div id="target_box" ondrop="drop(event)" ondragover="allowDrop(event)"></div>

<p><b>Step 6:</b></p>
<div id="target_box" ondrop="drop(event)" ondragover="allowDrop(event)"></div>

<br>

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

**Step 1:** Set an initial value.

**Step 2:** Generate a new value (proposal).

**Step 3:** Compare the posterior value of the new proposal and the height of the posterior at the previous step.

**Step 4:** If the new proposal has a higher posterior value than the most recent sample, then accept the new proposal.

**Step 5:** If the new proposal has a lower posterior value than the most recent sample, compute the proportion of the posterior value of the new proposal and the height of the posterior at the previous step.

**Step 6:** Choose to accept or reject the new proposal concerning the computed proportion.

</div >
</div >
</div >

<script>
function allowDrop(ev) {
  ev.preventDefault();
}

function drag(ev) {
  ev.dataTransfer.setData("text", ev.target.id);
}

function drop(ev) {
  ev.preventDefault();
  var data = ev.dataTransfer.getData("text");
  ev.target.appendChild(document.getElementById(data));
}
</script>

### Excursion: Probabilistic modeling with Stan {#ch-03-03-estimation-Stan}

There are a number of software solutions for Bayesian posterior approximation, all of which implement a form of MCMC sampling, and most of which also realize at least one other form of parameter estimation. Many of these use a special language to define the model and rely on a different programming language (like R, Python, Julia, etc.) to communicate with the program that does the sampling. Some options are:

- [WinBUGS](https://www.mrc-bsu.cam.ac.uk/software/bugs/the-bugs-project-winbugs/): a classic which has grown out of use a bit
- [JAGS](http://mcmc-jags.sourceforge.net): another classic
- [Stan](https://mc-stan.org): strongly developed current workhorse
- [WebPPL](http://webppl.org): light-weight, browser-based full probabilistic programming language
- [pyro](http://pyro.ai): for probabilistic (deep) machine learning, based on PyTorch
- [greta](https://greta-stats.org): R-only probabilistic modeling package, based on Python and TensorFlow

This section will showcase an example using Stan.
Later parts of this book will focus on regression models, for which we will use an R package called `brms`.
This package uses Stan in the background.
We do not have to write or read Stan code to work with `brms`.
Still, a short peek at how Stan works is interesting if only to get a rough feeling for what is happening under the hood.

#### Basics of Stan

In order to approximate a posterior distribution over parameters for a model, given some data, using an MCMC algorithm, we need to specify the model for the sampler. In particular, we must tell it about (i) the parameters, (ii) their priors, and (iii) the likelihood function. The latter requires that the sampler knows about the data. To communicate with Stan we will use the R package `rstan` (there are similar packages also for Python, Julia and other languages). More information about Stan can be found in [the documentation section of the Stan homepage](https://mc-stan.org/users/documentation/).

The usual workflow with Stan and `rstan` consists of the following steps. First, we use R to massage the data into the right format for passing to Stan (a named list, see below). Second, we write the model in the Stan programming language. We do this in a stand-alone file.^[RStudio provides syntax highlighting for Stan code. Use the file ending `*.stan`.] Then, we run the Stan code with the R command `rstan::stan` supplied by the package `rstan`. Finally, we collect the output of this operation (basically: a set of samples from the posterior distribution) and do with it as we please (plotting, further analysis, diagnosing the quality of the samples, ...).

This is best conveyed by a simple example.

#### Binomial Model

Figure \@ref(fig:ch-03-03-Binomial-Model-repeated) shows the Binomial model for coin flips, as discussed before. We are going to implement it in Stan.

```{r ch-03-03-Binomial-Model-repeated, echo = F, fig.cap="The Binomial Model (repeated from before).", out.width = '40%'}
knitr::include_graphics("visuals/binomial-model.png")
```

We use the data from the [King of France example](#app-93-data-sets-king-of-france), where we are interested in the number $k = 109$ of "true" responses to sentences with a false presupposition over all $N = 311$ relevant observations. 

We collect this information in a named list, which we will pass to Stan.

```{r}
KoF_data_4_Stan <- list(
  k = 109,
  N = 311
)
```

Next, we need to write the actual model. Notice that Stan code is strictly regimented to be divided into different blocks, so that Stan knows what is data, what are parameters and what constitutes the actual model (prior and likelihood). Stan also wants to know the type of its variables (and the ranges of values these can take on).


```{mystan, eval = F}
data {
  int<lower=0> N ;
  int<lower=0,upper=N> k ;
}
parameters {
  real<lower=0,upper=1> theta ;
} 
model {
  # prior 
  theta ~ beta(1,1) ;
  # likelihood
  k ~ binomial(N, theta) ;
}
```

<link rel="stylesheet" href="hljs.css">
<script src="stan.js"></script>
<script>$('pre.mystan code').each(function(i, block) {hljs.highlightBlock(block);});</script>

We save this Stan code in a file `binomial_model.stan` (which you can download [here](https://raw.githubusercontent.com/michael-franke/intro-data-analysis/master/models_stan/binomial_model.stan)) in a folder `models_stan` and then use the function `rstan::stan` to run the Stan code from within R.

```{r}
stan_fit_binomial <- rstan::stan(
  # where is the Stan code
  file = 'models_stan/binomial_model.stan',
  # data to supply to the Stan program
  data = KoF_data_4_Stan,
  # how many iterations of MCMC
  iter = 3000,
  # how many warmup steps
  warmup = 500
)
```

The object returned from this call to Stan is a special model fit object. If we just print it, we get interesting information about the estimated parameters:

```{r}
print(stan_fit_binomial)
```
<!-- TODO: explain this  -->

To get the posterior samples in a tidy format we use a function from the `tidybayes` package:

```{r}
tidy_samples <- tidybayes::tidy_draws(stan_fit_binomial) %>% select(theta)
tidy_samples
```

We can then `pull` out the column `theta` as a vector and feed it into the summary function from the `aida` package to get our key Bayesian estimates:

```{r}
Bayes_estimates <- tidy_samples %>%
  pull(theta) %>%
  aida::summarize_sample_vector("theta")
Bayes_estimates
```

Figure \@ref(fig:ch-03-03-binomial-posterior) moreover shows a density plot derived from the MCMC samples, together with the estimated 95% HDI and the true posterior distribution (in back), as derived by conjugacy.

```{r ch-03-03-binomial-posterior, echo = F, fig.cap = "Posterior over bias $\\theta$ given $k=109$ and $N=311$ approximated by samples from Stan, with estimated 95% credible interval (red area). The black curve shows the true posterior, derived through conjugacy."}
# get density estimates from samples
dens <- tidybayes::tidy_draws(stan_fit_binomial) %>% pull(theta) %>% density()
# plot estimated density with true posterior
tibble(
  parameter = dens$x,
  density = dens$y
) %>%
  ggplot(aes(x = parameter, y = density)) +
  geom_line(color = "firebrick") +
  geom_area(aes(x = ifelse(parameter > Bayes_estimates[1,2] %>% as.numeric & parameter < Bayes_estimates[1,4] %>% as.numeric , parameter, 0)),
            fill = "firebrick", alpha = 0.5) +
  ylim(0, max(dens$y)) +
  xlim(min(dens$x), max(dens$x)) +
  geom_line(
    data = tibble(
      parameter = seq(0,1, length.out = 401),
      density = dbeta(parameter, 109, 311-109)
    ),
    color = "black"
  ) +
  ylim(c(0,15.6)) +
  labs(
    x = latex2exp::TeX("Parameter $\\theta$")
  )
```



<!-- The resulting `draws` object is a special kind of object, an `MCMC.list` as defined in the `coda` package. This is not very important for us, however, we simply transform the samples into a tidy representation, using the function `ggs` from the `ggmcmc` package: -->

<!-- ```{r} -->
<!-- # cast results (type 'mcmc.list') into tidy tibble -->
<!-- tidy_draws = ggmcmc::ggs(draws) -->
<!-- tidy_draws -->
<!-- ``` -->

<!-- We can use these samples to compute Bayesian point- and interval estimates, for example: -->

<!-- ```{r, eval = T} -->
<!-- # obtain Bayesian point and interval estimates -->
<!-- Bayes_estimates <- tidy_draws %>%  -->
<!--   group_by(Parameter) %>% -->
<!--   summarise( -->
<!--     '|95%' = HDInterval::hdi(value)[1], -->
<!--     mean = mean(value), -->
<!--     '95|%' = HDInterval::hdi(value)[2] -->
<!--   ) -->
<!-- Bayes_estimates -->
<!-- ``` -->

<!-- Using the (controversial) method of inspecting posterior estimates, we would conclude that $\theta = 0$ is not an *a posteriori* credible value for the inclination to judge the truth of sentences with a false presupposition.  -->

<!-- Figure \@ref(fig:ch-03-03-binomial-posterior) moreover shows a density plot derived from the MCMC samples, together with the estimated 95% HDI and the true posterior distribution (in back), as derived by conjugacy. -->

<!-- ```{r ch-03-03-binomial-posterior, echo = F, fig.cap = "Posterior over coin bias $\\theta$ given $k=109$ and $N=311$ approximated by samples from `greta`, with estimated 95% credible interval (red area). The black curve shows the true posterior, derived through conjugacy."} -->
<!-- # get density estimates from samples -->
<!-- dens <- filter(tidy_draws, Parameter == "theta") %>% pull(value) %>%  -->
<!--   density() -->
<!-- # plot estimated density with true posterior -->
<!-- tibble( -->
<!--   parameter = dens$x, -->
<!--   density = dens$y -->
<!-- ) %>%  -->
<!--   ggplot(aes(x = parameter, y = density)) + -->
<!--   geom_line(color = "firebrick") + -->
<!--   geom_area(aes(x = ifelse(parameter > Bayes_estimates[1,2] %>% as.numeric & parameter < Bayes_estimates[1,4] %>% as.numeric , parameter, 0)), -->
<!--             fill = "firebrick", alpha = 0.5) + -->
<!--   ylim(0, max(dens$y)) + -->
<!--   xlim(min(dens$x), max(dens$x)) + -->
<!--   geom_line( -->
<!--     data = tibble( -->
<!--       parameter = seq(0,1, length.out = 401), -->
<!--       density = dbeta(parameter, 109, 311-109) -->
<!--     ), -->
<!--     color = "black" -->
<!--   ) +  -->
<!--   ylim(c(0,15.6)) +  -->
<!--   labs( -->
<!--     x = latex2exp::TeX("Parameter $\\theta$") -->
<!--   ) -->
<!-- ``` -->

<!-- ### T-Test Model for Mental Chronometry -->

<!-- <div style = "float:right; width:15%;"> -->
<!-- <img src="visuals/badge-mental-chronometry.png" alt="badge-mental-chronometry">   -->
<!-- </div>   -->

<!-- We will use the [Mental Chronometry](#app-93-data-sets-mental-chronometry) data to compare the reaction times in the "go/No-go" condition to the reaction times in the "discrimination" condition. To do this, we implement a T-Test model by hand in `greta`. -->

<!-- First, we read in the data and get some handy summary statistics: -->


<!-- ```{r} -->
<!-- mc_data_cleaned <- aida::data_MC_cleaned -->
<!-- means_and_diffs <- mc_data_cleaned %>% -->
<!--   filter(block != "reaction") %>% -->
<!--   group_by(block) %>% -->
<!--   summarise( -->
<!--     mean_RT = mean(RT) -->
<!--   ) %>% -->
<!--   pivot_wider( -->
<!--     names_from = block, -->
<!--     values_from = mean_RT -->
<!--   ) %>% -->
<!--   mutate( -->
<!--     `discr - gng` = discrimination - goNoGo -->
<!--   ) -->
<!-- means_and_diffs -->
<!-- ``` -->

<!-- The model we will use for this situation is the T-Test model shown in Figure \@ref(fig:ch-03-03-T-Test-Model-Difference), repeated from the previous Chapter. We use the model which explicitly codes the difference between means (the variable $\delta$) to directly address the question of whether $\delta = 0$ is a plausible point-value for this parameter. -->

<!-- ```{r ch-03-03-T-Test-Model-Difference, echo = F, out.width = '70%', fig.cap="A T-Test Model where one group is the default and the difference between group means is explicitly coded as a parameter."} -->
<!-- knitr::include_graphics("visuals/t-test-model-difference.png") -->
<!-- ``` -->

<!-- We extract the relevant data and declare it as a `greta` object: -->

<!-- ```{r, eval = F} -->
<!-- # isolate data vectors -->
<!-- RT_goNoGo <- mc_data_cleaned %>% filter(block == "goNoGo") %>% pull(RT) -->
<!-- RT_discrm <- mc_data_cleaned %>% filter(block == "discrimination") %>% pull(RT) -->
<!-- # declare as greta data arrays -->
<!-- y0 <- as_data(RT_goNoGo) -->
<!-- y1 <- as_data(RT_discrm) -->
<!-- ``` -->


<!-- We then define the model, using weakly informative, partly regularizing priors, i.e., priors that are informed by the data to ensure swift convergence (expecting values of `mean_0` to lie in the plausible region), but that are not very biased to allow a large impact of the likelihood function (using relatively large standard deviations). -->


<!-- ```{r, eval = F} -->
<!-- # priors  -->
<!-- mean_0   <- normal(430, 50) -->
<!-- delta    <- normal(0, 100) -->
<!-- sigma    <- normal(100, 10, truncation = c(0, Inf)) -->
<!-- # derived prameters -->
<!-- mean_1   <- mean_0 + delta -->
<!-- # likelihood -->
<!-- distribution(y0) <- normal(mean_0, sigma) -->
<!-- distribution(y1) <- normal(mean_1, sigma) -->
<!-- # model  -->
<!-- m <- model(mean_0, mean_1, delta, sigma)## --- sampling --- -->
<!-- draws <- greta::mcmc(m, warmup = 4000, n_samples = 6000, thin = 2) -->
<!-- ``` -->

<!-- ```{r, echo = F} -->
<!-- # saveRDS(draws, '../models_greta/ttest_draws.rds') -->
<!-- draws <- readRDS('models_greta/ttest_draws.rds') -->
<!-- ``` -->

<!-- Bayesian point- and interval-estimates can be calculated from the posterior samples, including  -->

<!-- ```{r, echo = T} -->
<!-- tidy_draws = ggmcmc::ggs(draws) -->
<!-- Bayes_estimates <- tidy_draws %>%  -->
<!--   group_by(Parameter) %>% -->
<!--   summarise( -->
<!--     '|95%' = HDInterval::hdi(value)[1], -->
<!--     mean = mean(value), -->
<!--     '95|%' = HDInterval::hdi(value)[2] -->
<!--   ) -->
<!-- Bayes_estimates -->
<!-- ``` -->

<!-- The Bayesian point-estimates for means and the difference correspond closely to the summary statistics we derived previously: -->

<!-- ```{r} -->
<!-- means_and_diffs -->
<!-- ``` -->

<!-- But we also now get indications of credible ranges of parameter values. Most interestingly, we obtain a 95% credible interval for the $\delta$ parameter, the difference between the means, which quite clearly does not include the case $\delta = 0$. The lower bound of the estimated 95% credible interval is more than 40 ms. We could conclude from this that, given this data set and the model used here, it is plausible that the difference in mean reaction times between the "discrimination" condition and the "go/no-go" condition is at least 40ms. -->

<!-- The plot below shows the density estimated from the posterior samples of $\delta$, together with the estimated 95% credible interval.  -->

<!-- ```{r} -->
<!-- dens <- filter(tidy_draws, Parameter == "delta") %>% pull(value) %>%  -->
<!--   density() -->

<!-- tibble( -->
<!--   delta = dens$x, -->
<!--   density = dens$y -->
<!-- ) %>%  -->
<!--   ggplot(aes(x = delta, y = density)) + -->
<!--   geom_line() + -->
<!--   geom_area(aes(x = ifelse( -->
<!--     delta > Bayes_estimates[1,2] %>% as.numeric &  -->
<!--       delta < Bayes_estimates[1,4] %>% as.numeric ,  -->
<!--     delta, 0)), -->
<!--     fill = "firebrick", alpha = 0.5) + -->
<!--   ylim(0, max(dens$y)) + -->
<!--   xlim(min(dens$x), max(dens$x)) -->
<!-- ``` -->

<!-- The actual posterior is multi-dimensional, and it always pays to inspect the full joint-posterior distribution so as not to miss any unexpected dependencies that might indicate sub-optimal inference or modeling. The `mcmc_pairs` function from the `bayesplot` package plots samples individually for each pair of parameters. Doing this we see the obvious (and perfectly fine) linear relation between estimates of `mean_0` and `delta`: the lower `mean_0` is estimated, the higher `delta` needs to be to yield a value of `mean_` that explains the data well. -->

<!-- ```{r} -->
<!-- bayesplot::mcmc_pairs(draws) -->
<!-- ``` -->

<!-- ## Addressing point-valued hypotheses with parameter estimation {#ch-03-03-estimation-testing} -->

<!-- Using interval-based estimates, we can address research questions formulated as **point-valued hypotheses** about a parameter of interest. A point-valued hypothesis could be, for example, that a particular coin is fair. This hypothesis can be expressed as the assumption that the coin bias $\theta_{c}$ in a Binomial Model is exactly $\theta_{c} = 0.5$. We then collect data $D$, compute the posterior $P_{\theta_{c} \mid D}$, and then check, roughly put, how credible the specific value of interest $\theta_{c} = 0.5$ is.  -->

<!-- More concretely, let $\Theta$ be the parameter space of a model $M$. We are interested in some component $\Theta_i$ and our hypothesis is $\Theta_i = \theta^*_i$ for some specific value $\theta^*_i$. A simple (but crude and controversial) way of addressing this point-valued hypothesis based on observed data $D$ is to look at whether $\theta^*_i$ lies inside some credible interval for parameter $\Theta_i$ in the posterior derived by updating with data $D$. A customary choice here are 95% credible intervals, but also other choices, e.g., 80% credible intervals, are used. -->

<!-- @kruschke2015 extends this approach to addressing point-valued hypotheses. He argues that we should *not* be concerned with point-valued hypotheses, but rather with intervals constructed around the point-value of interest. Kruschke, therefore, suggests to look at a **region of practical equivalence** (ROPE), usually defined by some $\epsilon$-region around $\theta^*_i$: -->

<!-- $$\text{ROPE}(\theta^*_i) = [\theta^*_i- \epsilon, \theta^*_i+ \epsilon]$$ -->

<!-- The choice of $\epsilon$ is context-dependent and requires an understanding of the scale at which parameter values $\Theta_i$ differ. If the parameter of interest is, for example, a difference $\delta$ in the means of reaction times, like in the Mental Chronometry example (to be introduced later), this parameter is intuitively interpretable. We can say, for instance, that an $\epsilon$-region of $\pm 5\text{ms}$ is really so short that any value in $[-5\text{ms}; 5\text{ms}]$ would be regarded as identical to $0$ for all practical purposes because of what we know about reaction times and their potential differences. However, with parameters that are less clearly anchored to a concrete physical measurement about which we have solid distributional knowledge and/or reliable intuitions, fixing the size of the ROPE can be more difficult. For the bias of a coin flip, for instance, which we want to test at the point value $\theta^* = 0.5$ (testing the coin for fairness), we might want to consider a ROPE like $[0.49; 0.51]$, although this choice may be less objectively defensible without previous experimental evidence from similar situations.  -->

<!-- In Kruschke's ROPE-based approach where $\epsilon > 0$, the decision about a point-valued hypothesis becomes ternary. If $[l;u]$ is an interval-based estimate of parameter $\Theta_i$ and $[\theta^*_i - \epsilon; \theta^*_i + \epsilon]$ is the ROPE around the point-value of interest, we would: -->

<!-- - **accept** the point-valued hypothesis iff $[l;u]$ is contained entirely in $[\theta^*_i - \epsilon; \theta^*_i + \epsilon]$; -->
<!-- - **reject** the point-valued hypothesis iff $[l;u]$ and $[\theta^*_i - \epsilon; \theta^*_i + \epsilon]$ have no overlap; and -->
<!-- - **withhold judgement** otherwise. -->

<!-- Consider the 24/7 example, where the point-valued hypothesis of interest is $\theta^* = 0.5$ (testing the coin for fairness) and the ROPE is $[0.49; 0.51]$ ($\epsilon = 0.1$, arbitrarily set here). The point- and interval-estimates for a Bayesian analysis (assuming flat priors in the Binomial Model) are as follows: -->

<!-- ```{r} -->
<!-- estimates_24_7 <- tibble( -->
<!--   `lower_Bayes` = HDInterval::hdi(function(x) qbeta(x, 8,18))[1], -->
<!--   `point_Bayes` = 8/25, -->
<!--   `upper_Bayes` = HDInterval::hdi(function(x) qbeta(x, 8,18))[2] -->
<!-- ) %>%  -->
<!--   pivot_longer( -->
<!--     everything(), -->
<!--     names_pattern = "(.*)_(.*)", -->
<!--     names_to = c(".value", "approach") -->
<!--   ) -->
<!-- estimates_24_7 -->
<!-- ``` -->

<!-- Figure \@ref(fig:ch-03-03-estimation-ROPE-24-7) shows these estimates next to the ROPE. We see that the Bayesian 95% credible interval has no overlap with the ROPE, so that we would *reject* the null-hypothesis of $\theta^* = 0.5$ by the ROPE+estimation logic of statistical decision making. -->

<!-- ```{r ch-03-03-estimation-ROPE-24-7, fig.cap = "Comparing point- and interval-estimates (Bayesian credible intervals and frequentist confidence intervals) against a ROPE of $[0.49; 0.51]$ (red shaded area) around the point-valued hypothesis of interest is $\\theta^* = 0.5$.", echo = F} -->
<!-- estimates_24_7 %>%  -->
<!--   ggplot(aes(x = approach, y = point, ymin = lower, ymax = upper, color = approach)) + -->
<!--   geom_point(size = 4, color = "black") +  -->
<!--   geom_rect( -->
<!--     aes(xmin = 0.75, xmax = 1.25, ymin = 0.49, ymax = 0.51),  -->
<!--     fill = "firebrick",  -->
<!--     color = "gray", alpha = 0.3 -->
<!--   ) + -->
<!--     geom_rect( -->
<!--     aes(xmin = 0.75, xmax = 1.25, ymin = 0.49999, ymax = 0.50001),  -->
<!--     fill = "firebrick",  -->
<!--     color = "firebrick", alpha = 1 -->
<!--   ) + -->
<!--   # geom_hline(aes(yintercept = 0.5), color = "firebrick") + -->
<!--   geom_linerange(size = 1.5) +  -->
<!--   geom_point(size = 4, color = "black") +  -->
<!--   coord_flip() +  -->
<!--   #ylim(c(0.1 ,0.55)) + -->
<!--   guides(color = "none") + -->
<!--   labs( -->
<!--     y = latex2exp::TeX("Bias parameter $\\theta$"), -->
<!--     x = "" -->
<!--   ) -->
<!-- ``` -->

## Estimating the parameters of a Normal distribution {#ch-03-04-parameter-estimation-normal}

To keep matters simple and the sample size low (so as to better see effects of different priors; more on this below), we look at a (boring) fictitious data set, which we imagine being measurements of height of two species of flowers, unflowerly named species 'A' and 'B'.

```{r}
# fictitious data from height measurements (25 flowers of two species each in cm)

heights_A <- c(6.94, 11.77, 8.97, 12.2, 8.48, 
               9.29, 13.03, 13.58, 7.63, 11.47, 
               10.24, 8.99, 8.29, 10.01, 9.47, 
               9.92, 6.83, 11.6, 10.29, 10.7, 
               11, 8.68, 11.71, 10.09, 9.7)

heights_B <- c(11.45, 11.89, 13.35, 11.56, 13.78, 
               12.12, 10.41, 11.99, 12.27, 13.43, 
               10.91, 9.13, 9.25, 9.94, 13.5, 
               11.26, 10.38, 13.78, 9.35, 11.67, 
               11.32, 11.98, 12.92, 12.03, 12.02)
```

On the assumption that the metric measurements for flower 'A' come from a normal distribution, the goal is to estimate credible values for that normal distribution's parameters $\mu_{A}$ and $\sigma_{A}$; and similarly for flower 'B'.
The "research question" of interest is whether it is credible that the mean of heights for flower 'A' is smaller than that of 'B' - or, in other words, whether the difference in means $\delta = \mu_{B} - \mu_{A}$ is credibly positive.

Here are relevant summary statistics for this case, and a plot, both of which seem to support the conjecture that flower 'A' is smaller, on average, than flower 'B'.

```{r}
# bring data into a more practical format
ffm_data <- tibble(
  A = heights_A,
  B = heights_B
) %>%
  pivot_longer(
    cols      = everything(),
    names_to  = 'species',
    values_to = 'height'
    )

# some summary statistics
ffm_data %>%
  group_by(species) %>%
  summarise(
    mean     = mean(height),
    std_dev  = sd(height)
  )
```

```{r}
ffm_data %>%
  ggplot(aes(x = height)) +
  geom_density(aes(color = species), size = 2) +
  geom_rug() +
  facet_grid(~species, scales = "free") +
  theme(legend.position = 'none')
```

The remainder of this chapter will introduce two models for inferring the parameters of a (single) normal distribution, both of which are set-up in such a way that it is possible to compute a closed-form solution for the posterior distributions over $\mu$ and $\sigma$:
(i) a model with uninformative priors, and
(ii) a model with conjugate priors.
These two models are also explained in the video below.

<iframe src="https://player.vimeo.com/video/412459401" width="640" display="block" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>

### Uninformative priors

The model with uninformative priors is shown in Figure \@ref(fig:ch-03-03-estimation-normal-uninformative-model).

```{r ch-03-03-estimation-normal-uninformative-model, echo = F, fig.align = "center", out.width = '60%', fig.cap="A model to infer the parameter of a normal distribution with non-informative priors."}
knitr::include_graphics("visuals/inferring-gaussian-uninformative-prior-model.png")
```

The posterior for variance $\sigma^{2}$ and mean $\mu$ for this model with uninformative priors is as follows:

$$
\begin{align*}
P(\mu, \sigma^2 \mid \mathbf{y})
& = {\color{7F2615}{P(\sigma^2 | \mathbf{y})}} \ \ \  {\color{353585}{P(\mu \mid \sigma^2, \mathbf{y})}} & \text{with:} \\
\sigma^2 \mid \mathbf{y} & \sim \mathrm{Inv}\text{-}\chi^2 \left(n-1,\ s^2 \right) \\
\mu \mid \sigma^2, \mathbf{y} & \sim \mathrm{Normal} \left (\bar{y} \mid \frac{\sigma}{\sqrt{n}} \right)
\end{align*}
$$

The `aida` package provides the convenience function `aida::get_samples_single_noninformative`, which we use below but also show explicitly first. It takes a vector `data_vector` (like `height_A`) of metric observations as input and returns `n_samples` samples from the posterior.

```{r eval=F}
get_samples_single_noninformative <- function(data_vector, n_samples = 1000) {
  # determine sample variance
  s_squared <- var(data_vector)
  # posterior samples of the variance
  var_samples <- extraDistr::rinvchisq(
    n = n_samples,
    nu = length(data_vector) - 1,
    tau = s_squared
  )
  # posterior samples of the mean given the sampled variance
  mu_samples <- map_dbl(
    var_samples,
    function(var) rnorm(
      n = 1,
      mean = mean(data_vector),
      sd = sqrt(var / length(data_vector))
    )
  )
  # return pairs of values
  tibble(
    mu    = mu_samples,
    sigma = sqrt(var_samples)
  )
}
```

If we apply this function for the data of flower 'A', we get samples of likely pairs consisting of means *and* standard deviations (each row is one pair of associated samples):

```{r}
aida::get_samples_single_noninformative(heights_A, n_samples = 5)
```

By taking more samples from this 2-dimensional (joint) posterior distribution a scatter point reveals its approximate shape.

```{r}
# take 10,000 samples from the posterior
post_samples_A_noninfo <- aida::get_samples_single_noninformative(data_vector = heights_A, n_samples = 10000)

# look at a scatter plot
post_samples_A_noninfo %>%
  ggplot(aes(x = sigma, y = mu)) +
  geom_point(alpha = 0.4, color = "lightblue")
```

The plot below shows the marginal distributions of each variable, $\mu$ and $\sigma$, separately:

```{r}
post_samples_A_noninfo %>%
  pivot_longer(cols = everything(), names_to = "parameter", values_to = "value") %>%
  ggplot(aes(x = value)) +
  geom_density() +
  facet_grid(~ parameter, scales = "free")
```

As usual, we can also produce the relevant Bayesian summary statistics for our samples, like so:

```{r}
rbind(
  aida::summarize_sample_vector(post_samples_A_noninfo$mu,    "mu"),
  aida::summarize_sample_vector(post_samples_A_noninfo$sigma, "sigma")
)
```

### Conjugate priors

The model with uninformative priors is useful when modelers have no or wish to not include any *a priori* assumptions about $\mu$ and $\sigma$.
When prior assumptions are relevant, we can use a slightly more complex model with conjugate priors.
The model is shown in Figure \@ref(fig:ch-03-03-estimation-normal-conjugate).

```{r ch-03-03-estimation-normal-conjugate, echo = F, fig.align = "center", out.width = '60%', fig.cap="Model with conjugate priors."}
knitr::include_graphics("visuals/inferring-gaussian-conjugate-prior-model.png")
```

With this prior structure, the posterior is of the form:

$$
\begin{align*}
P(\mu, \sigma^2 \mid \mathbf{y})
& = {\color{7F2615}{P(\sigma^2 | \mathbf{y})}} \ \ \  {\color{353585}{P(\mu \mid \sigma^2, \mathbf{y})}} & \text{with:} \\
\sigma^2 \mid \mathbf{y} & \sim {\color{7F2615}{\mathrm{Inv}\text{-}\chi^2 \left({\color{3F9786}{\nu_1}},\ {\color{3F9786}{\sigma^2_1}} \right)}} \\
\mu \mid \sigma^2, \mathbf{y} & \sim {\color{353585}{\mathrm{Normal} \left ({\color{3F9786}{\mu_1}}, \frac{\sigma}{\sqrt{{\color{3F9786}{\kappa_1}}}} \right)}} & \text{where:} \\
{\color{3F9786}{\nu_1}} & = \nu_0 + n \\
\nu_n{\color{3F9786}{\sigma_1^2}} & =  \nu_0 \sigma_0^2 + (n-1) s^2 + \frac{\kappa_0 \ n}{\kappa_0 + n} (\bar{y} - \mu_0)^2 \\
{\color{3F9786}{\mu_1}} & = \frac{\kappa_0}{\kappa_0 + n} \mu_0 + \frac{n}{\kappa_0 + n} \bar{y} \\
{\color{3F9786}{\kappa_1}} & = \kappa_0 + n
\end{align*}
$$


<div class = "exercises">
**Exercise 9.7**

The `aida` package provides the convenience function `aida::sample_Norm_inv_chisq` for sampling from the 'normal inverse-$\chi^2$' prior. Here is the source code of this function:

```{r eval=F}
sample_Norm_inv_chisq <- function(
  n_samples = 10000,
  nu        = 1,
  var       = 1,
  mu        = 0,
  kappa     = 1
  )
{
  var_samples <- extraDistr::rinvchisq(
    n   = n_samples,
    nu  = nu,
    tau = var
  )
  mu_samples <- map_dbl(
    var_samples,
    function(s) rnorm(
      n    = 1,
      mean = mu,
      sd   = sqrt(s / kappa)
    )
  )
  tibble(
    sigma = sqrt(var_samples),
    mu    = mu_samples
  )
}
```

In the code below, we use this function to plot 10,000 samples from the prior with a particular set of parameter values. Notice the line `  filter(abs(value) <= 10)` which is useful for an informative plot (try commenting it out: what does that tell you about the range of values reasonably likely to get sampled?).

```{r}
# samples from the prior
samples_prior_1 <- aida::sample_Norm_inv_chisq(
  nu = 1,
  var = 1, # a priori "variance of the variance"
  mu = 0,
  kappa = 1
)

samples_prior_1 %>%
  pivot_longer(cols = everything(), names_to = "parameter", values_to = "value") %>%
  filter(abs(value) <= 10) %>%
  ggplot(aes(x = value)) +
  geom_density() + 
  facet_grid(~parameter, scales = "free")
```

To get comfortable with this 'normal inverse-$\chi^2$' distribution, fill in the `XXX` in the following code box (possibly removing or altering parts of the plotting code if you need to) to find parameter values that encode a prior belief according to which credible values of $\sigma$ are not much bigger than (very roughly) 7.5, and credible values of $\mu$ lie (very roughly) in the range of 15 to 25. (Hint: intuit what the meaning of each parameter value is by a trial-error-think method.) The plot you generate could look roughly like the one below.

(Motivation for the exercise: you should get familiar with this distribution, and also realize that it is clunky and that you might want to use a different prior structure in order to encode specific beliefs ... which is exactly why we might want to be more flexible and go beyond conjugate priors in some cases.)

```{r, eval = F}
# samples from the prior
samples_prior_2 <- aida::sample_Norm_inv_chisq(
  nu    = XXX,
  var   = XXX,
  mu    = XXX,
  kappa = XXX
)

samples_prior_2 %>%
  pivot_longer(cols = everything(), names_to = "parameter", values_to = "value") %>%
  filter(!(parameter == "mu" & (value >= 40 | value <= 0))) %>%
  filter(!(parameter == "sigma" & value >= 10)) %>%
  ggplot(aes(x = value)) +
  geom_density() + 
  facet_grid(~parameter, scales = "free")
```

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

```{r}
# samples from the prior
samples_prior_2 <- aida::sample_Norm_inv_chisq(
  nu = 1,
  var = 1, # a priori "variance of the variance"
  mu = 20,
  kappa = 1
)

samples_prior_2 %>%
  pivot_longer(cols = everything(), names_to = "parameter", values_to = "value") %>%
  filter(!(parameter == "mu" & (value >= 40 | value <= 0))) %>%
  filter(!(parameter == "sigma" & value >= 10)) %>%
  ggplot(aes(x = value)) +
  geom_density() + 
  facet_grid(~parameter, scales = "free")
```

</div>
</div>

</div>

Here is another convenience function from the `aida` package for obtaining posterior samples for the conjugate prior model, taking as input a specification of the prior beliefs. Again, we first show the function explicitly before applying it to the flower data set.

```{r eval=F}
get_samples_single_normal_conjugate <- function(
  data_vector,
  nu    = 1,
  var   = 1,
  mu    = 0,
  kappa = 1,
  n_samples = 1000
)
{
  n <- length(data_vector)
  aida::sample_Norm_inv_chisq(
    n_samples = n_samples,
    nu        = nu + n,
    var       = (nu * var + (n - 1) * var(data_vector) + (kappa * n) / (kappa + n)) / (nu + n),
    mu        = kappa / (kappa + n) * mu + n / (kappa + n) * mean(data_vector),
    kappa     = kappa + n
  )
}
```

The code below calls this function to obtain samples from the posterior for two different models.
This will help illustrate the effect of priors on the posterior once more, especially for a case like the one at hand where we have only rather few data observations.

```{r}
# posterior samples for prior 1
post_samples_A_conjugate_1 <- aida::get_samples_single_normal_conjugate(
  heights_A,
  nu    = 1,
  var   = 1,
  mu    = 0,
  kappa = 1,
  n_samples = 10000
)

# posterior samples for prior 2
post_samples_A_conjugate_2 <- aida::get_samples_single_normal_conjugate(
  heights_A,
  nu    = 1,
  var   = 1/1000,
  mu    = 40,
  kappa = 10,
  n_samples = 10000
)

rbind(
  aida::summarize_sample_vector(post_samples_A_conjugate_1$mu,    "mu") %>% mutate(model = 1),
  aida::summarize_sample_vector(post_samples_A_conjugate_1$sigma, "sigma") %>% mutate(model = 1),
  aida::summarize_sample_vector(post_samples_A_conjugate_2$mu,    "mu") %>% mutate(model = 2),
  aida::summarize_sample_vector(post_samples_A_conjugate_2$sigma, "sigma") %>% mutate(model = 2)
)

```

The posterior is a mixture of prior and likelihood.
The prior for model 1 is rather weak (high variance, low $\kappa$ leading to a large range of plausible values for $\mu$).
The prior for model 2 is rather biased.
The credible values of $\mu$ are rather high.

### Estimating the difference between group means

The ulterior "research question" to address is: should we believe that flowers of type B are higher, on average, than flowers of type A?
To address this question, it suffices to take samples for $\mu_{A}$ and $\mu_{B}$, obtained by one of the methods introduced in the previous sections (using the same model for both flower types, unless we have a good reason not to), and then to inspect the vector of differences between samples $\delta = \mu_{B} - \mu_{A}$.
If the derived samples of $\delta$ are credibly bigger than zero, there is reason to believe that there is a difference between flower types such that 'B' is bigger than 'A'.

So, let's use the (conjugate) prior of model 1 from above to also take 10,000 samples from the posterior when conditioning with the data in `heights_B`. Store the results in a vector called `post_samples_B_conjugate_1`.

```{r}
post_samples_B_conjugate_1 <- aida::get_samples_single_normal_conjugate(
  heights_B,
  nu    = 1,
  var   = 1,
  mu    = 0,
  kappa = 1,
  n_samples = 10000
)
```

The summary of the difference vector gives us information about credible values of $\delta = \mu_{B} - \mu_{A}$.

```{r}
delta_flower_heights <- post_samples_B_conjugate_1$mu - post_samples_A_conjugate_1$mu
aida::summarize_sample_vector(delta_flower_heights, name = 'delta')
```

We might conclude from this that a positive difference in height is credible.
More on Bayesian testing of such hypotheses about parameter values in Chapter \@ref(ch-03-07-hypothesis-testing-Bayes).
