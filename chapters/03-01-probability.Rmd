# (PART) Models and inferences {-}

# Basics of Probability Theory {#Chap-03-01-probability}

<hr>

<div style = "float:right; width:35%;">
<img src="visuals/badge-probability.png" alt="badge probability">  
</div>  

Probability is the basic ingredient of statistical inference.

In this chapter, we will cover the very basics of probability theory. We will visit its axiomatic definition and some common interpretations in Section \@ref(Chap-03-01-probability-basics), where we also start with the main mental exercise of this section: seeing how **probability distributions can be approximately represented by samples**. We will cover important concepts such as joint and marginal probability in Section \@ref(Chap-03-01-probability-marginal). This paves the way for learning about conditional probability and Bayes rule in Section \@ref(Chap-03-01-probability-conditional). Section \@ref(Chap-03-01-probability-random-variables) introduces the notion of a random variable. Finally, Section \@ref(Chap-03-01-probability-R) briefly covers how information about common probability distributions can be accessed in R.

```{block2, type='overview'}

**Learning goals:** 

- become familiar with the notion of probability and also:
  - its axiomatic definition
  - the notion of joint, marginal and conditional probability
- understand and apply Bayes rule
- get comfortable with random variables
- become able to handle probability distributions in R
- understand how probability distributions are approximately represented by samples

```

## Probability  {#Chap-03-01-probability-basics}

### Outcomes, events, observations

We are interested in the space $\Omega$ containing all **elementary outcome** $\omega_1,
\omega_2, \dots \in \Omega$ of a process or an event whose execution is (partially) random or
unknown. Elementary outcomes are mutually exclusive ($\omega_i \neq \omega_j \; \text{for} \,\forall i \neq j$). The set $\Omega$ exhausts all
possibilities.^[For the simplicity of exposure, we gloss over subtleties arising when
dealing with infinite sets $\Omega$. We make up for this when we define probability
*density* functions for continuous random variables, which have an uncountably infinite number of elementary outcomes. We will usually be concerned with continuous random variables within applied statistics.]


```{block2, type='infobox'}
**Example.** The set of elementary outcomes of a single coin flip is $\Omega_{\text{coin flip}} = \set{\text{heads}, \text{tails}}$. The elementary outcomes of tossing a six-sided die are $\Omega_{\text{standard die}} = \{$&#9856;, &#9857;, &#9858;, &#9859;, &#9860;, &#9861; $\}$.^[Think of $\Omega$ as a partition of the space of all possible ways in which the world could be, where we lump together into one partition cell (one elementary outcome) all ways in which the world could be that are equivalent regarding those aspects of reality that we are interested in. We do not care whether the coin lands in the mud or the sand. It only matters whether it came up heads or tails. Each elementary event can be realized in myriad ways. $\Omega$ is our, the modelers', first crude simplification of nature, abstracting away aspects we currently do not care about.]
```

An **event** $A$ is a subset of $\Omega$. Think of an event as a (possibly partial)
observation. We might observe, for instance, not the full outcome of tossing a die, but only
that there is a dot in the middle. This would correspond to the event
$A = \{$  &#9856;, &#9858;,  &#9860; $\}$,
i.e., observing an odd-numbered outcome. The *trivial observation* $A = \Omega$ and the
*impossible observation* $A = \emptyset$ are counted as events, too. The latter is included for
technical reasons that we don't need to know for our purpose.


For any two events $A, B \subseteq \Omega$, standard set operations correspond to logical
connectives in the usual way. For example, the conjunction $A \cap B$ is the observation of
both $A$ and $B$; the disjunction $A \cup B$ is the observation that it is either $A$ or $B$;
the negation of $A$, $\overline{A} = \set{\omega \in \Omega \mid \omega \not \in A}$, is the
observation that it is not $A$.

### Probability distributions

A **probability distribution** $P$ over $\Omega$ is a function
$P \ \colon \ \mathfrak{P}(\Omega) \rightarrow \mathbb{R}$ ^[For any of you who are interested in the precise mathematical description of probability space, $\mathfrak{P}(\Omega)$ is called Borel set. It is important since probabilities can be only defined for measurable sets. https://en.wikipedia.org/wiki/Borel_set] that assigns to all events $A \subseteq \Omega$ a real number (from the unit interval, see A1 below), such that the following (so-called Kolmogorov axioms) are satisfied:

A1. $0 \le P(A) \le 1$

A2. $P(\Omega) = 1$

A3. $P(A_1 \cup A_2 \cup A_3 \cup \dots) = P(A_1) + P(A_2) + P(A_3) + \dots$ whenever $A_1, A_2, A_3, \dots$ are mutually exclusive^[A3 is the axiom of *countable additivity*. Finite additivity may be enough for finite or countable sets $\Omega$, but infinite additivity is necessary for full generality in the uncountable case.]

Occasionally we encounter notation $P \in \Delta(\Omega)$ to express that $P$ is a probability
distribution over $\Omega$. (E.g., in physics, theoretical economics or game theory. Less so in psychology or statistics.) If $\omega \in \Omega$ is an elementary event, we often write $P(\omega)$ as a shorthand for $P(\set{\omega})$. In fact, if $\Omega$ is finite, it suffices to assign probabilities to elementary outcomes.

A number of rules follow immediately from the definition:

C1. $P(\emptyset) = 0$

C2. $P(\overline{A}) = 1 - P(A)$

C3. $P(A \cup B) = P(A) + P(B) - P(A \cap B)$ for any $A, B \subseteq \Omega$

# ```{block2, type='infobox'}
# **Exercise 7.1 ** Prove C1, C2 and C3 using A1, A2 and A3.
# ```
# 
# Solutions: C1: $P(\Omega \cup \emptyset) = P(\Omega) + P(\emptyset)  \Leftrightarrow  P(\Omega) = P(\Omega) + P(\emptyset)  \Leftrightarrow  0 = P(\emptyset)$ following A3 since $\omega$ and $\emptyset$ are mutually exclusive.
# C2: $P(\Omega) = P(A \cup \overline{A}) = P(A) + P(\overline{A}) = 1 $.
# C3: $P(A \cup B) = P((A-B) \cup (A \cap B) \cup (B-A)) = P(A-B) + P(A \cap B) + P(B-A) = (P(A \cup B) - P(B)) + P(A \cap B) + (P(A \cup B) - P(A)) = 2 P(A \cup B) - P(A) - P(B) + P(A \cap B)  \Leftrightarrow   P(A \cup B) = P(A) + P(B) - P(A \cap B)$


### Interpretations of probability

Preliminarily, it is reasonably safe to think of probability as defined above as a handy mathematical primitive which is useful for certain applications. There are at least three ways of thinking about where this primitive probability might come from:

1. **Frequentist:** Probabilities are generalizations of intuitions/facts about frequencies of events in repeated executions of a random event.
2. **Subjectivist:** Probabilities are subjective beliefs by a rational agent who is
  uncertain about the outcome of a random event.
3. **Realist:** Probabilities are the property of an intrinsically random world.

### Distributions as samples

No matter what your metaphysics of probability are, it is useful to realize that probability distributions can be approximately represented by sampling.

Think of an **urn** as a container with balls of different colors with different proportions (see Figure \@ref(fig:03-01-single-urn)). In the simplest case, there is a number of $N > 1$ balls of which $k > 0$ are black and $N-k > 0$ are white. (There are at least one black and one white ball.) For a single random draw from our urn we have: $\Omega_{\text{our urn}} = \set{\text{white}, \text{black}}$. We now draw from this urn with replacement. That is, we shake the urn, draw one ball, observe its color, take note of the color, and put it back into the urn. Each ball has the same chance of being sampled. If we imagine an infinite sequence of single draws from our urn with replacement, the limiting proportion with which we draw a black ball is $\frac{k}{N}$. This statement about frequency is what motivates saying that the probability of drawing a black ball on a single trial is (or should be^[If probabilities are subjective beliefs, a rational agent is, in a sense, normatively required to assign exactly this probability.])
$P(\text{black}) = \frac{k}{N}$.

```{r 03-01-single-urn, echo = F, fig.cap="An urn with seven black balls and three white balls. Imagine shaking this container, and then drawing blindly a single ball from it. It every ball has an equal probability of being drawn, what is the probability of drawing a black ball? That would be 0.7."}
knitr::include_graphics("visuals/urn-single.png")
```



The following code demonstrates how the proportion of black balls drawn from an urn-like in Figure \@ref(fig:03-01-single-urn) with $k = 7$ black balls and $N = 10$ balls in total, gravitates to the probability 0.7 when we keep drawing and drawing.

```{r}
# urn with 7 black and 3 white balls
urn <- c(
  rep("black", 7),
  rep("white", 3)
)

# number of samples to take 
n_samples <- 10000

# take `n_samples` samples from the urn (with replacement)
draws <- sample(
  # vector to sample from (default probability is uniform)
  x = urn, 
  # take a million samples
  size = n_samples,
  # put each ball back after drawing
  replace = TRUE
)

# plotting the development of the proportion of 'black' balls
tibble(
  draw_nr = 1:n_samples,
  draw = draws,
  # relative frequency of 'black' balls
  prop_black = cumsum(draw == "black") / draw_nr 
) %>% 
  # take only multiples of 10 draws
  filter(draw_nr %% 10 == 0) %>% 
  ggplot(aes(x = draw_nr, y = prop_black)) +
  geom_line(color = "darkgray") +
  # add a red line for the true limiting probability 
  geom_hline(aes(yintercept = 0.7), color = "firebrick") +
  labs(
    x = "number of draws",
    y = "proportion of 'black' balls drawn",
    title = "Temporal development of the proportion of draws from an urn"
  )

  
```

To sum this up concisely, we have a random process (drawing once from the urn) whose outcome is uncertain, and we convinced ourselves that the probability of an outcome corresponds to the relative frequency it occurs, in the limit of repeatedly executing the random process (i.e., sampling from the urn). From here it requires only a small step to a crucial but ultimately very liberating realization. If the probability of an event occurring can be approximated by its frequency in a large sample, then we can represent (say: internally in a computer) a probability distribution as one of two things:

1. a large set of (what is called: representative) samples; or even better as
2. an oracle (e.g., in the form of a clever algorithm) which quickly returns a representative sample.

This means that, for approximately computing with probability, we can represent distributions through samples or a sample-generating function. We do not need to know precise probability or be able to express them in a mathematical formula. Samples or **sampling is enough to approximate probability distributions**.

## Structured events & marginal distributions {#Chap-03-01-probability-marginal}

### Probability table for a flip-and-draw scenario

Suppose we have two urns. Both have $N=10$ balls. Urn 1 has $k_1=2$ black and $N-k_1 = 8$ white
balls. Urn 2 has $k_2=4$ black and $N-k_2=6$ white balls. We sometimes draw from urn 1,
sometimes from urn 2. To decide from which urn a ball should be drawn, we flip a fair coin. If it comes up heads, we draw from urn 1;
if it comes up tails, we draw from urn 2. The process is visualized in Figure \@ref(fig:03-01-flip-and-draw) below.

An elementary outcome of this two-step process of flip-and-draw is a pair $\tuple{\text{outcome-flip}, \text{outcome-draw}}$. The set of all possible such outcomes is:

$$\Omega_{\text{flip-and-draw}} = \set{\tuple{\text{heads}, \text{black}}, \tuple{\text{heads}, \text{white}}, \tuple{\text{tails}, \text{black}}, \tuple{\text{tails}, \text{white}}}\,.$$

The probability of event $\tuple{\text{heads}, \text{black}}$ is given by multiplying the probability of seeing "heads" on the first flip, which happens with probability $0.5$, and then drawing a black ball, which happens with probability $0.2$, so that $P(\tuple{\text{heads}, \text{black}}) = 0.5 \mult 0.2 = 0.1$. The probability distribution over $\Omega_{\text{flip-draw}}$ is consequently as in Table \@ref(tab:flipdrawprobabilities). (If in doubt, start flipping & drawing and count your outcomes.)

```{r flipdrawprobabilities, echo = F}
knitr::kable(
  tibble(
    " " = c("black", "white"),
    heads = c("$0.5 \\times 0.2 = 0.1$", "$0.5 \\times 0.8 = 0.4$"),
    tails = c("$0.5 \\times 0.4 = 0.2$", "$0.5 \\times 0.6 = 0.3$")
  ),
  booktabs = T,
  caption = 'Joint probability table for the flip-and-draw scenario',
  escape = F
)
```


```{r 03-01-flip-and-draw, echo = F, fig.cap="The flip-and-draw scenario, with transition and full path probabilities."}
knitr::include_graphics("visuals/flip-and-draw-scenario.png")
```



### Structured events and joint-probability distributions

Table \@ref(tab:flipdrawprobabilities) is an example of a **joint probability distribution** over a structured event space, which here have two dimensions. Since
our space of outcomes is the Cartesian product of two simpler outcome spaces, namely
$\Omega_{flip-\&-draw} = \Omega_{flip} \times \Omega_{draw}$,^[With
  $\Omega_{\text{flip}} = \set{\text{heads}, \text{tails}}$ and
  $\Omega_{\text{draw}} = \set{\text{black}, \text{white}}$.] we can use notation
$P(\text{heads}, \text{black})$ as shorthand for $P(\tuple{\text{heads}, \text{black}})$. More
generally, if $\Omega = \Omega_1 \times \dots \Omega_n$, we can think of $P \in \Delta(\Omega)$
as a joint probability distribution over $n$ subspaces.

### Marginalization

If $P$ is a joint-probability distribution over event space $\Omega = \Omega_1 \times \dots \Omega_n$, the **marginal distribution** over subspace  $\Omega_i$, $1 \le i \le n$ is the probability distribution that assigns to all $A_i \subseteq \Omega_i$ the probability:^[This notation, using $\sum$, assumes that subspaces are countable. In other cases, a parallel definition with integrals can be used.]

$$  P(A_i) = \sum_{A_1 \subseteq \Omega_{1}, \dots , A_{i-1} \subseteq \Omega_{i-1}, A_{i+1} \subseteq \Omega_{i+1}, \dots, A_n \subseteq \Omega_n} P(A_1, \dots, A_{i-1}, A_{i}, A_{i+1}, \dots A_n) $$

For example, the marginal distribution of draws derivable from Table \@ref(tab:flipdrawprobabilities) has $P(\text{black}) = P(\text{heads, black}) + P(\text{heads, white}) = 0.3$ and $P(\text{white}) = 0.7$.^[The term ``marginal distribution'' derives from such probability tables, where traditionally the sum of each row/column was written in the margins.] The marginal distribution of coin flips derivable from the joint probability distribution in Table \@ref(tab:flipdrawprobabilities) gives $P(\text{heads}) = P(\text{tails}) = 0.5$, since the sum of each column is exactly $0.5$. 

## Conditional probability {#Chap-03-01-probability-conditional}

Let us assume probability distribution $P \in \Delta(\Omega)$ and events $A,B \subseteq \Omega$ are given. The conditional probability of $A$ given $B$, written as $P(A \mid B)$, gives the probability of $A$ on the assumption that $B$ is true.^[We also verbalize this as "the conditional probability of $A$ conditioned on $B$."] It is defined like so:

$$P(A \mid B) = \frac{P(A \cap B)}{P(B)}$$

Conditional probabilities are only defined when $P(B) > 0$.^[Updating with events which have probability zero entails far more severe adjustments of the underlying belief system than just ruling out information hitherto considered possible. Formal systems that capture such *belief revision* are studied in formal epistemology by  @Halpern2003:Reasoning-about .]

```{block2, type="infobox"}
**Example.** If a die is unbiased, each of its six faces has equal probability to come up after a toss. The probability of event $B = \{$ &#9856;, &#9858;, &#9860; $\}$ that the tossed number is odd has probability $P(B) = \frac{1}{2}$. The probability of event $A = \{$ &#9858;, &#9859;,  &#9860;, &#9861; $\}$ that the tossed number is bigger than two is $P(A) = \frac{2}{3}$. The probability that the tossed number is bigger than two \emph{and} odd is $P(A \cap B) = P(\{$ &#9858;,  &#9860; $\}) = \frac{1}{3}$. The conditional probability of tossing a number that is bigger than two, when we know that the toss is odd, is $P(A \mid B) = \frac{1 / 3}{1 / 2} = \frac{2}{3}$.
```

Algorithmically, conditional probability first rules out all events in which $B$ is not true
and then simply renormalizes the probabilities assigned to the remaining events in such a way
that the relative probabilities of surviving events remain unchanged. Given this, another way
of interpreting conditional probability is that $P(A \mid B)$ is what a rational agent should
\emph{should} believe about $A$ after observing (nothing more than) that $B$ is true. The
agent rules out, possibly hypothetically, that $B$ is false, but otherwise does not change
opinion about the relative probabilities of anything that is compatible with $B$.

### Bayes rule

Looking back at the joint-probability distribution in Table \@ref(tab:flipdrawprobabilities), the conditional probability $P(\text{black} \mid \text{heads})$ of drawing a black ball, given that the initial coin flip
showed heads, can be calculated as follows:

$$
P(\text{black} \mid \text{heads}) =
\frac{P(\text{black} , \text{heads})}{P(\text{heads})} =
\frac{0.1}{0.5} = 0.2
$$
This calculation, however, is quite excessive. We can read out the conditional probability directly already from the way the flip-and-draw scenario was set up. After flipping heads, we draw from urn 1, which has $k=2$ out\ of $N=10$ black balls, so clearly: if the flip returns head, then the probability of a black ball is $0.2$. Indeed, in a step-wise random generation process like the flip-and-draw scenario, some conditional probabilities are very clear, and sometimes given by definition. These are, usually, the conditional probabilities that define how the process unfolds forward in time, so to speak.

**Bayes rule** is a way of expressing, in a manner of speaking, conditional probabilities in terms of the
"reversed" conditional probabilities:

$$P(B \mid A) = \frac{P(A \mid B) \mult P(B)}{P(A)}$$

Bayes rule is a straightforward corollary of the definition of conditional probabilities,
according to which $P(A \cap B) = P(A \mid B) \mult P(B)$, so that:


$$
P(B \mid A) =
\frac{P(A \cap B)}{P(A)} =
\frac{P(A \mid B) \cdot P(B)}{P(A)}
$$


Bayes rule allows for reasoning backward from observed causes to likely underlying effects. When we have a feed-forward model of how unobservable effects probabilistically constrain observable outcomes, Bayes rule allows us to draw inferences about *latent/unobservable variables* based on the observation of their downstream effects.

Consider yet again the flip-and-draw scenario. But now assume that Jones flipped the coin and
drew a ball. We see that it is black. What is the probability that it was drawn from urn 1,
or equivalently, that the coin landed heads? It is not $P(\text{heads}) = 0.5$, the so-called
*prior probability* of the coin landing heads. It is a conditional probability, also
called the *posterior probability*,^[The terms *prior* and *posterior*
  make sense when we think about an agent's belief state before (prior to) and after (posterior
  to) an observation.] namely $P(\text{heads} \mid \text{black})$. But it is not as easy and straightforward to write down as the reverse probability
$P(\text{black} \mid \text{heads})$ of which we said above that it is an almost trivial part of
the set up of the flip-and-draw scenario. It is here that Bayes rule has its purpose:

$$
P(\text{heads} \mid \text{black}) =
\frac{P(\text{black} \mid \text{heads}) \mult P(\text{heads})}{P(\text{black})} =
\frac{0.2 \mult 0.5}{0.3} =
\frac{1}{3}
$$
This result is quite intuitive. Drawing a black ball from urn 2 (i.e. after seeing tails) is twice
as likely as drawing a black ball from urn 1 (i.e. after seeing heads). Consequently, after
seeing a black ball drawn, with equal probabilities of heads and tails, the probability that
the coin landed tails is also twice as large as that it landed heads.

# ```{block2, type='infobox'}
# **Exercise 7.2 ** You have been coughing for the last few days and have been tested for COVID-19. The test result is negative (You are not immune to the virus). It is known that around 6% of the population has immunity against the virus. Furthermore, some information about the accuracy of the test are known. The specificity of the test (The test result is negative when the subject really does not have immunity, true negative) is 98%. The sensitivity of the test(true positive) is 95%. Can you tell how likely is it that you are truly not immune to the virus?
# ```
# 
# Solutions: https://www.oehoedatascience.com/wp-content/uploads/2020/06/Bayes-Rule-Applied-on-Covid-19-Testing.pdf
# First, lets abbreviate the test result being negative or positive as $t-$ and $t+$ and actual immunity as $-$ and $+$. What we want to find out is $P(-|t-)$. According to Bayes rule, $P(-|t-) = \frac {P(t-|-) P(-)} {P(t-)}$. We are given that $P(t-|-) = 0.98$, $P(t-|+) = 1 - P(t+|+) = 0.05$ and $P(-) = 1 - P(+) = 0.94$. Furthermore, $P(t-) = \sum_{i \in \text{immunity}} P(t-, i) = P(t-,+) + P(t-,-) = P(t-|+) P(+) + P(t-|-) P(-) = 0.9242$. Putting this all together, we get $P(-|t-) \approx 99.7 \%$. So it seems like you can be pretty certain that you never had a virus. 


```{block2, type="infobox"}
**Excursion: Bayes rule for data analysis** In later chapters, we will use Bayes rule for data analysis. The flip-and-draw scenario structurally "preflects" what will happen later. Think of the color of the ball drawn as the *data* $D$ which we observe. Think of the coin as a *latent parameter* $\theta$ of a statistical model. Bayes rule for data analysis then looks like this:
  
  $$P(\theta \mid D) = \frac{P(D \mid \theta) \ P(\theta)}{P(D)}$$
  
We will discuss this at length in Chapter \@ref(Chap-03-03-models) and thereafter.
```

### Stochastic (in-)dependence {#Chap-03-01-probability-independence}

Event $A$ is **stochastically independent** of $B$ if, intuitively speaking, learning $B$ does not change one's beliefs about $A$, i.e. $P(A \mid B) = P(A)$. If $A$ is stochastically independent of $B$, then $B$ is stochastically independent of $A$ because:

$$
\begin{aligned}
 P(B \mid A) 
 & =
 \frac{P(A \mid B) \ P(B)}{P(A)} && \text{[Bayes rule]}
 \\
 & =
 \frac{P(A) \ P(B)}{P(A)} && \text{[by ass. of independence]}
 \\
 & =
 P(B) && \text{[cancellation]}
 \\
\end{aligned}
$$


For example, imagine a flip-and-draw scenario where the initial coin flip has a bias of $0.8$ towards heads, but each of the two urns has the same number of black balls, namely $3$ black and $7$ white balls. Intuitively and formally, the probability of drawing a black ball is then *independent* of the outcome of the coin flip; learning that the coin landed heads, does not change our beliefs about how likely the subsequent draw will result in a black ball. The probability table for this example is in Table \@ref(tab:flipdrawprobabilities-independent).

```{r flipdrawprobabilities-independent, echo = F}
knitr::kable(
  tibble(
    " " = c("black", "white", "$\\Sigma$ columns"),
    "heads" = c("$0.8 \\mult 0.3 = 0.24$", "$0.8 \\mult 0.7 = 0.56$", 0.8),
    tails = c("$0.2 \\mult 0.3 = 0.06$", "$0.2 \\mult 0.7 = 0.14$", 0.2),
    "$\\Sigma$ rows" = c(0.3, 0.7, 1)
  ),
  booktabs = T,
  caption = 'Joint probability table for a flip-and-draw scenario where the coin has a bias of $0.8$ towards heads and where each of the two urns holds $3$ black and $7$ white balls.',
  escape = F
)
```

Independence shows in Table \@ref(tab:flipdrawprobabilities-independent) in the fact that the probability in each cell is the product of the two marginal probabilities. This is a direct consequence of stochastic independence:



```{proposition, label = "conjunction-independent-events", name = "Probability of conjunction of stochastically independent events"}
For any pair of events $A$ and $B$ with non-zero probability:
  
$$P(A \cap B) = P(A) \ P(B) \, \ \ \ \ \text{[if } A \text{ and } B \text{ are stoch. independent]} $$ 
```

```{proof}
By assumption of independence, it holds that $P(A \mid B) = P(A)$. But then:
  
$$
\begin{aligned}
 P(A \cap B)
 & =
 P(A \mid B) \ P(B) && \text{[def. of conditional probability]}
 \\
 & =
 P(A) \ P(B) && \text{[by ass. of independence]}
\end{aligned}
$$
```

&nbsp;

## Random variables {#Chap-03-01-probability-random-variables}

So far, we have defined a probability distribution as a function that assigns a probability to
each subset of the space $\Omega$ of elementary outcomes. A special case occurs when we are
interested in a space of numeric outcomes.

A **random variable** is a function $X \ \colon \ \Omega \rightarrow \mathbb{R}$ that
assigns to each elementary outcome a numerical value. It is reasonable to think of this number as a **summary statistic**: a number that captures one aspect of relevance of what is actually a much more complex chunk of reality.

```{block2, type='infobox'}
**Example.** For a single flip of a coin we have $\Omega_{\text{coin flip}} = \set{\text{heads}, \text{tails}}$. A usual way of mapping this onto numerical outcomes is to define $X_{\text{coin flip}} \ \colon \ \text{heads} \mapsto 1; \text{tails} \mapsto 0$. Less trivially, consider  flipping a coin two times. Elementary outcomes should be individuated by the outcome of the   first flip and the outcome of the second flip, so that we get:
$$
    \Omega_{\text{two flips}} = \set{\tuple{\text{heads}, \text{heads}}, \tuple{\text{heads}, \text{tails}},
    \tuple{\text{tails}, \text{heads}}, \tuple{\text{tails}, \text{tails}}}
$$
Consider the random variable $X_{\text{two flips}}$ that counts the total number of heads. Crucially, $X_{\text{two flips}}(\tuple{\text{heads}, \text{tails}}) = 1 = X_{\text{two flips}}(\tuple{\text{tails}, \text{heads}})$. We assign the same numerical value to different elementary outcomes since the order is not relevant.
```


### Notation & terminology

Traditionally random variables are represented by capital letters, like $X$. The numeric values they take on are written as small letters, like $x$.

We write $P(X = x)$ as a shorthand for the probability $P(\set{\omega \in \Omega \mid X(\omega) = x})$, that an event $\omega$ occurs which is mapped onto $x$ by the random variable $X$. For example, if our coin is fair, then $P(X_{\text{two flips}} = x) = 0.5$ for $x=1$ and $0.25$ for $x=0,2$. Similarly, we can also write $P(X \le x)$ for the probability of observing an event that $X$ maps to a number not bigger than $x$.

If the range of $X$ is countable (not necessarily finite), we say that $X$ is **discrete**. For ease of exposition, we may say that if the range of $X$ is an interval of real numbers, $X$ is called **continuous**.


### Cumulative distribution functions, mass & density

For a discrete random variable $X$, the **cumulative distribution function** $F_X$ associated with $X$ is defined as:
$$
  F_X(x) = P(X \le x) = \sum_{x' \in \set{\text{Rng}(X) \mid x' \le x}} P(X = x)
$$
The **probability mass function** $f_x$ associated with $X$ is defined as:
$$
  f_X(x) = P(X = x)
$$

<div class="infobox">
**Example.** Suppose we flip a coin with a bias of $\theta$ towards heads $n$ times. What is the probability that we  will see heads $k$ times? If we map the outcome of heads to 1 and tails to 0, this  probability is given by the Binomial distribution, as follows:
$$
    \text{Binom}(K = k ; n, \theta) = \binom{n}{k} \,  \theta^{k} \, (1-\theta)^{n-k}
$$
Here $\binom{n}{k} = \frac{n!}{k!(n-k)!}$ is the binomial coefficient. It gives the number of possibilities of drawing an unordered subset with $k$ elements from a set with a total of $n$ elements. Figure \@ref(fig:ch-03-BinomialDistribution-Mass) gives an example of the Binomial distribution, concretely its probability mass function, for two values of the coin's bias, $\theta = 0.25$ or $\theta = 0.5$, when flipping the coin $n=24$  times. Figure \@ref(fig:ch-03-BinomialDistribution-Cumulative) gives the corresponding cumulative  distributions.
<!-- add some more explanation -->

```{r ch-03-BinomialDistribution-Mass, echo = FALSE, fig.cap = "Examples of the Binomial distribution. The $y$-axis give the probability of seeing $k$ heads when flipping a coin $n=24$ times with a bias of either $\\theta = 0.25$ or $\\theta = 0.5$."}
binom.plot.data = expand.grid(n = 24, theta = c(0.25, 0.5), k = 0:24) %>%
  mutate(
    probability = dbinom(k,n,theta),
    theta = as.factor(theta)
  )
ggplot(binom.plot.data, aes(x = k, y = probability, fill = theta)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = project_colors) +
  ylab(bquote("Binom(K = k; n = 24," ~ theta * ")"))
```

```{r ch-03-BinomialDistribution-Cumulative, echo = FALSE, fig.cap = "Examples of the cumulative distribution of the Binomial. The $y$-axis gives the probability of seeing $k$ or fewer outcomes of heads when flipping a coin $n=24$ times with a bias of either $\\theta = 0.25$ or $\\theta = 0.5$."}
binom.plot.data = expand.grid(n = 24, theta = c(0.25, 0.5), k = 0:24) %>%
  mutate(
    probability = pbinom(k,n,theta),
    theta = as.factor(theta)
  )
ggplot(binom.plot.data, aes(x = k, y = probability, fill = theta)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = project_colors) +
  ylab(bquote("Binom(K" <= "k; n = 24," ~ theta * ")"))
```

</div>



For a continuous random variable $X$, the probability $P(X = x)$ will usually be zero: it is virtually impossible that we will see precisely the value $x$ realized in a random event that can realize uncountably many numerical values of $X$. However, $P(X \le x)$ does take workable values and so we define the cumulative distribution function $F_X$ associated with $X$ as:
$$
  F_X(x) = P(X \le x)
$$
Instead of a probability **mass** function, we derive a **probability density function** from the cumulative function as:
$$
  f_X(x) = F'(x)
$$
A probability density function can take values greater than one, unlike a probability mass
function.

<div class="infobox">
**Example.** The **Gaussian (Normal) distribution** characterizes many natural distributions of  measurements which are symmetrically spread around a central tendency. It is defined as:
$$
    \mathcal{N}(X = x ; \mu, \sigma) = \frac{1}{\sqrt{2 \sigma^2 \pi}} \exp \left ( -
      \frac{(x-\mu)^2}{2 \sigma^2} \right)
$$
where parameter $\mu$ is the *mean*, the central tendency, and parameter $\sigma$ is the *standard deviation*. Figure \@ref(fig:ch-03-NormalDistribution-Density) gives examples of the probability density function of two normal distributions. Figure \@ref(fig:ch-03-NormalDistribution-Cumulative) gives the corresponding cumulative distribution functions.

```{r ch-03-NormalDistribution-Density, echo = F, fig.cap = "Examples of the Normal distribution. In both cases $\\mu = 0$, once with $\\sigma = 1$ and once with $\\sigma = 4$"}
ggplot(data.frame(x = c(-8, 8)), aes(x = x)) +
  stat_function(fun = function(x) dnorm(x, sd = 4), aes(color = "4"), size = 3) +
  stat_function(fun = function(x) dnorm(x, sd = 1), aes(color = "1"), size = 3) +
  scale_colour_manual("standard deviation",
                      breaks = c("4", "1"),
                      values = project_colors) +
  ylab(bquote("Norm(X = x;" ~ mu == 0 * "," ~ sigma * ")")) 
```

```{r ch-03-NormalDistribution-Cumulative, echo = F, fig.cap = "Examples of the cumulative normal distribution corresponding to the previous probability density functions."}
ggplot(data.frame(x = c(-8, 8)), aes(x = x)) +
  stat_function(fun = function(x) pnorm(x, sd = 4), aes(color = "4"), size = 3) +
  stat_function(fun = function(x) pnorm(x, sd = 1), aes(color = "1"), size = 3) +
  scale_colour_manual("standard deviation",
                      breaks = c("4", "1"),
                      values = project_colors) +
  # ylab("Norm(X <= x ; mu = 0, sd")
  ylab(bquote("Norm(X" <= "x;" ~ mu == 0 * "," ~ sigma * ")")) 
```

</div>

### Expected value & variance

The **expected value** of a random variable $X$ is a measure of central tendency. It tells us, like the name suggests, which average value of $X$ we can expect when repeatedly sampling from $X$. If $X$ is continuous, the expected value is:
$$
  \mathbb{E}_X = \sum_{x} x \mult f_X(x)
$$
If $X$ is continuous, it is:
$$
  \mathbb{E}_X = \int x \mult f_X(x) \ \text{d}x
$$
The expected value is also frequently called the **mean**.

The **variance** of a random variable $X$ is a measure of how much likely values of $X$ are spread or clustered around the expected value. If $X$ is discrete, the variance is:
$$
  \text{Var}(X) = \sum_x (\mathbb{E}_X - x)^2 \mult f_X(x)
$$
If $X$ is continuous, it is:
$$
  \text{Var}(X) = \int (\mathbb{E}_X - x)^2 \mult f_X(x) \ \text{d}x
$$

<div class="infobox">
**Example.** If we flip a coin with bias $\theta = 0.25$ a total of $n=24$ times, we expect on average to see $n \mult \theta = 24 \mult 0.25 = 6$ outcomes showing heads.^[This is not immediately obvious from our definition, but it is intuitive and you can derive it.] The variance of a binomally distributed variable is  $n \mult \theta \mult (1-\theta) = 24 \mult 0.25 \mult 0.75 = \frac{24 \mult 3}{16} = \frac{18}{4} = 4.5$.

The expected value of a normal distribution is just its mean $\mu$ and its variance is $\sigma^2$.
</div>

### Composite random variables

Composite random variables are random variables generated by mathematical operations conjoining other random variables. For example, if $X$ and $Y$ are random variables, then we can define a new derived random variable $Z$  using notation like: 

$$Z = X + Y$$

This notation looks innocuous but is conceptually tricky yet ultimately very powerful. On the face of it, we are doing as if we are using `+` to add two functions. But a sampling-based perspective makes this quite intuitive. We can think of $X$ and $Y$ as large samples, representing the probability distributions in question. Then we build a sample by just adding elements in $X$ and $Y$. (If samples are of different size, just add a random element of $Y$ to each $X$.)

Consider the following concrete example. $X$ is the probability distribution of rolling a fair dice with six sides. $Y$ is the probability distribution of flipping a biased coin which lands heads (represented as number 1) with probability 0.75. The derived probability distribution $Z = X + Y$ can be approximately represented by samples derived as follows:

```{r}
n_samples <- 1e6
# `n_samples` rolls of a fair dice
samples_x <- sample(
  1:6, 
  size = n_samples, 
  replace = T
)

# `n_samples` flips of a biased coin
samples_y <- sample(
  c(0,1), 
  prob = c(0.25, 0.75), 
  size = n_samples, 
  replace = T
)

samples_z <- samples_x + samples_y

tibble(outcome = samples_z) %>% 
  dplyr::count(outcome) %>% 
  mutate(n = n/sum(n)) %>% 
  ggplot(aes(x = outcome, y = n)) +
  geom_col() +
  labs(y = "proportion")
```


## Probability distributions in R {#Chap-03-01-probability-R}

Appendix \@ref(app-91-distributions) covers a number of common probability distributions that are relevant for the purposes of this course. Appendix \@ref(app-92-exponential-family) furthermore provides additional theoretical background on the *exponential family*, an important class of probability distributions widely used in statistics.

R has built-in functions for most common probability distributions. Further distributions are covered in additional packages. If `mydist` is the name of a probability distribution, then R routinely offers four functions for `mydist`, distinguished by the first letter:

1. `dmydist(x, ...)` the *density function* gives the probability (mass/density) $f(x)$ for `x`
2. `pmydist(x, ...)` the *cumulative probability function* gives the cumulative distribution function $F(x)$ for `x`
3. `qmydist(p, ...)` the *quantile function* gives the value $x$ for which `p = pmydist(x, ...)`
4. `rmydist(n, ...)` the *random sample function* returns `n` samples from the distribution

For example, the family of functions for the normal distribution has the following functions:

```{r}

# density of standard normal at x = 1
dnorm(x = 1, mean = 0, sd = 1)

# cumulative density of standard normal at q = 0
pnorm(q = 0, mean = 0, sd = 1)

# point where the cumulative density of standard normal is p = 0
qnorm(p = 0.5, mean = 0, sd = 1)

# n = 3 random samples from a standard normal
rnorm(n = 3, mean = 0, sd = 1)

```

