# Categorical predictors {#Chap-04-03-predictors}

<hr>

The previous chapters applied linear regression models to cases where we wanted to predict a metric variable $y$ based on the values of associated metric measurements $x_i$ ($1 \le i \le n$).
In this chapter, we are generalizing this approach to also deal with the case where a predictor $x_i$ is a categorical variable, such as an indicator value showing which group or experimental condition a measurement of $y$ belongs to.
In this way, at the end of the chapter, we will be able to apply linear regression modeling to the analysis of (metric) measurements, for instance, from a factorial design - a common design type of psychological experiments (see Chapter \@ref(Chap-02-01-data)).

As we will see in this chapter, the only "trick" to generalizing linear regression modeling to also cover **categorical predictors**, as we will call them, is to map levels of a categorical variable onto numbers.
For example, if we have two groups in a predictor variable $x$, say group $A$ and $B$, we could just encode group $A$ as a value of $x = 0$ and group $B$ as a value of $x = 1$.
But there are many sensible mappings of this kind, and many helplessly ridiculous ones.
The relevant technical term here is **contrast coding**, i.e., a scheme of mapping categorical distinctions onto numeric representations in such a way that the theoretically interesting *contrasts* (i.e., the distinctions between groups that we want to test) can be easily tested with the resulting regression model. 

The chapter is structured as follows.
Section \@ref(Chap-04-03-predictors-two-levels) uses the [Simon task data](#app-93-data-sets-simon-task) to cover the case of a single categorical predictor with just two category levels (e.g., just groups $A$ and $B$).
Section \@ref(Chap-04-03-predictors-multi-levels) then uses data from the [Mental Chronometry experiment](#app-93-data-sets-mental-chronometry) to look at the more general case of a single categorical predictor with more than two category levels (e.g., just groups $A$, $B$ and $C$). 
Section \@ref(Chap-04-03-predictors-multiple-predictors) considers cases with several categorical predictors, including their **interaction**, based on the [politeness data set](#app-93-data-sets-politeness).

The video below provides a dense coverage of single-predictor contrast coding, i.e., the contents of Sections \@ref(Chap-04-03-predictors-two-levels) and \@ref(Chap-04-03-predictors-multi-levels).

<iframe src="https://player.vimeo.com/video/422832571" width="640" height="360" frameborder="0" allow="fullscreen" allowfullscreen></iframe>

```{block, type='infobox'}
The learning goals for this chapter are:

- understand the basic idea behind *contrast coding*
- be able to interpret the results of a regression analysis that uses *treatment coding*
- be able to interpret inferences for models including an *interaction coefficient* for two categorical predictors

```

## Single two-level predictor {#Chap-04-03-predictors-two-levels}

Let's revisit the data from the [Simon task](#app-93-data-sets-simon-task).
Just like in chapter \@ref(ch-03-07-hypothesis-testing-Bayes), we will be looking at the hypothesis that, among all correct responses, the mean reaction times for the congruent condition are lower than those of the incongruent condition.

```{r}
# extract just the currently relevant columns 
#  from the data set
data_ST_excerpt <- aida::data_ST %>% 
  filter(correctness == "correct") %>% 
  select(RT, condition)

# show the first couple of lines
head(data_ST_excerpt, 5)
```

Notice that this tibble contains the data in a tidy format, i.e., each row contains a pair of associated measurements. 
We want to explain or predict the variable `RT` in terms of the variable `condition`.
The variable `RT` is a metric measurement.
But the variable `condition` is categorical variable with two category levels.

Before we head on, let's look at the data (again).
Here's a visualization of the distribution of RTs in each condition:

```{r}
data_ST_excerpt %>% 
  ggplot(aes(x = condition, y = RT, color = condition, fill = condition)) +
  geom_violin() +
  theme(legend.position = "none")
```

The means for both conditions are:

```{r}
data_ST_excerpt %>% 
  group_by(condition) %>% 
  summarize(mean_RT = mean(RT))
```

The difference between the means of conditions is:

```{r}
data_ST_excerpt %>% filter(condition == "incongruent") %>% pull(RT) %>% mean() -
  data_ST_excerpt %>% filter(condition == "congruent") %>% pull(RT) %>% mean()
```

While numerically this difference seems high, the question remains whether this difference is, say, big enough to earn our trust.
We address this question here using posterior inference based on a regression model. 
Notice that we simply use the same formula syntax as before: we want a model that explains `RT` in terms of `condition`.

```{r}
fit_brms_ST <- brm(
  formula = RT ~ condition,
  data = data_ST_excerpt
)
```

Let's inspect the summary information for the posterior samples, which we do here using the `summary` function for the `brms_fit` object from which we extract information only about the `fixed` effects, showing the mean (here called "Estimate") and indicators of the lower and upper 95% inner quantile.

```{r}
summary(fit_brms_ST)$fixed[,c("l-95% CI", "Estimate", "u-95% CI")]
```

We see that the model inferred a value for an "Intercept" variable and for another variable called "conditionincongruent".
What are these?
If you look back at the empirically inferred means, you will see that the mean estimate for "Intercept" corresponds to the mean of RTs in the "congruent" condition and that the mean estimate for the variable "conditionincongruent" closely matches the computed difference between the means of the two conditions.
And, indeed, that is what this regression model is doing for us.
Using a uniform formula syntax, `brms` has set up a regression model in which a predictor, given as a character (string) column, was internally coerced somehow into a format that produced an estimate for the mean of one condition and an estimate for the difference between conditions.

How do these results come about?
And why are the variables returned by `brms` called "Intercept" and "conditionincongruent"?
In order to use the simple linear regression model, the categorical predictor $x$ has been coded as either $0$ or $1$.
Concretely, `brms` has introduced a new predictor variable, call it `new_predictor`, which has value $0$ for the "congruent" condition and $1$ for the "incongruent" condition.
By default, `brms` chooses the level that is alphanumerically first as the so-called **reference level**, assigning to it the value $0$.
Here, that's "congruent".

The result would look like this:

```{r}
data_ST_excerpt %>% 
  mutate(new_predictor = ifelse(condition == "congruent", 0, 1)) %>% 
  head(5)
```
Now, with this new numeric coding of the predictor, we can calculate the linear regression model as usual:

$$
\begin{aligned}
\xi_i & = \beta_0 + \beta_1 x_i & y_i & \sim \text{Normal}(\mu = \xi_i, \sigma)
\end{aligned}
$$

As a consequence, the linear model's intercept parameter $\beta_0$ can be interpreted as the predicted mean of the reference level: if for some $i$ we have $x_i = 0$, then the predictor $\xi_i$ will just be $\xi_i = \beta_0$; whence that the intercept $\beta_0$ will be fitted to the mean of the reference level if for some $i$ we have $x_i = 1$ instead, the predicted value will be computed as $\xi_i = \beta_0 + \beta_1$, so that the slope term $\beta_1$ will effectively play the role of the difference $\delta$ between the mean of the groups.
The upshot is that we can conceive of a **$t$-test as a special case of a linear regression model!**

Schematically, we can represent this coding scheme for coefficients like so:
```{r, echo = F}
tribble(
  ~"condition", ~"x_0", ~"x_1",
  "congruent", 1, 0,
  "incongruent", 1, 1
)
```


<div class = "exercises">
**Exercise 14.1**
For the given data below, compute (or guess) the MLEs of the regression coefficients. Choose the appropriate 0/1 encoding of group information.
We have two groups, and three measurements of $y$ for each:

groupA: (1,0,2) and groupB: (10,13,7)

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

For $\xi_i  = \beta_0 + \beta_1 x_i$, let $x_i =0$ if the data point is from groupA and $x_i=1$ if it's from groupB. Then the mean of groupA is computed by the intercept $\mu_A  = \beta_0$ and the mean of groupB is computed as the sum of the intercept and the slope $\mu_B  = \beta_0 + \beta_1$. Since $\mu_A = 1$ and $\mu_B = 10$, we can guess that $\beta_0 = 1$ and $\beta_1 = 10 - 1 = 9$.

</div>
</div>
</div>

## Single multi-level predictor {#Chap-04-03-predictors-multi-levels}

<div style = "float:right; width:15%;">
<img src="visuals/badge-mental-chronometry.png" alt="badge-mental-chronometry">  
</div>  

The 0/1 coding scheme above works fine for a single categorical predictor value with two levels.
It is possible to use linear regression also for categorical predictors with more than two levels.
Only, in that case, there are quite a few more reasonable **contrast coding** schemes, i.e., ways to choose numbers to encode the levels of the predictor.

The [mental chronometry data](#app-93-data-sets-mental-chronometry) has a single categorical predictor, called `block`, with three levels, called "reaction", "goNoGo" and "discrimination".
We are interested in regressing reaction times, stored in variable `RT`, against `block`.
Our main question of interest is whether these inequalities are supported by the data:

$$
\text{RT in 'reaction'} < 
\text{RT in 'goNoGo'} <
\text{RT in 'discrimination'}
$$
So we are interested in the $\delta$s, so to speak, between 'reaction' and 'goNoGo' and between 'discrimination' and 'goNoGo'.


Let's consider only the data relevant for our current purposes:

```{r}
# select the relevant columns
data_MC_excerpt <- aida::data_MC_cleaned %>% 
  select(RT, block)

# show the first couple of lines
data_MC_excerpt %>% 
  head(5)
```

Here are the means of the reaction times for different `block` levels:

```{r}
data_MC_excerpt %>% 
  group_by(block) %>% 
  summarize(mean_RT = mean(RT))
```

And here is a plot of the distribution of measurements in each block:

```{r, echo = F}
data_MC_excerpt %>% 
  ggplot(aes(x = RT, color = block, fill = block)) +
  geom_density(alpha = 0.3)
```


To fit this model with `brms`, we need a simple function call with the formula `RT ~ block` that precisely describes what we are interested in, namely explaining reaction times as a function of the experimental condition:

```{r eval = T}
fit_brms_mc <- brm(
  formula = RT ~ block,
  data = data_MC_excerpt
)
```


To inspect the posterior fits of this model, we can extract the relevant summary statistics as before:

```{r}
summary(fit_brms_mc)$fixed[,c("l-95% CI", "Estimate", "u-95% CI")]
```

Notice that there is an intercept term, as before. 
This corresponds to the mean reaction time of the reference level, which is again set based on alphanumeric ordering, so corresponding to "discrimination".
There are two slope coefficients, one for the difference between the reference level and "goNoGo" and another for the difference between the reference level and the "reaction" condition.

These intercepts are estimated to be credibly negative, suggesting that the "discrimination" condition indeed had the highest mean reaction times.
This answers one half of the comparisons we are interested in:

$$
\text{RT in 'reaction'} < 
\text{RT in 'goNoGo'} <
\text{RT in 'discrimination'}
$$
Unfortunately, it is not directly possible to read off information about the second comparison we care about, namely the comparison between "reaction" and "goNoGo".
And here is where we see the point of **contrast coding** pop up for the first time.
We would like to encode predictor levels ideally in such a way that we can read off (test) the hypotheses we care about directly.
In other words, if possible, we would like to have parameters in our model in the form of slope coefficients, which directly encode the $\delta$s, so to speak, that we want to test.^[To be precise, it is possible to also test derived random variables from the posterior samples. So, it is not *necessary* to encode the contrasts of interests directly. But, most often, in Bayesian analyses it will make sense to put priors on exactly these $\delta$s (e.g., skeptical priors biased against a hypothesis to be tested) and for *that* purpose, it is (almost) practically necessary to have the relevant contrasts expressed as slope coefficients in the model.]

In the case at hand, all we need to do is change the reference level.
If the reference level is the "middle category" (as per our ordered hypothesis), the two slopes will express the contrasts we care about.
To change the reference level, we only need to make `block` a factor and order its levels manually, like so:

```{r}
data_MC_excerpt <- data_MC_excerpt %>% 
  mutate(block_reordered = factor(block, levels = c("goNoGo", "reaction", "discrimination")))
```

We then run another Bayesian regression model, regressing `RT` against `block_reordered`.

```{r}
fit_brms_mc_reordered <- brm(
  formula = RT ~ block_reordered,
  data = data_MC_excerpt
)
```

And inspect the summary of the posterior samples for the relevant coefficients:

```{r}
summary(fit_brms_mc_reordered)$fixed[,c("l-95% CI", "Estimate", "u-95% CI")]
```
Now the "Intercept" corresponds to the new reference level "goNoGo".
And the two slope coefficients give the differences to the other two levels.

Which numeric encoding leads to this result?
In formulaic terms, we have three coefficients $\beta_0, \dots, \beta_2$.
The predicted mean value for observation $i$ is $\xi_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}$. 
We assign numeric value $1$ for predictor $x_1$ when the observation is from the "reaction" block.
We assign numeric value $1$ for predictor $x_2$ when the observation is from the "discrimination" block.
Schematically, what we now have is:
```{r, echo = F}
tribble(
  ~"block", ~"x_0", ~"x_1", ~"x_2",
  "goNoGo", 1, 0, 0,
  "reaction", 1, 1, 0,
  "discrimination", 1, 0, 1
)
```

As we may have expected, the 95% inter-quantile range for both slope coefficients (which, given the amount of data we have, is almost surely almost identical to the 95% HDI) does not include 0 by a very wide margin.
We could therefore conclude that, based on a Bayesian approach to hypothesis testing in terms of posterior estimation, the reaction times of conditions are credibly different.

The coding of levels in terms of a reference level is called *treatment coding*, or also *dummy coding*.
The video included at the beginning of this chapter discusses further contrast coding schemes, and also shows in more detail how a coding scheme translates into "directly testable" hypotheses.

<div class = "exercises">
**Exercise 14.2**

Suppose that there are three groups, A, B, and C as levels of your predictor. You want the regression intercept to be the mean of group A. You want the first slope to be the difference between the means of group B and group A. And, you want the second slope to be the difference between the mean of C and B. How do you numerically encode these contrasts in terms of numeric predictor values?

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

Schematically, like this:

```{r, echo = F}
tribble(
  ~"group", ~"x_0", ~"x_1", ~"x_2",
  "A", 1, 0, 0,
  "B", 1, 1, 0,
  "C", 1, 1, 1
)
```


As group A is a reference category, $\beta_0$ expresses the mean reaction time of group A. The mean reaction time of group B is $\beta_0 + \beta_1$, so we need $(x_{i1} =1 , x_{i2} = 0)$ for any $i$ which is of group B. In the text above, the mean reaction time of group C is given by $\beta_0 + \beta_2$. However, the value we need now is given by $\beta_0 + \beta_1 + \beta_2$, so $(x_{i1} =1 , x_{i2} = 1)$.

</div>
</div>
</div>

## Multiple predictors {#Chap-04-03-predictors-multiple-predictors}

Factorial designs, which have more than one categorical predictor variable, are common in experimental psychology.
Any contrast coding scheme usable for encoding a single categorical predictor can, in principle, also be used when there are multiple categorical predictors.
But having multiple categorical predictors also requires some additional considerations relating to how (the model assumes that) different predictors might or might not interact with one another.

Here is an informal example.
Suppose that we have metric measurements of how tasty a snack is perceived to be.
There are two categorical factors that we want to use to predict the average tastiness of a snack.
The first predictor is `mayo` and we encode it numerically as: 0 if the dish does not contain mayonnaise and 1 if it does.
The second predictor is `chocolate` and we encode it similarly as: 0 if the dish does not contain chocolate and 1 if it does.
Suppose we estimate these two  slope coefficients (one for `mayo` and one for `chocolate`) for our imaginary data set and find that both are credibly positive.
That means that there is reason to believe that, all else equal, when we find `mayo` in a snack we may expect it to be rated as more tasty, and, all else equal, when we find `chocolate` in a snack we may also expect it to be rated as more tasty.^[Notice the deliberate avoidance of causal terminology. We should not say that there is reason to believe that *adding* `mayo` to any dish will make it taste better, just that, epistemically put, observing it in a dish should alter our expectations.] 
But what about a dish with *both* `mayo` *and* `chocolate`?
Maybe we can agree to assume for the sake of argument that, on average, snacks containing both `mayo` and `chocolate` are *not* rated as tasty at all.
Or, at least, we might want to include in our model the possibility that the combination of `mayo` and `chocolate` has a different effect than the sum of the contributions of (i) `mayo` on its own and that of (ii) `chocolate` of its own.
That is why, when we have multiple categorical predictors, we also often want to include yet another type of slope coefficient, so-called **interaction terms**, that capture how the combination of different factor levels from different categorical predictors, well, interact.
If you like a more precise characterization at this moment already (although an example below will make things hopefully much clearer), we could say that, in the context of a linear regression model, an interaction between levels of several predictors is a (potential) deviation from the sum of all of the additive effects of the individual predictor levels in isolation.

<div style = "float:right; width:15%;">
<img src="visuals/badge-politeness.png" alt="badge-politeness">
</div>

To make this more precise, let us consider the example of the [politeness data](#app-93-data-sets-politeness).^[Part of the following content is a distilled version of a short tutorial on Bayesian regression modeling for factorial designs [@FrankeRoettger2019:Bayesian-regres], which can be downloaded [here](https://psyarxiv.com/cdxv3).]
The to-be-predicted data are measurements of voice pitch in a $2 \times 2$ factorial design, with factors `gender` and `context`.
The factor `gender` has (sadly only) two levels: "male" and "female".
The factor `context` has two levels, namely "informal" for informal speech situations and "polite" for polite speech situations.

Let us first load the data & inspect it.

```{r}
politeness_data <- aida::data_polite
politeness_data %>% head(5)
```

The research hypotheses of interest are:

1. **H1: (gender)**: the voice pitch of male speakers is lower than that of female speakers;
2. **H2: (context)**: the voice pitch of speech in polite contexts is lower than in informal contexts; and
3. **H3: (interaction)**: the effect of context (= the difference of voice pitch between polite and informal context; as mentioned in the second hypothesis) is larger for female speakers than for male speakers.

The first two hypotheses are statements related to what is often called **main effects**, namely differences between levels of a single categorical predictor, averaging over all levels of any other categorical predictor.
Consequently, we could also rephrase this as saying: "We expect a main effect of gender (H1) and a main effect of context (H2)." only thereby omitting the direction of the difference between respective factor levels.
The third hypothesis is a more convoluted formulation about the interaction of the two categorical predictors.

To understand hypotheses about main effects and interactions better, at least in the easiest case of a $2 \times 2$ factorial design, it is useful to consider stylized diagrams, like in Figure \@ref(fig:04-03-2x2-hypotheses), which show how the data would look like if main effects or various interaction relations are present or absent.
Concretely, the panels in Figure \@ref(fig:04-03-2x2-hypotheses) depict the following types of situations:

- **A**: no main effect (neither gender nor context) and no interaction; 
- **B**: main effect of gender, no main effect of context and no interaction; 
- **C**: main effect of context, no main effect of gender and no interaction; 
- **D**: main effects of both context and gender but no interaction; 
- **E**: main effects of both context and gender with an interaction amplifying the strength of the main effect of context for the female category; and 
- **F**: as in E but with a different kind of interaction (effect reversal).

Notice that the type of situation shown in panel E is the expectation derivable from the conjunction of the hypotheses H1-H3 formulated above: we predict/expect main effects for both predictors (in the direction shown in panel E) and we expect the effect of context to be stronger for female speakers than for male speakers.

```{r 04-03-2x2-hypotheses, echo = F, fig.cap="Schematic representation of the presence/absence of main effects and (different kinds of) interactions. The situations shown are as follows: A: no main effect (neither gender nor context) and no interaction; B: main effect of gender only w/ no interaction; C: main effect of context only w/ no interaction; D: main effects of both context and gender but no interaction; E: main effects of both context and gender with an interaction amplifying the strength of the main effect of context for the female category (this is the situation envisaged by hypotheses 1-3 from the main text); F: as in E but with a different kind of interaction (effect reversal).", fig.height=8}
data_2x2_hypotheses <- tribble(
  ~'situation', ~'gender', ~'context', ~'pitch',
  # no main effects
  "A: no main effects", 'male', 'polite',  5,
  "A: no main effects", 'male', 'informal',  5,
  "A: no main effects", 'female', 'polite',  5,
  "A: no main effects", 'female', 'informal',  5,
  # gender
  "B: main eff. gender", 'male', 'polite', 4,
  "B: main eff. gender", 'male', 'informal', 4,
  "B: main eff. gender", 'female', 'polite', 5,
  "B: main eff. gender", 'female', 'informal', 5,
  # context
  "C: main eff. context", 'male', 'polite', 5,
  "C: main eff. context", 'male', 'informal', 4,
  "C: main eff. context", 'female', 'polite', 5,
  "C: main eff. context", 'female', 'informal', 4,
  # main effect both 
  "D: both main eff.s", 'male', 'polite', 4,
  "D: both main eff.s", 'male', 'informal', 3,
  "D: both main eff.s", 'female', 'polite', 6,
  "D: both main eff.s", 'female', 'informal', 5,
  # interaction (strength)
  "E: interaction (strength)", 'male', 'polite', 4,
  "E: interaction (strength)", 'male', 'informal', 3,
  "E: interaction (strength)", 'female', 'polite', 6,
  "E: interaction (strength)", 'female', 'informal', 3.5,
  # interaction (direction)
  "F: interaction (direction)", 'male', 'polite', 3,
  "F: interaction (direction)", 'male', 'informal', 2,
  "F: interaction (direction)", 'female', 'polite', 3.5,
  "F: interaction (direction)", 'female', 'informal', 6
)

data_2x2_hypotheses %>% 
  ggplot(aes(y = pitch, x = gender, fill = context)) +
  geom_line(
    aes(group = context, color = context), 
    # alpha = 0.3,
    position = position_dodge(0.1)
  ) + 
  geom_point(
    position = position_dodge(0.1), 
    pch = 21, 
    colour = "black",
    size = 5
  ) +
  theme(
    axis.text.y = element_blank(),
    axis.ticks = element_blank()
  ) +
  ylim(1.5, 6.5) +
  scale_colour_manual(breaks = c("informal", "polite"),
                      labels = c("informal", "polite"),
                      values = c("#f1a340", "#998ec3")) +
  scale_fill_manual(breaks = c("informal", "polite"),
                      labels = c("informal", "polite"),
                      values = c("#f1a340", "#998ec3")) +
  facet_wrap(.~situation, ncol = 2, scales = "free") 
```

Let us now take a look at the actual data:

```{r, echo = F}
# this code is copy pasted from tutorial paper
politedata <- politeness_data 
politedata.agg <- 
  politedata %>% 
    group_by(gender, context, sentence) %>% 
    summarize(mean_frequency = mean(pitch))

politedata.agg2 <- 
  politedata %>%
  group_by(gender, context) %>% 
  summarize(mean_frequency = round(mean(pitch), 0))

ggplot(data = politedata.agg, 
       aes(x = gender, 
           y = mean_frequency, 
           colour = context)) + 
  geom_point(position = position_dodge(0.5), 
             alpha = 0.3, 
             size = 3) +
  geom_point(data = politedata.agg2, 
             aes(x = gender, 
                 y = mean_frequency, 
                 #colour = context,
                 fill = context),
             position = position_dodge(0.5), 
             pch = 21, 
             colour = "black",
             size = 5) +
  scale_x_discrete(breaks = c("F", "M"),
                  labels = c("female", "male")) +
  scale_y_continuous(expand = c(0, 0), breaks = (c(50,100,150,200,250,300)), limits = c(50,300)) +
  scale_colour_manual(breaks = c("inf", "pol"),
                      labels = c("informal", "polite"),
                      values = c("#f1a340", "#998ec3")) +
  scale_fill_manual(breaks = c("inf", "pol"),
                      labels = c("informal", "polite"),
                      values = c("#f1a340", "#998ec3")) +
  ylab("pitch in Hz\n") +
  xlab("\ngender")
```

Judging from visual inspection, we might say that the empirical data most resembles panel D in Figure \@ref(fig:04-03-2x2-hypotheses).
It looks as if there might be a rather strong effect of gender.
The measurements in the female category seem (on average) higher than in the male category.
Also, there might well be a main effect of context.
Probably the voice pitch in informal contexts is higher than in polite contexts, but we cannot be as sure as for a potential main effect of gender.
It is very difficult to discern whether the data supports the hypothesized interaction.

In the following, we are therefore going to test these hypotheses (more or less directly) with two different kinds of coding schemes: treatment coding and sum coding.

### Treatment coding

In a $2 \times 2$ factorial design there are essentially four pairs of factor levels (so-called **design cells**).
For the politeness data, these are female speakers in informal contexts, female speakers in polite contexts, male speakers in informal contexts and male speakers in polite contexts. 
Different coding schemes exist by means of which different comparisons of means of design cells (or single factors) can be probed. 
A simple coding scheme for differences in our $2 \times 2$ design is shown in Figure \@ref(fig:Chap-04-02-beyond-simple-regression-factorial-coefficients).
This is a straightforward extension of *treatment coding* for the single predictors introduced previously which additionally includes a potential interaction.

```{r Chap-04-02-beyond-simple-regression-factorial-coefficients, echo = F, fig.cap="Regression coefficients for a factorial design (using so-called 'treatment coding').", fig.width=4, out.width='90%'}
knitr::include_graphics("visuals/coefficients_factorial_design.png")
```

The coding scheme in Figure \@ref(fig:Chap-04-02-beyond-simple-regression-factorial-coefficients) considers the cell "female+informal" as the reference level and therefore models its mean as intercept $\beta_0$.
We then have a slope term $\beta_{\text{pol}}$ which encodes the difference between female pitch in informal and female pitch in polite contexts. 
Analogous reasoning holds for $\beta_{\text{male}}$. 
Finally, we also include a so-called **interaction term**, denoted as $\beta_{\text{pol&male}}$ in Figure \@ref(fig:Chap-04-02-beyond-simple-regression-factorial-coefficients). 
The interaction term quantifies how much a change away from the reference level in both variables differs from the sum of unilateral changes.

Another way of describing what the interaction term $\beta_{\text{pol&male}}$ captures is that it represents the difference which the manipulation of context has on female and male speakers.
To see this, notice that the "extent of the effect of context", i.e., the decrease in pitch between informal and polite contexts, for female speakers is:

$$
\text{eff_context_on_female} = \beta_0 - (\beta_0 + \beta_\text{pol})  = - \beta_\text{pol}
$$
The bigger this number, the larger, so to speak, "the effect of context on female speakers".

The effect of context on male speaker's pitch is correspondingly:

$$
\text{eff_context_on_male} = (\beta_0 + \beta_\text{male} + \beta_{\text{pol}} + \beta_\text{pol&male}) - (\beta_\text{pol} + \beta_\text{male}) = - \beta_{\text{pol}} - \beta_\text{pol&male}
$$

Therefore, the difference -comparing female and male speakers- of the effect of context is:
$$\text{eff_context_on_female} - \text{eff_context_on_male} = \beta_\text{pol&male}$$

How do these model coefficients help address the research hypotheses we formulated above? - 
The interaction term $\beta_\text{pol&male}$ directly relates to hypothesis 3 above, namely that the context-effect is larger for female speakers than for male speakers.
In other words, we can express H3 as the parameter-based hypothesis that:

$$\textbf{H3: (interaction)} \ \ \ \ \beta_\text{pol&male} > 0$$
The other two hypotheses are not directly expressible as a statement involving a single coefficient.
But they can be expressed as a complex hypothesis involving more than one coefficient of the model.
Hypothesis H1 states that the pitch of male speakers (averaging over context types) is lower than that of female speakers (averaging over context types).
This translates directly into the following statement (where the LHS/RHS is the average pitch of male/female speakers):

$$
\frac{1}{2} (\beta_0 + \beta_\text{male} + \beta_0 + \beta_\text{male} + \beta_\text{pol} + \beta_\text{pol&male}) < 
\frac{1}{2} (\beta_0 + \beta_0 + \beta_\text{pol})
$$
This can be simplified to:

$$
\textbf{H1: (gender)} \ \ \ \ \beta_\text{male} + \frac{1}{2} \beta_\text{pol&male} < 0
$$

Similar reasoning leads to the following formulation of hypothesis H2 concerning a main effect of factor context:

$$
\textbf{H2: (context)} \ \ \ \ \beta_\text{pol} + \frac{1}{2} \beta_\text{pol&male} < 0
$$

To test these hypotheses, we can fit a regression model with this coding scheme using the formula `pitch ~ gender * context`. 
Importantly the star `*` between explanatory variables `gender` and `context` indicates that we also want to include the interaction term.^[If the interaction term should be excluded, the formula `pitch ~ gender + context` can be used, so with `+` instead of `*`.]

```{r eval = F}
fit_brms_politeness <- brm(
  # model 'pitch' as a function of 'gender' and 'context',
  #  also including the interaction between `gender` and `context`
  formula = pitch ~ gender * context,
  data = politeness_data
)
```

```{r echo = F, eval = T}
fit_brms_politeness <- readRDS('models_brms/politeness_fit.rds')
```

The summary statistics below lists Bayesian summary statistics for the (marginal) posteriors of the model parameters indicated in Figure \@ref(fig:Chap-04-02-beyond-simple-regression-factorial-coefficients).

```{r}
summary(fit_brms_politeness)$fixed[,c("l-95% CI", "Estimate", "u-95% CI")]
```

The function `brms::hypothesis` can test the relevant hypotheses based on the `brms_fit` object stored in `fit_brms_politeness`.
Starting with H1, we find very strong support for a main effect of gender:

```{r}
brms::hypothesis(fit_brms_politeness, "genderM + 0.5 * genderM:contextpol < 0")
```

As for H2, we also find very strong evidence in support of a belief in a main effect of context:

```{r}
brms::hypothesis(fit_brms_politeness, "contextpol + 0.5 * genderM:contextpol < 0")
```

In contrast, based on the data and the model, there is at best very mildly suggestive evidence in favor of the third hypothesis according to which female speakers are more susceptible to pitch differences induced by different context types.

```{r}
brms::hypothesis(fit_brms_politeness, "genderM:contextpol > 0")
```

We can interpret this as saying that, given model and data, it is plausible to think that male speakers had lower voice pitch than female speakers (averaging over both context types).
We may also conclude that given model and data, it is plausible to think that voice pitch was lower in polite contexts than informal contexts (averaged over both levels of factor `gender`).
The posterior of the interaction term `genderM:contextpol` does not give any indication to think that 0, or any value near it, is not plausible. 
This can be interpreted as saying that there is no indication, given model and data, to believe that male speakers' voice pitch changes differently from informal to polite contexts than female speakers' voice pitch does.

<div class = "exercises">
**Exercise 14.3**

Based on the estimate given above, what is the mean estimate for male speakers speaking in informal contexts?

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">
The mean estimate for male speakers speaking in informal contexts is given by $\beta_0 +\beta_{\text{male}} = 261.02993 -116.53009 \approx 144$.
</div>
</div>
</div>

### Sum coding

Treatment coding allowed us to *directly* test H3 in terms of a single coefficient, but testing of hypotheses about so-called "main effects" (H1 and H2) cannot be directly read off a single coefficient's posterior.
As hypotheses about main effects are natural and common in experimental psychology, another coding scheme is very popular, namely **sum coding**.^[The comparisons tested by sum coding are the same contrasts also addressed by analysis of variance (ANOVA), a special case of regression modeling which was very popular and is still the standard method of statistical analysis for factorial designs in some areas of experimental psychology, despite good arguments in favor of (hierarchical) generalized regression analyses [@Jaeger2008:Categorical-dat].]

Figure \@ref(fig:Chap-04-03-coefficients-sum-coding) shows how the mean of each design cell in our $2\times2$ design is expressed in terms of four regression coefficients.
Parameter $\beta_0$ is called "intercept" as usual, but encodes the so-called **grand mean**, i.e. the mean value of all data observations.
To see this, just sum all of the four terms in Figure \@ref(fig:Chap-04-03-coefficients-sum-coding) and divide by 4: the result is $\beta_0$.
The parameters $\beta_\text{male}$ and $\beta_\text{pol}$ are slope coefficients, but they now encode the deviance from the grand mean.
For example, $\beta_\text{male}$ encodes the difference between (i) the average pitch of all measurements taken from male participants and (ii) the grand mean.
Finally, the interaction coefficient $\beta_\text{pol&male}$ serves the same function as before in treatment coding, namely to make room for a difference in the strength of one main effect, e.g., of context, on the levels of the other predictor, e.g., gender.


```{r Chap-04-03-coefficients-sum-coding, echo = F, fig.cap="Regression coefficients for a factorial design (using so-called 'sum coding').", fig.width=4, out.width='90%'}
knitr::include_graphics("visuals/coefficients_factorial_design_sum-coding.png")
```

It is then clear that under treatment coding, the hypotheses H1 and H2, which target main effects, can be straightforwardly stated as inequalities concerning singular coefficients, namely:

$$
\textbf{H1: (gender)} \ \ \ \ \beta_\text{male} < 0
$$

$$
\textbf{H2: (context)} \ \ \ \ \beta_\text{pol} < 0
$$

What is less obvious is that the interaction term, as defined under sum coding, still directly expresses the interaction hypothesis H3.
To see this, calculate as before:

$$
\begin{align*}
& \text{eff_context_on_female} \\
& = (\beta_0 - \beta_\text{male} - \beta_\text{pol} + \beta_\text{pol&male}) -   (\beta_0 - \beta_\text{male} + \beta_\text{pol} - \beta_\text{pol&male})  \\
& = - 2 \beta_\text{pol} + 2 \beta_\text{pol&male}
\end{align*}
$$

The effect of context on male speaker's pitch is:

$$
\begin{align*}
& \text{eff_context_on_male} \\ 
& = (\beta_0 + \beta_\text{male} - \beta_\text{pol} - \beta_\text{pol&male}) -   (\beta_0 + \beta_\text{male} + \beta_\text{pol} + \beta_\text{pol&male})  \\
& = - 2 \beta_\text{pol} - 2 \beta_\text{pol&male}
\end{align*}
$$

Consequently, the difference -comparing female and male speakers- of the effect of context under sum coding is expressed as:
$$\text{eff_context_on_female} - \text{eff_context_on_male} = 4 \beta_\text{pol&male}$$
To implement sum coding for use in `brms`, R provides the functions `contrasts` and `contr.sum`.
Here is an example.

```{r}
# make predictors 'factors' b/c that's required for contrast coding
#   also: change order to match coding assumed in the main text
data_polite <- aida::data_polite %>% 
  mutate(
    gender = factor(gender, levels = c('M', 'F')),
    context = factor(context, levels = c('pol', 'inf'))
  )

# apply 'sum' contrasts
contrasts(data_polite$context) <- contr.sum(2)
contrasts(data_polite$gender) <- contr.sum(2)

# add intelligible name to the new contrast coding
colnames(contrasts(data_polite$context)) <- ":polite"
colnames(contrasts(data_polite$gender)) <- ":male"

# run brms as usual
fit_brms_politeness_sum <- brm(
  pitch ~ gender * context,
  data_polite
)
```

We can inspect the coefficients as usual:

```{r}
summary(fit_brms_politeness_sum)$fixed[, c("l-95% CI", "Estimate" ,"u-95% CI")]
```

The summary statistics for the posterior already directly address all three hypotheses in question, but we should compare our previous results to the full results of using `brms::hypothesis` also for the sum-coded analysis.

```{r}
# testing H1
brms::hypothesis(fit_brms_politeness_sum, "gender:male < 0")
# testing H2
brms::hypothesis(fit_brms_politeness_sum, "context:polite < 0")
# testing H3
brms::hypothesis(fit_brms_politeness_sum, "gender:male:context:polite > 0")
```

Since we didn't use any priors, which could have altered results slightly between treatment- and sum-coded regression modeling, we find (modulo sampling imprecision) the same "evidence ratios" and posterior probabilities of these hypotheses.
The overall conclusions are therefore the exact same: evidence for both main effects; no evidence for interaction.
