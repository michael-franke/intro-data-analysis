# (PART) Frequentist statistics {-}

# Null Hypothesis Significance Testing {#ch-05-01-frequentist-hypothesis-testing}

<hr>

<div style = "float:right; width:35%;">
<img src="visuals/badge-testing.png" alt="badge testing">  
</div>  

In this chapter, we are going to look at basic notions and principles of frequentist statistics.
Section \@ref(ch-05-01-frequentist-testing-overview) sets the scene by describing what frequentist statistics is and how it differs from Bayesian approaches.
The key notion of a $p$-value is discussed in Section \@ref(ch-03-05-hypothesis-p-values).
Section \@ref(ch-03-05-hypothesis-testing-CLT) takes an excursion to introduce the Central Limit Theorem.
Section \@ref(ch-03-04-hypothesis-significance-errors) shortly surveys the main ideas of the Neyman-Pearson approach to statistical inference, including notions like *$\beta$-error* and *statistical power*.
Section \@ref(ch-05-01-frequentist-testing-confidence-intervals) covers *confidence intervals*.
Finally, Section \@ref(ch-03-05-hypothesis-testing-tests) introduces some of the most commonly used frequentist tests from a model-centric perspective.

```{block, type='infobox'}
The learning goals for this chapter are:

- become familiar with frequentist hypothesis testing
  - see the differences between the Bayesian and the frequentist approaches
- understand key statistical notions such as:
  - sampling distribution
  - $p$-value
  - confidence interval
  - statistical significance
- understand and become able to apply and interpret basic frequentist tests:
  - binomial test, $t$-tests, ANOVA, linear regression, $\chi^2$-test, likelihood-ratio test
```

## Frequentist statistics: why & how {#ch-05-01-frequentist-testing-overview}

Bayesian ideas have first been expressed in the 17th century (by Thomas Bayes) and have been given a solid mathematical treatment starting in the 18th century (by French mathematician Pierre-Simon Laplace).
A rigorous philosophical underpinning of subjective probability has been given in the early 20th century (by authors like Frank Ramsey or Bruno de Finetti).
Still, in the first part of the 20th century, the rise of statistics as a modern, ubiquitous tool for empirical knowledge gain in science took off in a distinctly non-Bayesian direction.
Key figures in the early development of statistical ideas (such as Ronald Fisher, Egon Pearson and Jerzy Neyman), were rather opposed to Bayesian ideas.
While the precise mechanisms of this historical development are very interesting (from the point of view of history and philosophy of science), suffice it to say here that at least the following two (interrelated) objections are very likely to have contributed a lot to how history unfolded:

1. By requiring subjective priors, Bayesian statistical methods have an air of vagueness and non-rigidity.
2. Bayesian inference is very demanding (mathematically and computationally).^[Notice also that Bayesian computation is a very recent field. The MCMC algorithm was only invented in the middle of the 20th century. Before close to the turn of the 21st century, researchers lacked wide-spread access to computers powerful enough to run algorithms that approximate Bayesian inference for complex models.]

As an alternative to Bayesian approaches, the dominant method of statistical inference of the 20th century is **frequentist statistics**.
As a matter of strong philosophical conviction, frequentist statistics makes do entirely without subjective probability: no priors on parameters, no priors on models, no researcher beliefs whatsoever.
A crude (and certainly simplified) explanation of why frequentist approaches eschew subjective beliefs is this. 
Extreme frequentism denies that a probability distribution over a latent parameter like $\theta$ is meaningful.
Whatever we choose, the choice cannot be justified or defended in a scientifically rigorous manner.
The only statements about probabilities that are conceptually sound, according to a fundamentalist frequentist interpretation, are those that derive from intuitions about limiting frequencies when (hypothetically) performing a random process (like throwing a dice or drawing a ball from an urn) repeatedly.
Bluntly put, there is no "(thought) experiment" which can be repeated so that its objective results, on average, align with whatever subjective prior beliefs the Bayesian analysis needs. 

Of course, the objections to the use of priors could be less fundamentalist. 
Researchers who have no metaphysical troubles with subjective priors in principle might reject the use of priors in data analysis because they feel that the necessity to specify priors is a burden or a spurious degree of freedom in empirical science. 
Thus, priors should be avoided to stay as objective, procedurally systematic, and streamlined as possible.

So, after having seen Bayesian inference in action extensively, you may wonder: how -on earth- is inference and hypothesis testing even possible without (subjective) priors, even if assumed to be non-informative? - 
Frequentist statistics has devised very clever means of working around subjective probability.
Frequentists do accept probabilities, of course.
But only of the objective kind: in the form of a likelihood function which can be justified by appeal to repeated executions and limiting frequencies of events.
In a (simplified) nutshell, you may think of frequentism as squeezing everything (inference, estimation, testing, model comparison) out of suitably constructed likelihood functions, which are constructed around point-valued assumptions about any remaining model parameters. 

Central to frequentist statistics is the notion of a *$p$-value*, which plays a central role in hypothesis testing.
Let's consider an example.
The goal is to get a first idea of how frequentist methods can work without subjective probability.
More detail will be presented in the following sections.
Say, we are interested in the question of whether a given coin is fair, so: $\theta_c = 0.5$.
This assumption $\theta_{c} = 0.5$ is our *null hypothesis* $H_{0}$.^[The term "null hypothesis" does not want to imply that the hypothesis is that some value of interest is equal to zero (although in practice that is frequently the case). The term rather implicates that this hypothesis is put out there in order to be possibly refuted, i.e., nullified, by the data [@Gigerenzer2004:Mindless-Statis].]
We refuse to express beliefs about the likely value of $\theta_c$, both prior and posterior.
But we *are* allowed to engage in hypothetical mind games.
So, let's *assume for the sake of argument* that the coin is fair.
So, we assume the null hypothesis to be true for the sake of argument.
That's not a belief; it's a thought experiment: nothing wrong with that.
We build a **frequentist model** around this point-valued assumption, and call it a **null-model**.
The null-model assumes that $\theta_{c} = 0.5$ but also uses the obvious likelihood function, $\text{Binomial}(k, N, \theta_c = 0.5)$, stating how likely it would be to observe $k$ heads in $N$ tosses, for fixed $\theta_c = 0.5$.
These likelihoods are not subjective, but grounded in limiting frequencies of events.
Now, we also bring into the picture some actual data.
Unsurprisingly: $k=7$, $N=24$.
We can now construct a measure of how surprising this data observation would be on the purely instrumental assumption that the null-model is true.
In more Bayesian words, we are interested in measuring the perplexity of an agent whose belief is captured by the null-model (so an agent who genuinely and inflexibly believes that the coin is fair), when that agent observes $k=7$ and $N=24$.
If the agent's perplexity is extremely high, i.e., if the null-model would not have predicted the data at hand to a high enough degree, we take this as **evidence against the null-model**.
Since the main contestable assumption in the construction of the null-model is the null hypothesis itself, we treat this as evidence against the null hypothesis, and so are able to draw conclusions about a point-valued hypothesis of interest, crucially, without any recourse to subjective probability.
The quantified notion of evidence against a null-model is the $p$-value, famously introduced by Ronald Fisher, which we will discuss in the next section \@ref(ch-03-05-hypothesis-p-values).

Historically, there have been different schools of thought in frequentist statistics, all of which relate in some way or other to a quantified notion of evidence against a null-model, but that differ substantially in other ways.
The three main approaches, shown in Figure \@ref(fig:ch-03-04-frequenist-flavors), are Fisher's approach, the Neyman-Pearson approach and the hybrid approach, often referred to simply as NHST (= null hypothesis significance testing), which is the standard practice today.
In simplified terms, the main difference is that Fisher's approach is belief-oriented, while the Neyman-Pearson approach is action-oriented.
Fisher's approach calculates $p$-values as measures of evidence against a model, but does not necessarily prescribe what to do with this quantitative measure.
The approach championed by Neyman-Pearson calculates a $p$-value and also fixes categorical decision rules relating to whether to accept or reject the null-model (or an alternative model, which is also explicitly featured in the Neyman-Pearson approach).
The main rationale for the Neyman-Pearson approach is to to have a tight upper bound on certain types of statistical error.
Section \@ref(ch-03-04-hypothesis-significance-errors) deals with the Neyman-Pearson approach in more detail, where we also introduce the notion of **statistical significance**.

While both Fisher's approach and the Neyman-Pearson approach are intrinsically sound, in actual practice we often see a rather inconsistent mix of ideas and practices.
That's why Figure \@ref(fig:ch-03-04-frequenist-flavors) characterizes the modern NHST approach more in terms of how we actually find it applied in practice (and, unfortunately, also taught) rather than how it should be.


```{r ch-03-04-frequenist-flavors, echo = F, fig.cap="The three most prominent flavors of frequentist statistics.", out.width = '100%'}
knitr::include_graphics("visuals/frequentist-flavors.png")
```

## Quantifying evidence against a null-model with *p*-values {#ch-03-05-hypothesis-p-values}

All prominent frequentist approaches to statistical hypothesis testing (see Section \@ref(ch-05-01-frequentist-testing-overview)) agree that if empirical observations are sufficiently *un*likely from the point of view of the null hypothesis $H_0$, this should be treated (in some way or other) as evidence *against* the null hypothesis. 
A measure of how unlikely the data is in the light of $H_0$ is the $p$-value.^[For clarity: the $p$-value is actually a measure of how surprising the data is in light of the whole null-model, which is built around the null hypothesis. As the further ingredients in the null-model are usually considered to be relatively uncontroversial (at least relative to the assumption of the null hypothesis), the $p$-value parlance directly targets the null hypothesis.]
To preview the main definition and intuition (to be worked out in detail hereafter), let's first consider a verbal and then a mathematical formulation.

<div class = "mathstuff">

**Definition $p$-value.** The $p$-value associated with observed data $D_\text{obs}$ gives the probability, derived from the assumption that $H_0$ is true, of observing an outcome for the chosen test statistic that is at least as extreme evidence against $H_0$ as the observed outcome.

Formally, the $p$-value of observed data $D_\text{obs}$ is:
$$
p\left(D_{\text{obs}}\right) = P\left(T^{|H_0} \succeq^{H_{0,a}} t\left(D_{\text{obs}}\right)\right)  % = P(\mathcal{D}^{|H_0} \in \{D \mid t(D) \ge t(D_{\text{obs}})\}) 
$$
where $t \colon \mathcal{D} \rightarrow \mathbb{R}$ is a **test statistic** which picks out a relevant summary statistic of each potential data observation, $T^{|H_0}$ is the **sampling distribution**, namely the random variable derived from test statistic $t$ and the assumption that $H_0$ is true, and $\succeq^{H_{0,a}}$ is a linear order on the image of $t$ such that $t(D_1) \succeq^{H_{0,a}} t(D_2)$ expresses that test value $t(D_1)$ is at least as extreme evidence *against* $H_0$ as test value $t(D_2)$ when compared to an alternative hypothesis $H_a$.^[This formulation in terms of a context-dependent, i.e., $H_0$-dependent, ordering is not usual. However, the interpretation *is* de facto context-dependent in this way, and so it makes sense to highlight this aspect of the use of $p$-values also formally. Notice, however, that we can get rid of the context-dependence by using different test statistics. But this is also not how it is done in practice. Essentially, this definition aims for maximal generality so as to cover all cases of use. Since the class of use cases is fuzzy, the definition needs this flexibility. Alternative mathematical definitions that appear to be simpler just do not capture all the use cases.]

</div>

A few aspects of this definition are particularly important (and subsequent text is dedicated to making these aspects more comprehensible):

1. this is a frequentist approach in the sense that probabilities are entirely based on (hypothetical) repetitions of the assumed data-generating process, which assumes that $H_0$ is true;
2. the test statistic *t* plays a fundamental role and should be chosen such that:
    - it must necessarily select exactly those aspects of the data that matter to our research question,
    - it should optimally make it possible to derive a closed-form (approximation) of $T$,^[This latter aspect has been particularly important historically. Given more readily available computing power, alternative approaches based on Monte Carlo simulation of $p$-values can also be used.] and
    - it would be desirable (but not necessary) to formulate $t$ in such a way that the comparison relation $\succeq^{H_{0,a}}$ coincides with a simple comparison of numbers: $t(D_1) \succeq^{H_{0,a}} t(D_2)$ iff $t(D_1) \ge t(D_2)$;
3. there is an assumed data-generating model buried inside notation $T^{|H_0}$; and
4. the notion of "more extreme evidence against $H_0$", captured in comparison relation $\succeq^{H_{0,a}}$ depends on our epistemic purposes, i.e., what research question we are ultimately interested in.^[It is admittedly a bit of a notational overkill to write this comparison relation as a function of $H_0$ and $H_a$ (the alternative hypothesis). Other definitions of the $p$-value do not. But the comparison *is* context-dependent, and you deserve to see this clearly. To see it clearly, a certain heaviness of notation is the price to pay.]

The remainder of this section will elaborate on all of these points. It is important to mention that especially the third aspect (that there is an implicit data-generating model "inside of" classical hypothesis tests) is not something that receives a lot of emphasis in traditional statistics textbooks. Many textbooks do not even mention the assumptions implicit in a given test. Here we will not only stress key assumptions behind a test but present all of the assumptions behind classical tests in a graphical model, similar to what we did for Bayesian models. This arguably makes all implicit assumptions maximally transparent in a concise and lucid representation. It will also help see parallels between Bayesian and frequentist approaches, thereby helping to see both as more of the same rather than as something completely different. In order to cash in this model-based approach, the following sections will therefore introduce new graphical tools to communicate the data-generating model implicit in the classical tests we cover.
  
### Frequentist null-models

We start with the Binomial Model because it is the simplest and perhaps most intuitive case. We work out what a $p$-value is for data for this model and introduce the new graphical language to communicate "frequentist models" in the following. We also introduce the notions of *test statistic* and *sampling distribution* based on a case that should be very intuitive, if not familiar.

The Binomial Model was covered before from a Bayesian point of view, where we represented it using graphical notation like in Figure \@ref(fig:ch-03-04-Binomial-Model-repeated) (repeated from before). Remember that this is a model to draw inferences about a coin's bias $\theta$ based on observations of outcomes of flips of that coin. The Bayesian modeling approach treated the number of observed heads $k$ and the number of flips in total $N$ as given, and the coin's bias parameter $\theta$ as latent.

```{r ch-03-04-Binomial-Model-repeated, echo = F, fig.cap="The Binomial Model (repeated from before) for a Bayesian approach to parameter inference/testing.", out.width = '40%'}
knitr::include_graphics("visuals/binomial-model.png")
```

Actually, this way of writing the Binomial Model is a shortcut. It glosses over each individual data observation (whether the $i$-th coin flip was heads or tails) and jumps directly to the most relevant summary statistic of how many of the $N$ flips were heads. This might, of course, be just the relevant level of analysis. If our assumption is true that the outcome of each coin flip is independent of any other flip, and given our goal to learn something about $\theta$, all that really matters is $k$. But we can also rewrite the Bayesian model from Figure \@ref(fig:ch-03-04-Binomial-Model-repeated) as the equivalent extended model in Figure \@ref(fig:ch-03-04-Binomial-Model-extended). In the latter representation, the individual outcomes of each flip are represented as $x_i \in \{0,1\}$. Each individual outcome is sampled from a [Bernoulli distribution](#app-91-distributions-bernoulli). Based on the whole vector of $x_i$-s and our knowledge of $N$, we derive the **test statistic** $k$, which maps each observation (a vector $x$ of zeros and ones) to a single number $k$ (the number of heads in the vector). Notice that the node for $k$ has a solid double edge, indicating that it follows deterministically from its parent nodes. This is why we can think of $k$ as a sample from a random variable constructed from "raw data" observations $x$.

```{r ch-03-04-Binomial-Model-extended, echo = F, fig.cap="The Binomial Model for a Bayesian approach, extended to show 'raw observations' and the 'summary statistic' implicitly used.", out.width = '60%'}
knitr::include_graphics("visuals/binomial-model-extended.png")
```

Compare this latter representation in Figure \@ref(fig:ch-03-04-Binomial-Model-extended) with the frequentist Binomial Model in Figure \@ref(fig:ch-03-04-Binomial-Model-frequentist). The frequentist model treats the number of observations $N$ as observed, just like the Bayesian model. But it also fixes a specific value for the coin's bias $\theta$. This is where the (point-valued) null hypothesis comes in. For purposes of analysis, we fix the value of the relevant unobservable latent parameter to a specific value (because we do not want to assign probabilities to latent parameters, but we still like to talk about probabilities somehow). In our graphical model in Figure \@ref(fig:ch-03-04-Binomial-Model-frequentist), the node for the coin's bias is shaded (= treated as known) but also has a dotted second edge to indicate that this is where our null hypothesis assumption kicks in. We then treat the data vector $x$ and, with it, the associated test statistic $k$ as unobserved. The data we actually observed will, of course, come in at some point. But the frequentist model leaves the observed data out at first in order to bring in the kinds of probabilities frequentist approaches feel comfortable with: probabilities derived from (hypothetical) repetitions of chance events. So, the frequentist model can now make statements about the likelihood of (raw) data $x$ and values of the derived summary statistic $k$ based on the assumption that the null hypothesis is true. Indeed, for the case at hand, we already know that the **sampling distribution**, i.e., the distribution of values for $k$ given $\theta_0$ is the [Binomial distribution](#app-91-distributions-binomial).


```{r ch-03-04-Binomial-Model-frequentist, echo = F, fig.cap="The Binomial Model for a frequentist binomial test.", out.width = '80%'}
knitr::include_graphics("visuals/binomial-model-frequentist.png")
```

Let's take a step back. The frequentist model for the binomial case considers ("raw") data of the form $\langle x_1, \dots, x_N \rangle$ where each $x_i \in \{0,1\}$ indicates whether the $i$-th flip was a success (= heads, = 1) or a failure (= tails, = 0). We identify the set of all binary vectors of length $N$ as the set of hypothetical data that we could, in principle, observe in a fictitious repetition of this data-generating process. $\mathcal{D}^{|H_0}$ is then the random variable that assigns each potential observation $D = \langle x_1, \dots, x_N \rangle$ the probability with which it would occur if $H_0$ (= a specific value of $\theta$) is true. In our case, that is:

$$P(\mathcal{D}^{|H_0} = \langle x_1, \dots, x_N \rangle) = \prod_{i=1}^N \text{Bernoulli}(x_i, \theta_0)$$

The model does not work with this raw data and its implied distribution (represented by random variable $\mathcal{D}^{|H_0}$), it instead uses a (very natural!) **test statistic** $t \colon \langle x_1, \dots, x_N \rangle \mapsto \sum_{i=1}^N x_i$. The **sampling distribution** for this model is therefore the distribution of values for the derived measure $k$ - a distribution that follows from the distribution of the raw data ($\mathcal{D}^{|H_0}$) and this particular test statistic $t$. In its most general form, we write the sampling distribution as $T^{|H_0} = t(\mathcal{D^{H_0}})$. ^[Most often, the random variable capturing the sampling distribution is just written as $T$, but it does make sense to stress also notationally that $T$ depends crucially on $H_0$.] It just so happens (what a relief!) that we know how to express $T^{|H_0}$ in a mathematically very concise fashion. It's just the Binomial distribution, so that $k \sim \text{Binomial}(\theta_0, N)$. (Notice how the sampling distribution is really a function of $\theta_0$, i.e., the null hypothesis, and also of $N$.)


### One- vs. two-sided $p$-values

After seeing a frequentist null model and learning about notions like "test statistic" and "sampling distribution", let's explore what a $p$-value is based on the frequentist Binomial Model. Our running example will be the 24/7 case, where $N = 24$ and $k = 7$. Notice that we are glossing over the "raw" data immediately and work with the value of the test statistic of the observed data directly: $t(D_{\text{obs}}) = 7$.

Remember that, by the definition given above, $p(D_{\text{obs}})$ is the probability of observing a value of the test statistic that is at least as extreme evidence against $H_0$ as $t(D_{\text{obs}})$, under the assumption that $H_0$ is true:

$$
  p(D_{\text{obs}}) = P(T^{|H_0} \succeq^{H_{0,a}} t(D_{\text{obs}})) % = P(\mathcal{D}^{|H_0} \in \{D \mid t(D) \ge t(D_{\text{obs}})\}) 
$$

To fill this with life, we need to set a null hypothesis, i.e., a value $\theta_0$ of coin bias $\theta$, that we would like to collect evidence *against*. A fixed $H_0$ will directly fix $T^{|H_0}$, but we will have to put extra thought into how to conceptualize $\succeq^{H_{0,a}}$ for any given $H_0$. To make exactly this clearer is the job of this section. Specifically, we will look at what is standardly called a **two-sided $p$-value** and a **one-sided $p$-value**.
The difference lies in whether we are testing a point-valued or an interval-based null hypothesis.
So, let's suppose that we want to test the following null hypotheses:

- Is the coin fair ($\theta = 0.5$)?
- Is the coin biased towards heads ($\theta > 0.5$)?

In the case of testing for fairness  ($\theta = 0.5$), the pair of null hypothesis and alternative hypothesis are:

$$
\begin{aligned}
H_0 \colon \theta = 0.5 && H_a \colon \theta \neq 0.5
\end{aligned}
$$

The case for testing the null hypothesis $\theta > 0.5$ is slightly more convoluted.
The frequentist construction of a null model strictly requires point-valued assumptions about all model parameters.
Otherwise, subjective priors would sneak it.
(NB: Even the assumption of equal probability of parameter values, as in a non-informative prior, *is* a biased and subjective assumption, according to frequentism.)
We therefore actually test the point-valued null hypothesis $\theta = 0.5$, but we contrast it with a different  alternative hypothesis, which is now one-sided:

$$
\begin{aligned}
H_0 \colon \theta = 0.5 && H_a \colon \theta < 0.5
\end{aligned}
$$

**Case $\theta = 0.5$.** To begin with, assume that we want to address the question of whether the coin is fair.
Figure \@ref(fig:ch-03-04-testing-binomial-sampling-distribution) shows the sampling distribution of the test statistic $k$.
The probability of the observed value of the sampling statistic is shown in red. 

```{r ch-03-04-testing-binomial-sampling-distribution, echo = F, fig.cap = "Sampling distribution (here: Binomial distribution) and the probability associated with observed data $k=7$ highlighted in red, for $N = 24$ coin flips, under the assumption of a null hypothesis $\\theta = 0.5$."}
plotData = data.frame(x = 0:24, y = dbinom(0:24, 24, 0.5))
plotData2 = data.frame(x = 7, y = dbinom(7, 24, 0.5))
ggplot(plotData, aes(x = x , y = y )) + geom_bar(stat = "identity", fill = "gray", width = 0.35) +
  geom_bar(data = plotData2, aes(x = x, y = y), stat = "identity", fill = "firebrick", width = 0.35) +
  labs(
    x = latex2exp::TeX("test statistic $k$"),
    y = latex2exp::TeX("$Binomial(k \\, | \\, n = 24, \\theta = 0.5)$")
  )
```

The question we need to settle to obtain a $p$-value is how to interpret $\succeq^{H_{0,a}}$ for this case.
To do this, we need to decide which alternative values of $k$ would count as equally or more extreme evidence *against* the chosen null hypothesis when compared to the specified alternative hypothesis.
The obvious approach is to use the probability of any value of the test statistic $k$ directly and say that observing $D_1$ counts as at least as extreme evidence against $H_0$ as observing $D_2$, $t(D_1) \succeq^{H_{0,a}} t(D_2)$, iff the probability of observing the test statistic associated with $D_1$ is at least as unlikely as observing $D_2$: $P(T^{|H_0} = t(D_1)) \le P(T^{|H_0} = t(D_2))$. To calculate the $p$-value in this way, we therefore need to sum up the probabilities of all values $k$ under the Binomial distribution (with parameters $N=24$ and $\theta = \theta_0 = 0.5$) that are no larger than the value of the observed $k = 7$. In mathematical language:^[Here, the bracket notation $[ \mathit{Boolean} ]$ is the [Iverson bracket](https://en.wikipedia.org/wiki/Iverson_bracket), evaluating to 1 if the Boolean expression is true and to 0 otherwise.]

$$
p(k) = \sum_{k' = 0}^{N} [\text{Binomial}(k', N, \theta_0) <= \text{Binomial}(k, N, \theta_0)] \ \text{Binomial}(k', N, \theta_0)
$$

In code, we calculate this $p$-value as follows:

```{r}
# exact p-value for k = 7 with N = 24 and null hypothesis theta = 0.5
k_obs <- 7
N <- 24
theta_0 <- 0.5
tibble( lh = dbinom(0:N, N, theta_0) ) %>% 
  filter( lh <=  dbinom(k_obs, N, theta_0) ) %>% 
  pull(lh) %>% sum %>% round(5)
```

Figure \@ref(fig:ch-03-04-testing-binomial-p-value) shows the values that need to be summed over in red.

```{r ch-03-04-testing-binomial-p-value, echo = F, fig.cap = "Sampling distribution (Binomial likelihood function) and two-sided $p$-value for the observation of $k=7$ successes in $N = 24$ coin flips, under the assumption of a null hypothesis $\\theta = 0.5$."}
plotData = data.frame(x = 0:24, y = dbinom(0:24, 24, 0.5))
plotData2 = data.frame(x = c(0:7, 17:24), y = dbinom(c(0:7, 17:24), 24, 0.5))
ggplot(plotData, aes(x = x , y = y )) + geom_bar(stat = "identity", fill = "gray", width = 0.35) +
  geom_bar(data = plotData2, aes(x = x, y = y), stat = "identity", fill = "firebrick", width = 0.35) +
  geom_hline(yintercept=dbinom(7,24,0.5)) + 
  # geom_text(data.frame(x = 3, y = 0.05, label = paste0("p = " , round(1-sum(dbinom(8:16, 24, 0.5)),3), collapse = "")), aes(x = x, y = y, label = label))
  geom_label(x = 3, y = 0.03, label = paste0("p(k=7) = " , round(1-sum(dbinom(8:16, 24, 0.5)),5), collapse = "")) +
  labs(
    x = latex2exp::TeX("test statistic $k$"),
    y = latex2exp::TeX("$Binomial(k \\, | \\, n = 24, \\theta = 0.5)$")
  )
```


Of course, R also has a built-in function for a Binomial test. We can use it to verify that we get the same result for the $p$-value:

```{r}
binom.test(
  x = 7,     # observed successes
  n = 24,    # total no. of observations
  p = 0.5    # null hypothesis
)
```

<div class = "exercises">
**Exercise 16.1: Output of R's `binom.test`**

Look at the output of the above call to R's `binom.test` function. 
Which pieces of information in that output make sense to you (given your current knowledge) and which do not? 

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

The output given first states what was computed, namely an *exact binomial test*.
You should understand what a *binomial test* is.
The additional adjective *exact* refers to the fact that we did not use any approximation to get at the shown $p$-value.
Next, we see the data repeated and the calculated $p$-value, which we have seen how to calculate by hand.
The output then also names the alternative hypothesis, just like the text previously explained, making clear that this is a two-valued $p$-value.
Then comes something which you do not yet know about: the notion of a 95% confidence interval will be covered later in this chapter.
Finally, the output also gives the maximum likelihood estimate of the theta parameter.
Together with the 95% confidence interval, the test result therefore also reports the most common frequentist estimators, point-values (MLE) and interval-valued (95% confidence interval) for the parameter of interest.
   
</div>
</div>
</div>

**Case $\theta > 0.5$.** Let's now look at the case where we want to test whether the coin is biased towards heads $\theta > 0.5$. 
As explained above, we need a point-valued assumption for the coin bias $\theta$ to set up a frequentist model and retrieve a sampling distribution for the relevant test statistic.
We choose $\theta_{0} = 0.5$ as the point-valued null hypothesis, because *if* we get a high measure of the evidence against the hypothesis $\theta_{0} = 0.5$ (in a comparison against the alternative $\theta < 0.5$), we can discredit the whole interval-based hypothesis $\theta > 0.5$ because any other value of $\theta$ bigger than 0.5 would give at least as high a $p$-value.
In other words, we pick the single value for the comparison which is most favorable *for* the hypothesis $\theta > 0.5$ when compared against $\theta < 0.5$, so as when *even that* value is discredited, the whole hypothesis $\theta > 0.5$ is discredited.

But even though we use the same null-value of $\theta_0 = 0.5$, the calculation of the $p$-value will be different from the case we looked at previously. 
It will be one-sided.
The reason lies in a change to what we should consider more extreme evidence against this interval-valued null hypothesis, i.e., the interpretation of $\succeq^{H_{0,a}}$. 
Look at Figure \@ref(fig:ch-03-04-testing-binomial-p-value-one-sided).
As before we see the Bernoulli likelihood function derived from the point-value null hypothesis.
The $k$-value observed is $k=7$.
Again we need to ask: which values of $k$ would constitute equal or more evidence against the null hypothesis *when compared against the alternative hypothesis, which is now $\theta < 0.5$*
Unlike in the previous, two-sided case, observing large values of $k$, e.g., larger than 12, even if they are unlikely for the point-valued hypothesis $\theta_0 = 0.5$, does not constitute evidence against the interval-valued hypothesis we are interested in.
So therefore, we disregard the contribution of the right-hand side in Figure \@ref(fig:ch-03-04-testing-binomial-p-value) to arrive at a picture like in Figure \@ref(fig:ch-03-04-testing-binomial-p-value-one-sided). 

```{r ch-03-04-testing-binomial-p-value-one-sided, echo = F, fig.cap = "Sampling distribution (Binomial likelihood function) and one-sided $p$-value for the observation of $k=7$ successes in $N = 24$ coin flips, under the assumption of a null hypothesis $\\theta = 0.5$ compared against the alternative hypothesis $\\theta < 0$."}
plotData = data.frame(x = 0:24, y = dbinom(0:24, 24, 0.5))
plotData2 = data.frame(x = c(0:7), y = dbinom(c(0:7), 24, 0.5))
ggplot(plotData, aes(x = x , y = y )) + geom_bar(stat = "identity", fill = "gray", width = 0.35) +
  geom_bar(data = plotData2, aes(x = x, y = y), stat = "identity", fill = "firebrick", width = 0.35) +
  geom_hline(yintercept=dbinom(7,24,0.5)) + 
  # geom_text(data.frame(x = 3, y = 0.05, label = paste0("p = " , round(1-sum(dbinom(8:16, 24, 0.5)),3), collapse = "")), aes(x = x, y = y, label = label))
  geom_label(x = 3, y = 0.03, label = paste0("p(k=7) = " , round(sum(dbinom(0:7, 24, 0.5)),5), collapse = "")) +
  labs(
    x = latex2exp::TeX("test statistic $k$"),
    y = latex2exp::TeX("$Binomial(k \\, | \\, n = 24, \\theta = 0.5)$")
  )
```

The associated $p$-value with this so-called **one-sided test** is consequently:

```{r}
k_obs <- 7
N <- 24
theta_0 <- 0.5
# exact p-value for k = 7 with N = 24 and null hypothesis theta > 0.5
dbinom(0:k_obs, N, theta_0) %>% sum %>% round(5)
```

We can double-check against the built-in function `binom.test` when we ask for a one-sided test:

```{r}
binom.test(
  x = 7,     # observed successes
  n = 24,    # total no. of observations
  p = 0.5,    # null hypothesis
  alternative = "less" # the alternative to compare against is theta < 0.5
)
```

### Significance & categorical decisions

Fisher's early writings suggest that he considered $p$-values as quantitative measures of strength of evidence against the null hypothesis. 
What would need to be done or concluded from such a quantitative measure would need to depend on further careful case-by-base deliberation. 
In contrast, the Neyman-Pearson approach, as well as the presently practiced hybrid NHST approach use $p$-values to check, in a rigid conventionalized manner, whether a test result is noteworthy in a categorical, not quantitative way.
More on the Neyman-Pearson approach in Section \@ref(ch-03-04-hypothesis-significance-errors).

Fixing an $\alpha$-level of significance (with common values $\alpha \in \{0.05, 0.01, 0.001\}$), we say that a test result is **statistically significant** (at level $\alpha$) if the $p$-value of the observed data is lower than the specified $\alpha$.
The significance of a test result, as a categorical measure, can then be further interpreted as a trigger for decision making. 
Commonly, a significant test result is interpreted as the signal to reject the null hypothesis, i.e., to speak and act as if it was false. 
Importantly, a non-significant test results by some $\alpha$-level is *not* to be treated as evidence in favor of the null hypothesis.^[Frequentist hypothesis testing is superficially similar to Popperian falsificationism. It is, however, quite the opposite when looked at more carefully. Popper famously denied that empirical observation could constitute positive evidence in favor of a research hypothesis. Research hypotheses can only be refuted, viz., when their logical consequences are logically incompatible with the observed data. In a Popperian science, what is refuted are research hypotheses; frequentist statistics instead seeks to refute null hypotheses and counts successful refutation of a null hypothesis as evidence in favor of a research hypothesis.]

<div class = "exercises">
**Exercise 16.2: Significance & errors**

If the $p$-value is larger than a *prespecified* significance threshold $\alpha$ (e.g., $\alpha = 0.05$), we...

a. ...accept $H_0$.
b. ...reject $H_0$ in favor of $H_a$.
c. ...fail to reject $H_0$.

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

Statement c. is correct.

</div>
</div>
</div>

### How (not) to interpret *p*-values

Though central to much of frequentist statistics, $p$-values are frequently misinterpreted, even by seasoned scientists [@HallerKrauss2002:Misinterpretati]. To repeat, the $p$-value measures the probability of observing, if the null hypothesis is correct, a value of the test statistic that is (in a specific, contextually specified sense) more extreme than the value of the test statistic that we assign to the observed data. We can therefore treat $p$-values as a measure of evidence *against* the null hypothesis. And if we want to be even more precise, we interpret this as evidence against the whole assumed data-generating process, a central part of which is the null hypothesis.

The $p$-value is *not* a statement about the probability of the null hypothesis given the data. So, it is *not* something like $P(H_0 \mid D)$. The latter is a very appealing notion, but it is one that the frequentist denies herself access to. It can also only be computed based on some consideration of prior plausibility of $H_0$ in relation to some alternative hypothesis. Indeed, to calculate $P(H_0 \mid D)$ is unforgivingly a subjective, Bayesian notion.

<!-- exercise 1 -->
<div class = "exercises">
**Exercise 16.3: $p$-values**

1. Which statement(s) about $p$-values is/are true?

The $p$-value is...

a. ...the probability that the null hypothesis $H_0$ is true.
b. ...the probability that the alternative hypothesis $H_a$ is true.
c. ...the probability, derived from the assumption that $H_0$ is true, of obtaining an outcome for the chosen test statistic that is the exact same as the observed outcome.
d. ...a measure of evidence in favor of $H_0$.
e. ...the probability, derived from the assumption that $H_0$ is true, of obtaining an outcome for the chosen test statistic that is the same as the observed outcome or more extreme evidence for $H_a$.
f. ...a measure of evidence against $H_0$.

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

Statements e. and f. are correct. 
   
</div>
</div>
   
   
</div>

### [Excursion] Distribution of $p$-values

A result that might seem surprising at first is that if the null hypothesis is true, the distribution of $p$-values is uniform. This, however, is intuitive on second thought. Mathematically it is a direct consequence of the **Probability Integral Transform Theorem**.

<div class = "mathstuff">

```{theorem, label = "Probability-Integral-Transform", name = "Probability Integral Transform"}
If $X$ is a continuous random variable with cumulative distribution function $F_X$, the random variable $Y = F_X(X)$ is uniformly distributed over the interval $[0;1]$, i.e., $y \sim \text{Uniform}(0,1)$. 
  
```

<div class="collapsibleProof">
<button class="trigger">Proof</button>
<div class="content">


```{proof}
Notice that the cumulative density function of a standard uniform distribution $y \sim \text{Uniform}(0,1)$ is a linear line with intercept 0 and slope 1. It therefore suffices to show that $F_Y(y) = y$.
$$
\begin{aligned}
F_Y(y) & = P(Y \le y)  && [\text{def. of cumulative distribution}] \\
 & = P(F_X(X) \le y)  && [\text{by construction / assumption}] \\
 & = P(X \le F^{-1}_X(y))  && [\text{applying inverse cumulative function}] \\
 & = F_X(F^{-1}_X(y))  && [\text{def. of cumulative distribution}] \\
 & = y  && [\text{inverses cancel out}] \\
\end{aligned}
$$
```


&nbsp;

</div>
</div>

</div>

Seeing the uniform distribution of $p$-values (under a true null hypothesis) helps appreciate how the $\alpha$-level of significance is related to long-term error control. If the null hypothesis is true, the probability of a significant test result is exactly the significance level. 




## [Excursion] Central Limit Theorem {#ch-03-05-hypothesis-testing-CLT}

The previous sections expanded on the notion of a $p$-value and showed how to calculate $p$-values for different kinds of research questions for data from repeated Bernoulli trials (= coin flips). 
We saw that a natural test statistic is the Binomial distribution. The Binomial distribution described the sampling distribution precisely, i.e., the sampling distribution for the frequentist Binomial Model as we set it up *is* the Binomial distribution. Unfortunately, there are models and types of data for which the sampling distribution is not known precisely. In these cases, frequentist statistics work with approximations to the true sampling distribution. These approximations get better the more data was observed, i.e., these are limit-approximations that hold in the limit when the amount of data observed goes towards infinity. For small samples, the error might be substantial. Rules of thumb have become conventional guides for judging when (not) to use a given approximation. Which (approximation for a) sampling distribution to use needs to be decided on a case-by-case basis.

To establish that a particular distribution is a good approximation of the true sampling distribution, the most important formal result is the *Central Limit Theorem* (CLT). In rough terms, the CLT says that, under certain conditions, we can use a normal distribution as an approximation of the sampling distribution. 

To appreciate the CLT, let's start with another seminal result, the **Law of Large Numbers**, which we had already relied on when we discussed a sample-based approach to representing probability distributions. For example, the Law of Large Numbers justifies why taking (large) samples from a random variable sufficiently approximate a mean (the most prominent Bayesian point-estimator of, e.g., a posterior approximated by samples from MCMC algorithms).

<div class = "mathstuff">

```{theorem label = "Law-of-Large-Numbers", name = "Law of Large Numbers"}
Let $X_1, \dots, X_n$ be a sequence of $n$ differentiable random variables with equal mean, such that $\mathbb{E}_{X_i} = \mu_X$ for all $1 \le i \le n$.^[Though the result is more general, it is convenient to think of a natural application as the case where all $X_i$ are samples from the exact same distribution.] As the number of samples $n$ goes to infinity, the mean of any tuple of samples, one from each $X_i$, convergences almost surely to $\mu_X$:
  
$$ P \left(\lim_{n \rightarrow \infty} \frac{1}{n} \sum_{i = 1}^n X_i = \mu_X \right) = 1 $$
```

</div>

Computer simulations make the point and usefulness of this fact easier to appreciate:

```{r}
# sample from a standard normal distribution (mean = 0, sd = 1)
samples <- rnorm(100000)
# collect the mean after each 10 samples & plot
tibble(
  n = seq(100, length(samples), by = 10)
  ) %>% 
  group_by(n) %>% 
  mutate(
  mu = mean(samples[1:n])
  ) %>% 
  ggplot(aes(x = n, y = mu)) +
  geom_line()
```


For practical purposes, think of the Central Limit Theorem as an extension of the Law of Large Numbers. While the latter tells us that, as $n \rightarrow \infty$, the mean of repeated samples from a random variable $X$ converges to the mean of $X$, the Central Limit Theorem tells us something about the distribution of our estimate of $X$'s mean. The Central Limit Theorem tells us that the sampling distribution of the mean approximates a normal distribution for a large enough sample size.

<div class = "mathstuff">

```{theorem, label = "Central-Limit-Theorem", name = "Central Limit Theorem"}
Let $X_1, \dots, X_n$ be a sequence of $n$ differentiable random variables with equal mean  $\mathbb{E}_{X_i} = \mu_X$ and equal finite variance $\text{Var}(X_i) = \sigma_X^2$ for all $1 \le i \le n$.^[As with the Law of Large Numbers, the most common application is the case where all $X_i$ are samples from the exact same distribution.] The random variable $S_n$ which captures the distribution of the sample mean for any $n$ is:
$$ S_n = \frac{1}{n} \sum_{i=1}^n X_i $$
As the number of samples $n$ goes to infinity, the random variable $\sqrt{n} (S_n - \mu_X)$ converges in distribution to a normal distribution with mean 0 and standard deviation $\sigma_X$.
  
```

</div>

&nbsp;

A proof of the CLT is not trivial, and we will omit it here. We will only point to the CLT when justifying approximations of sampling distributions, e.g., for the case of Pearson's $\chi^2$-test.

Below you can explore the effect of different sample sizes and numbers of samples on the sampling distribution of the mean. Play around with the values and note how with increasing sample size and number of samples...

1. ...the sample mean approximates the population mean (Law of Large Numbers).
2. ...the distribution of sample means approximates a normal distribution (Central Limit Theorem).

To be able to simulate the CLT, we first need a population to sample from. In the drop-down menu below, you can choose how the population should be distributed, where the parameter values are fixed (e.g., if you choose "normally distributed", the population will be distributed according to $N(\mu = 4, \sigma = 1)$).^[If you want to change the default parameter values, simply click on the yellowish box and change the respective variable value. If you would like to see how different parameter values generally affect a given distribution, you should take a look at [Appendix B](#app-91-distributions).] Also try out the custom option to appreciate that both concepts hold for *every* distribution.

In variable `sample_size`, you can specify how many samples you want to take from the population. `number_of_samples` denotes how many samples of size `sample_size` are taken. E.g., if `number_of_samples = 5` and `sample_size = 3`, we would repeat the process of taking three samples from the population a total of five times. The output will show the population and sampling distribution with their means.

<select onchange="selectDist()" id=select>
  <option value="" selected disabled hidden>Population distribution</option>
  <option>normally distributed</option>
  <option>uniformly distributed</option>
  <option>beta distributed</option>
  <option>binomially distributed</option>
  <option>custom</option>
</select>

<p id="empty"></p>

<pre class=" CodeMirror-line " role="presentation"></pre>

<script>
function selectDist() {
  var dist = document.getElementById("select");

  if (dist.options[dist.selectedIndex].text == "normally distributed") {
    document.getElementById("empty").innerHTML = "<pre class='gaussian'>var sample_size = 3;\nvar number_of_samples = 5;\n///fold:\n\n// Population distribution\n\nvar mu = 4; // mean\nvar sigma = 1; // standard deviation\nvar n_samples = 10000; // number of samples used for approximation\n\nvar population = repeat(n_samples, function(x) {gaussian({mu: mu, sigma: sigma})});\nvar population_mean = sum(population)/n_samples;\ndisplay('Population distribution \\nPopulation mean = ' + population_mean);\nviz(population);\n\n// Sampling distribution\n\nvar random_sample = function(population, sample_size) {\n  if(sample_size == 1) {\n    var sample = [_.sample(population)];\n    return sample;\n  } else {\n    var sample = [_.sample(population)];\n    return sample.concat(random_sample(population, sample_size-1));\n  }\n}\n\nvar compute_mean = function(population, sample_size) {\n  var sample = random_sample(population, sample_size);\n  var sample_mean = sum(sample)/sample_size;\n  return sample_mean;\n}\n\nvar sampling_dist = repeat(number_of_samples, function(x){compute_mean(population, sample_size)});\nvar sampling_dist_mean = sum(sampling_dist)/number_of_samples;\ndisplay('Sampling distribution of the sample mean \\nSample mean = ' + sampling_dist_mean);\nviz.hist(sampling_dist);\n///</pre>";
    var type = 'gaussian';

  } else if (dist.options[dist.selectedIndex].text == "uniformly distributed") {
    document.getElementById("empty").innerHTML = "<pre class='uniform'>var sample_size = 2;\nvar number_of_samples = 6;\n///fold:\n\n// Population distribution\n\nvar a = 1; // lower bound\nvar b = 3; // upper bound (> a)\nvar n_samples = 10000; // number of samples used for approximation\n\nvar population = repeat(n_samples, function(x) {uniform({a: a, b: b})});\nvar population_mean = sum(population)/n_samples;\ndisplay('Population distribution \\nPopulation mean = ' + population_mean);\nviz(population);\n\n// Sampling distribution\n\nvar random_sample = function(population, sample_size) {\n  if(sample_size == 1) {\n    var sample = [_.sample(population)];\n    return sample;\n  } else {\n    var sample = [_.sample(population)];\n    return sample.concat(random_sample(population, sample_size-1));\n  }\n}\n\nvar compute_mean = function(population, sample_size) {\n  var sample = random_sample(population, sample_size);\n  var sample_mean = sum(sample)/sample_size;\n  return sample_mean;\n}\n\nvar sampling_dist = repeat(number_of_samples, function(x){compute_mean(population, sample_size)});\nvar sampling_dist_mean = sum(sampling_dist)/number_of_samples;\ndisplay('Sampling distribution of the sample mean \\nSample mean = ' + sampling_dist_mean);\nviz.hist(sampling_dist);\n///</pre>";
    var type = 'uniform';

  } else if (dist.options[dist.selectedIndex].text == "beta distributed") {
    document.getElementById("empty").innerHTML = "<pre class='beta'>var sample_size = 3;\nvar number_of_samples = 3;\n///fold:\n\n// Population distribution\n\nvar a = 4; // shape parameter alpha\nvar b = 2; // shape parameter beta\nvar n_samples = 10000; // number of samples used for approximation\n\nvar population = repeat(n_samples, function(x) {beta({a: a, b: b})});\nvar population_mean = sum(population)/n_samples;\ndisplay('Population distribution \\nPopulation mean = ' + population_mean);\nviz(population);\n\n// Sampling distribution\n\nvar random_sample = function(population, sample_size) {\n  if(sample_size == 1) {\n    var sample = [_.sample(population)];\n    return sample;\n  } else {\n    var sample = [_.sample(population)];\n    return sample.concat(random_sample(population, sample_size-1));\n  }\n}\n\nvar compute_mean = function(population, sample_size) {\n  var sample = random_sample(population, sample_size);\n  var sample_mean = sum(sample)/sample_size;\n  return sample_mean;\n}\n\nvar sampling_dist = repeat(number_of_samples, function(x){compute_mean(population, sample_size)});\nvar sampling_dist_mean = sum(sampling_dist)/number_of_samples;\ndisplay('Sampling distribution of the sample mean \\nSample mean = ' + sampling_dist_mean);\nviz.hist(sampling_dist);\n///</pre>";
    var type = 'beta';

  } else if (dist.options[dist.selectedIndex].text == "binomially distributed") {
    document.getElementById("empty").innerHTML = "<pre class='binomial'>var sample_size = 6;\nvar number_of_samples = 4;\n///fold:\n\n// Population distribution\n\nvar p = 0.5; // probability of success\nvar n = 10; // number of trials (>= 1)\nvar n_samples = 10000; // number of samples used for approximation\n\nvar population = repeat(n_samples, function(x) {binomial({p: p, n: n})});\nvar population_mean = sum(population)/n_samples;\ndisplay('Population distribution \\nPopulation mean = ' + population_mean);\nviz(population);\n\n// Sampling distribution\n\nvar random_sample = function(population, sample_size) {\n  if(sample_size == 1) {\n    var sample = [_.sample(population)];\n    return sample;\n  } else {\n    var sample = [_.sample(population)];\n    return sample.concat(random_sample(population, sample_size-1));\n  }\n}\n\nvar compute_mean = function(population, sample_size) {\n  var sample = random_sample(population, sample_size);\n  var sample_mean = sum(sample)/sample_size;\n  return sample_mean;\n}\n\nvar sampling_dist = repeat(number_of_samples, function(x){compute_mean(population, sample_size)});\nvar sampling_dist_mean = sum(sampling_dist)/number_of_samples;\ndisplay('Sampling distribution of the sample mean \\nSample mean = ' + sampling_dist_mean);\nviz.hist(sampling_dist);\n///</pre>";
    var type = 'binomial';
    
  } else if (dist.options[dist.selectedIndex].text == "custom") {
    document.getElementById("empty").innerHTML = "<pre class='custom'>var sample_size = 10;\nvar number_of_samples = 10;\nvar population = [1, 2, 4, 5, 5, 3, 7, 5, 3];\n///fold:\n\n// Population distribution\n\nvar population_mean = sum(population)/population.length;\ndisplay('Population distribution \\nPopulation mean = ' + population_mean);\nviz(population);\n\n// Sampling distribution\n\nvar random_sample = function(population, sample_size) {\n  if(sample_size == 1) {\n    var sample = [_.sample(population)];\n    return sample;\n  } else {\n    var sample = [_.sample(population)];\n    return sample.concat(random_sample(population, sample_size-1));\n  }\n}\n\nvar compute_mean = function(population, sample_size) {\n  var sample = random_sample(population, sample_size);\n  var sample_mean = sum(sample)/sample_size;\n  return sample_mean;\n}\n\nvar sampling_dist = repeat(number_of_samples, function(x){compute_mean(population, sample_size)});\nvar sampling_dist_mean = sum(sampling_dist)/number_of_samples;\ndisplay('Sampling distribution of the sample mean \\nSample mean = ' + sampling_dist_mean);\nviz.hist(sampling_dist);\n///</pre>";
    var type = 'custom';

  }
  
  // find all <pre> elements and set up the editor on them
  var preEls = Array.prototype.slice.call(document.getElementsByClassName(type));
  preEls.map(function(el) { console.log(el); editor.setup(el, {language: 'webppl'}); });  
}
</script>


## [Excursion] The Neyman-Pearson approach {#ch-03-04-hypothesis-significance-errors}

Neyman and Pearson criticized Fisher's approach for not being able to deliver results that could lead to accepting the null hypothesis.
Moreover, the Neynam-Pearson approach is motivated by establishing a tight regime of long-term error control: we want to keep a cap on the long-run amount of errors that we make in statistical decision making.
To do so, the N-P approach requires that researchers specify not only a null hypothesis $H_0$, but also a point-valued alternative hypothesis $H_a$, which are pitted against each other (similar to what we would expect from model comparison).
The point-value for $H_a$ usually comes from previous research or is chosen in a strategic manner.

If we have a pair of explicit, point-valued hypotheses, null and alternative, we can do more than just to reject or not reject the null hypothesis, so the N-P approach proposes.
We can reject the null hypothesis, in which case we accept the alternative hypothesis, or we can accept the null hypothesis, in which case we reject the alternative hypothesis.
Being entirely frequentist, the N-P approach makes this binary decision between competing hypotheses based on $p$-values derived from the null hypothesis, as before.
As we will see, this binary accept/reject logic is only tight if we make sure that a particular kind of error, the $\beta$-error, is low enough; which we do, essentially, by making sure that we have enough data.

If we have two hypotheses $H_0$ and $H_a$, and accept/reject them in a binary fashion, there are two types of error relevant to these considerations: 

1. the $\alpha$ or, Type-I error, is the error of falsely rejecting the null hypothesis, when in fact it is true.
2. the $\beta$ error, or Type-II error, is the error of falsely accepting the null hypothesis, when in fact the alternative hypothesis is true.

Figure \@ref(fig:chap-05-01-freq-testing-alpha-beta-error) visualizes these two error concepts.
The curves show the sampling distributions of a relevant test statistic under the assumption that $H_0$ is true (green) or that $H_a$ is true (yellow).
If we fix a significance level $\alpha$ for a $p$-value based test -as before-, the proportion of error we make in the case that $H_0$ is true, is given by $\alpha$ itself.
This is shown for a one-sided test in Figure  \@ref(fig:chap-05-01-freq-testing-alpha-beta-error), where $t^*$ is the critical value of the test statistic above which we get a significant result at $\alpha$ level.
But now assume that, by the new N-P decision rule, whenever our test result is not significant, i.e., we get a result for the test statistic below $t^*$, we would accept the null hypothesis and reject the alternative hypothesis $H_a$.
Then, the amount of errors we make, under the assumption that $H_a$ is actually true is given by $\beta$.

```{r chap-05-01-freq-testing-alpha-beta-error, echo = F, fig.cap="Schematic representation of $\\alpha$- and $\\beta$-errors. The green curve is the sampling distribution of some test statistic under the assumption that the null hypothesis is true. The yellow curve is the sampling distribution for the alternative hypothesis. The decision criterion for an N-P test is indicated as $t^*$. The shaded regions show the probabilities of falsely rejecting a true null hypothesis (in green) and that of false accepting a false alternative hypothesis."}
plot_data <- tibble(
  x = seq(-8, 12, length.out = 1000),
  null = dnorm(x, sd = 2),
  alternative = dnorm(x, mean = 4, sd = 2)
) %>% 
  pivot_longer(cols = 2:3)

plot_data %>% 
  ggplot(aes(x = x, y = value, color = name)) + 
    # null hypothesis segment & label
  geom_segment(
    aes(
      x = 0,
      xend = 0,
      y = -0.05,
      yend = 0.01
    ),
    color = "darkgray"
  ) + 
  geom_label(
    aes(
      x = 0,
      y = -0.055,
      label = "H_0"
    ),
    color = "darkgray",
    label.size = 0,
    size = 6
  ) +
  # alternative hypothesis segment & label
  geom_segment(
    aes(
      x =4,
      xend = 4,
      y = -0.05,
      yend = 0.01
    ),
    color = "darkgray"
  ) + 
  geom_label(
    aes(
      x = 4,
      y = -0.055,
      label = "H_a"
    ),
    color = "darkgray",
    label.size = 0,
    size = 6
  ) +
  # critical value segment and label
  geom_segment(
    aes(
      x = qnorm(0.95, sd = 2),
      xend = qnorm(0.95, sd = 2),
      y = -0.02,
      yend = 0.01
    ),
    color = "darkgray"
  ) + 
  geom_label(
    aes(
      x = qnorm(0.95, sd = 2),
      y = -0.025,
      label = "t*"
    ),
    color = "darkgray",
    label.size = 0,
    size = 6
  ) +
  # alpha error area
  geom_area(
    data = filter(
      plot_data, name == "null", 
      x >= qnorm(0.95, sd = 2)
    ), 
    fill = project_colors[2],
    color = project_colors[2],
    alpha = 0.3
  ) +
  # beta error area
  geom_area(
    data = filter(
      plot_data, name == "alternative", 
      x <= qnorm(0.95, sd = 2)
    ), 
    fill = project_colors[1],
    color = project_colors[1],
    alpha = 0.3
  ) +
  # main density lines
  geom_line(size = 2) +
  # alpha error explanation
  annotate(
    geom='text', 
    x=10, 
    y=0.1, 
    label=TeX("$\\alpha$-error", output='character'), 
    parse=TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = 4.3,
      xend = 10,
      y = 0.005,
      yend = 0.09
    ),
    color = "black"
  ) +
  # beta error explanation
  annotate(
    geom='text', 
    x=-6, 
    y=0.1, 
    label=TeX("$\\beta$-error", output='character'), 
    parse=TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = -0.3,
      xend = -6,
      y = 0.005,
      yend = 0.09
    ),
    color = "black"
  ) +
  # general cosmetics
  guides(color = F) +
  labs(x = "", y = "") +
  theme(
      axis.text = element_blank(),
      axis.ticks = element_blank()
    )
   
  
```

For this N-P logic of rejecting *and* accepting the null hypothesis to work, we would therefore need to make sure that $\beta$ is low enough.
We don't want to make that kind of mistake too often.
Otherwise, we should simply withhold judgement in case of a non-significant test result.

So, how do we make sure that the $\beta$ error is small?
We need to make sure that we have enough data.
By the Central Limit Theorem, we know that the sampling distribution of our test statistics will have lower variance the more samples we take.
Intuitively speaking, the more samples we have the steeper and tighter the curves in Figure \@ref(fig:chap-05-01-freq-testing-alpha-beta-error).
The $\alpha$ error will remain fixed, but the $\beta$ error decreases as the sample size increases.
Consequently, if a frequentist wants to be able to both reject *and* accept a null hypothesis using N-P testing logic, the frequentist will:

1. fix $H_0$ and $H_a$ (based on previous research if possible)
2. determine an acceptable level of $\alpha$
3. determine a desired level of **statistical power** defined as $1-\beta$
4. do a **power analysis**, i.e., compute (using math or simulations) how many samples are necessary to meet the $1-\beta$ power bar

For simple statistical tests, like $t$-tests and ANOVA, power calculations are mathematically tractable.
For more complex cases, analysts have to resort to simulations, which can be quite complex.^[Power analyses are also possible for Bayesian models, but not necessary for being able to quantify evidence in favor of a null hypothesis.]



## Confidence intervals {#ch-05-01-frequentist-testing-confidence-intervals}

The most commonly used interval estimate in frequentist analyses is *confidence intervals*. Although (frequentist) confidence intervals *can* coincide with (subjectivist) credible intervals in specific cases, they generally do not. And even when confidence and credible values yield the same numerical results, these notions are fundamentally different and ought not to be confused.

Let's look at credible intervals to establish the proper contrast. Recall that part of the definition of a credible interval for a posterior distribution over $\theta$, captured here notationally in terms of a random variable $\Theta$, was the probability $P(l \le \Theta \le u)$ that the value realized by random variable $\Theta$ lies in the interval $[l;u]$. This statement makes no sense to the frequentist. There cannot be any non-trivial value for $P(l \le \Theta \le u)$. The true value of $\theta$ is either in the interval $[l;u]$ or it is not. To speak of a probability that $\theta$ is in $[l;u]$ is to appeal to an ill-formed concept of probability which the frequentist denies.

In order to give an interval estimate nonetheless, the frequentist appeals to probabilities that she can accept: probabilities derived from (hypothetical) repetitions of a genuine random event with objectively observable outcomes. Let $\mathcal{D}$ be the random variable that captures the probability with which data $\mathcal{D}=D$ is realized. We obtain a pair of derived random variables $X_l$ and $X_u$ from a pair of functions $g_{l,u} \colon d \mapsto \mathbb{R}$. A **$\gamma\%$ confidence interval** for observed data $D_{\text{obs}}$ is the interval $[g_l(D_{\text{obs}}), g_u(D_{\text{obs}})]$ whenever functions $g_{l,u}$ are constructed in such a way that

$$
\begin{aligned}
  P(X_l \le \theta_{\text{true}} \le X_u) = \frac{\gamma}{100}
\end{aligned}
$$

where $\theta_{\text{true}}$ is the unknown but fixed true value of $\theta$. In more intuitive words, a confidence interval is an outcome of special construction (functions $g_{l,u}$) such that, when applying this procedure repeatedly to outcomes of the assumed data-generating process, the true value of parameter $\theta$ will lie inside of the computed confidence interval in exactly $\gamma$\% of the cases.

It is easier to think of the definition of a confidence interval in terms of computer code and sampling (see Figure \@ref(fig:03-03-estimation-confidence-interval-scheme)). Suppose Grandma gives you computer code, a `magic_function` which takes as input data observations, and returns an interval estimate for the parameter of interest. We sample a value for the parameter of interest repeatedly and consider it the "true parameter" for the time being. For each sampled "true parameter", we generate data repeatedly. We apply Grandma's `magic_function`, obtain an interval estimate, and check if the true value that triggered the whole process is included in the interval. Grandma's `magic_function` is a $\gamma\%$ confidence interval if the proportion of inclusions (the checkmarks in Figure \@ref(fig:03-03-estimation-confidence-interval-scheme)) is $\gamma\%$.

```{r 03-03-estimation-confidence-interval-scheme, echo = F, fig.cap="Schematic representation of what a confidence interval does: think of it as a magic function that returns intervals that contain the true value in $\\gamma$ percent of the cases."}
knitr::include_graphics("visuals/confidence-interval.png")
```

In some complex cases, the frequentist analyst relies on functions $g_{l}$ and $g_{u}$ that are easy to compute but only approximately satisfy the condition $P(X_l \le \theta_{\text{true}} \le X_u) = \frac{\gamma}{100}$. For example, we might use an
asymptotically correct calculation, based on the observation that, if $n$ grows to infinity, the binomial distribution approximates a normal distribution. We can then calculate a confidence interval *as if* our binomial distribution actually was a normal distribution. If $n$ is not large enough, this will be increasingly imprecise. Rules of thumb are used to decide how big $n$ has to be to involve at best a tolerable amount of imprecision (see the Info Box below). 

For our running example ($k = 7$, $n=24$), the rule of thumb mentioned in the Info Box below recommends *not* using the asymptotic calculation. If we did nonetheless, we would get a confidence interval of $[0.110; 0.474]$. For the binomial distribution, also a more reliable calculation exists, which yields $[0.126; 0.511]$ for the running example. (We can use numeric simulation to explore how good/bad a particular approximate calculation is, as shown in the next section.) The more reliable construction, the so-called *exact method*, implemented in the function `binom.confint` of R package `binom`, revolves around the close relationship between confidence intervals and $p$-values. (To foreshadow a later discussion: the exact $\gamma\%$ confidence interval is the set of all parameter values for which an exact (binomial) test does not yield a significant test result as the level of $\alpha = 1-\frac{\gamma}{100}$.)

<div class="infobox">
**Asymptotic approximation of a binomial confidence interval using a normal distribution.**

Let $X$ be the random variable that determines the binomial distribution, i.e., the probability of seeing $k$ successes in $n$ flips. For large $n$, $X$ approximates a normal distribution with a mean $\mu = n \ \theta$ and a standard deviation of    $\sigma = \sqrt{n \ \theta \ (1 - \theta)}$. The random variable $U$:

$$U = \frac{X - \mu}{\sigma} = \frac{X - n \  \theta}{\sqrt{n \  \theta \  (1-\theta)}}$$
Let $\hat{P}$ be the random variable that captures the distribution of our maximum likelihood estimates for an observed outcome $k$:

$$\hat{P} = \frac{X}{n}$$
Since $X = \hat{P} \  n$ we obtain:

$$U = \frac{\hat{P} \  n - n \  \theta}{\sqrt{n \  \theta \  (1-\theta)}}$$
We now look at the probability that $U$ is realized to lie in a symmetric interval $[-c,c]$, centered around zero --- a probability which we require to match our confidence level:

$$P(-c \le U \le c) = \frac{\gamma}{100}$$
We now expand the definition of $U$ in terms of $\hat{P}$, equate $\hat{P}$ with the current best estimate $\hat{p} = \frac{k}{n}$ based on the observed $k$ and rearrange terms, yielding the asymptotic approximation of a binomial confidence interval:

$$\left [ \hat{p} - \frac{c}{n} \  \sqrt{n \  \hat{p} \  (1-\hat{p})} ; \ \ 
   \hat{p} + \frac{c}{n} \  \sqrt{n \  \hat{p} \  (1-\hat{p})} \right ]$$
   
This approximation is conventionally considered precise enough when the following *rule of thumb* is met:

$$n \ \hat{p} \ (1 - \hat{p}) > 9$$

</div>

### Relation of *p*-values to confidence intervals

There is a close relation between $p$-values and confidence intervals.^[An important caveat applies here. There can be different (approximate) ways of defining $p$-values and confidence intervals. The relation described here does not hold when the (approximate) way of computing the $p$-value does not match the (approximate) way of computing the confidence interval.] For a two-sided test of a null hypothesis $H_0 \colon \theta = \theta_0$, with alternative hypothesis $H_a \colon \theta \neq \theta_0$, it holds for all possible data observations $D$ that

$$ p(D) < \alpha \ \ \text{iff} \ \ \theta_0 \not \in \text{CI}(D) $$
where $\text{CI}(D)$ is the $(1-\alpha) \cdot 100\%$ confidence interval constructed for data $D$.

This connection is intuitive when we think about long-term error. Decisions to reject the null hypothesis are false in exactly $(\alpha \cdot 100)\%$ of the cases when the null hypothesis is true. The definition of a confidence interval was exactly the same: the true value should lay outside a $(1-\alpha) \cdot 100\%$ confidence interval in exactly $(\alpha \cdot 100)\%$ of the cases. (Of course, this is only a vague and intuitively appealing argument based on the overall rate, not any particular case.)


<div class = "exercises">
**Exercise 16.4: $p$-value, confidence interval, interpretation etc.**

Suppose that we have reason to believe that a coin is biased to land heads. A hypothesis test should shed light on this belief. We toss the coin $N = 10$ times and observe $k = 8$ heads. We set $\alpha = 0.05$.

a. What is an appropriate null hypothesis, what is an appropriate alternative hypothesis?

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

$H_0$: $\theta_0 = 0.5$, $H_a$: $\theta > 0.5$.

</div>
</div>

b. Which alternative values of $k$ provide more extreme evidence against $H_0$?

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

Values greater than 8 (we conduct a one-sided hypothesis test).

</div>
</div>

c. The 95% confidence interval ranges between 0.493 and 1.0. Based on this information, decide whether the $p$-value is significant or non-significant. Why?

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

The $p$-value is non-significant because the value of the null hypothesis $H_0$: $\theta_0 = 0.5$ is contained within the 95% CI. Hence, it is not sufficiently unlikely that the observed outcome was generated by a fair coin.

</div>
</div>

d. Below is the probability mass function of the [Binomial distribution](#app-91-distributions-binomial) (our sampling distribution). The probability of obtaining exactly $k$ successes in $N$ independent trials is defined as: $$P(X = k)=\binom{N}{k}p^k(1-p)^{N-k},$$
where $\binom{N}{k}=\frac{N!}{k!(N-k)!}$ is the Binomial coefficient. Given the formula above, calculate the $p$-value (by hand) associated with our test statistic $k$, under the assumption that $H_0$ is true.

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

As this is a one-sided test, we look at those values of $k$ that provide more extreme evidence against $H_0$. We therefore compute the probability of at least 8 heads, given that $H_0$ is true:
$$
P(X\geq8)=P(X=8)+P(X=9)+P(X=10)\\
P(X=8)=\binom{10}{8}0.5^8(1-0.5)^{10-8}=45\cdot0.5^{10}\\
P(X=9)=\binom{10}{9}0.5^9(1-0.5)^{10-9}=10\cdot0.5^{10}\\
P(X=10)=\binom{10}{10}0.5^{10}(1-0.5)^{10-10}=1\cdot0.5^{10}\\
P(X\geq8)=0.5^{10}(45+10+1)\approx 0.0547
$$
</div>
</div>

e. Based on your result in d., decide whether we should reject the null hypothesis.

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

As we have a non-significant $p$-value ($p>\alpha$), we fail to reject the null hypothesis. Hence, we do not have evidence in favor of the hypothesis that the coin is biased to land heads.

</div>
</div>

f. Use R's built-in function for a Binomial test to check your results.

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

```{r}
binom.test(
  x = 8, # observed successes
  n = 10, # total no. of observations
  p = 0.5, # null hypothesis
  alternative = "greater" # alternative hypothesis
)
```

</div>
</div>

g. The $p$-value is affected by the sample size $N$. Try out different values for $N$ while keeping the proportion of successes constant to 80%. What do you notice with regard to the $p$-value?

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

With a larger sample size, the $p$-value is smaller compared to 10 coin flips. It requires only a few more samples to cross the significance threshold, allowing us to reject $H_0$. However, this is just the case if the null hypothesis is in fact false. NB: Don't collect more data *after* you observed the $p$-value! The sample size should be fixed prior to data collection and not increased afterwards.
  
</div>
</div>
</div>


## Selected tests {#ch-03-05-hypothesis-testing-tests}

This section captures a selection of commonly used frequentist tests.

### Pearson's $\chi^2$-tests {#ch-03-05-hypothesis-testing-Pearsons-Chi}

<div style = "float:right; width:16%;">
<img src="visuals/badge-BLJM.png" alt="badge-BLJM">  
</div>  

There are many tests that use the [$\chi^2$-distribution](#app-91-distributions-chi2) as an (approximate) sampling distribution. But given relevance and historical prominence, the name "$\chi^2$-test" is usually interpreted to refer to one of several flavor's of what we could specifically call "Pearson's $\chi^2$-test".

We will look at two flavors here. Pearson's $\chi^2$-test for **goodness of fit** tests whether an observed vector of counts is well explained by a given vector of predicted proportion. Pearson's $\chi^2$-test for **independence** tests whether a (two-dimensional) table of counts could plausibly have been generated by a process of independently selecting the column and the row category. We will explain how both of these tests work based on an application of the [BLJM data](#app-93-data-sets-BLJM), which we load as usual:

```{r}
data_BLJM_processed <- aida::data_BLJM
```

The focus is on the counts of music-subject choices:

```{r}
BLJM_associated_counts <- data_BLJM_processed %>% 
  select(submission_id, condition, response) %>% 
  pivot_wider(names_from = condition, values_from = response) %>% 
  # drop the Beach-vs-Mountain condition
  select(-BM) %>% 
  dplyr::count(JM,LB) 
BLJM_associated_counts
```

Remember that the lecturer's bold conjecture was that a preference for Logic over Biology goes together with a preference for Metal over Jazz. The visualization suggests that there might be such a trend, but the (statistical) jury is still out as to whether this conjecture has empirical support.

#### Pearson's $\chi^2$-test for goodness of fit

"Goodness of fit" is a term used in model checking (a.k.a. model criticism, model validation, ...). In such a context, tests for goodness-of-fit investigate whether a model's predictions are compatible with the observed data. Pearson's $\chi^2$-test for goodness of fit does exactly this for categorical data. 

Categorical data is data where each data observation falls into one of several unordered categories. If we have $k$ such categories, a **prediction vector** $\vec{p} = \langle p_1, \dots, p_k \rangle$  is a probability vector of length $k$ such that $p_i$ gives the probability with which a single data observation falls into the $i$-th category. The likelihood of a single data observation is given by the [Categorical distribution](#app-91-distributions-categorical), and the likelihood of $N$ data observations is given by the [Multinomial distribution](#app-91-distributions-multinomial). These are generalizations of the Bernoulli and Binomial distributions, which expand the case of two unordered categories to more than two unordered categories.

The BLJM data supplies us with categorical data. Here is the vector of counts of how many participants selected a given music+subject pair:

```{r}
# add category names
BLJM_associated_counts <- BLJM_associated_counts %>% 
  mutate(
    category = str_c(
      BLJM_associated_counts %>% pull(LB),
      "-",
      BLJM_associated_counts %>% pull(JM)
    )
  )
counts_BLJM_choice_pairs_vector <- BLJM_associated_counts %>% pull(n)
names(counts_BLJM_choice_pairs_vector) <- BLJM_associated_counts %>% pull(category)
counts_BLJM_choice_pairs_vector
```

Figure \@ref(fig:ch-03-04-BLJM-count-pairs-plot) shows a crude plot of these counts, together with a baseline prediction of equal proportion in each category.

```{r ch-03-04-BLJM-count-pairs-plot, echo = F, fig.cap="Observed counts of choice pairs of music+subject preference in the BLJM data."}
BLJM_associated_counts %>% 
  ggplot(aes(x = category, y = n)) +
  geom_col(aes(fill = category)) +
  guides(fill = "none") +
  geom_hline(aes(yintercept = sum(counts_BLJM_choice_pairs_vector) / 4), color = "firebrick") +
  geom_label(aes(x = 4, y = 27, label = "baseline expectation"), size = 3, color = "firebrick")
```

Pearson's $\chi^2$-test for goodness of fit allows us to test whether this data could plausibly have been generated by (a model whose predictions are given by) a prediction vector $\vec{p} = \langle p_1, \dots, p_4 \rangle$, where $p_1$ would be the predicted probability of a choice pair "Biology-Jazz" occurring for a single participant, and so on. Frequently, this test is used to check whether an equal baseline distribution could have generated the data. We do that here, too. We form the null hypothesis that $\vec{p} = \vec{p}_0$ with $p_{0i} = \frac{1}{4}$ for all categories $i$. 

Figure \@ref(fig:ch-03-04-chi2-model-goodness) shows a graphical representation of the model implicitly assumed in the background for a Pearson's $\chi^2$-test for goodness of fit. The model assumes that the observed vector of counts (like our `counts_BLJM_choice_pairs_vector` from above) follows a Multinomial distribution.^[Notice that for economy of presentation, we now (again) gloss over the "raw" data of individual choices and present the summarized count data instead. In the previous case of the Binomial Test, it made good pedagogical sense to tease apart the "raw" observations from the summarized counts because this helped to show what the test statistic is for a case, where the choice of it was very, very obvious; so much so, that we would normally not even bother to make it explicit. Now that we understood what a test statistic is in principle, we can gloss over some steps of data summarizing.] Each vector of (hypothetical) data is associated with a test statistic, called $\chi^2$, which sums over the standardized squared deviation of the observed counts from the predicted baseline in each cell. It can be shown that, if the number of observations $N$ is large enough, the sampling distribution of the $\chi^2$ test statistic is approximated well enough by the [$\chi^2$-distribution](#app-91-distributions-chi2) with $k-1$ degrees of freedom (where $k$ is the number of categories).^[A proof of this fact is non-trivial, but an intuition why this might be so is available if we think of each cell independently first. In each cell, with more and more samples, the distribution of counts will approximate a normal distribution by the CLT. The $\chi^2$-distribution rests (by construction) on a sum of squared samples from a standard normal distribution.] Notice that the approximation by a $\chi^2$-distribution hinges on an approximation, which is only met when there are enough samples (just as we needed in the CLT). A rule-of-thumb is that at most 20% of all cells should have expected frequencies below 5 in order for the test to be applicable, i.e., $np_i < 5$ for all $i$ in Figure \@ref(fig:ch-03-04-chi2-model-goodness).

```{r ch-03-04-chi2-model-goodness, echo = F, fig.cap="Graphical representation of Pearson's $\\chi^2$-test for goodness of fit (testing a vector of predicted proportion).", out.width = '90%'}
knitr::include_graphics("visuals/chi2-model-goodness.png" )
```

We can compute the $\chi^2$-value associated with the observed data $t(D_{obs})$ as follows:

```{r}
# observed counts
n <- counts_BLJM_choice_pairs_vector
# proportion predicted 
p <- rep(1/4, 4)
# expected number in each cell
e <- sum(n) * p
# chi-squared for observed data
chi2_observed <- sum((n - e)^2 * 1/e)
chi2_observed
```

We can then compare this value to the sampling distribution, which is a $\chi^2$-distribution with $k-1 = 3$ degrees of freedom. We compute the $p$-value associated with our data as the tail of the sampling distribution, as also shown in Figure \@ref(fig:ch-03-04-chi2-plot):^[Notice that this is a one-sided test due to the nature of the test statistic, which measures squared deviation from the baseline and not deviation in any particular direction (because it is hard to say what a "direction" would be in this case anyway).]

```{r}
p_value_BLJM <- 1 - pchisq(chi2_observed, df = 3)
```


```{r ch-03-04-chi2-plot, echo = F, fig.cap="Sampling distribution for a Pearson's $\\chi^2$-test of goodness of fit ($\\chi^2$-distribution with $k-1 = 3$ degrees of freedom), testing a flat baseline null hypothesis based on the BLJM data.", out.width = '90%'}
tibble(
  x = seq(0,16, length.out = 1000) ,
  y = dchisq(x, df = 3)
) %>% 
  ggplot(aes(x = x , y = y)) +
  geom_line() +
  labs(
    x = latex2exp::TeX("$\\chi^2$"),
    y = "density"
  ) +
  # line and label for observed value
  geom_segment(
    aes(
      x = chi2_observed,
      xend = chi2_observed,
      y = 0,
      yend = 0.15
    ), color = "firebrick"
  ) +
  geom_label(aes(x = chi2_observed, y = 0.16, label = "observed value\nof test statistic")) +
  # line and label for tail area
  geom_segment(
    aes(
      x = 10,
      xend = 12,
      y = 0.005,
      yend = 0.05
    ), color = "darkgray"
  ) +
  geom_label(aes(x = 12, y = 0.06, label = str_c("tail area =\n", round(p_value_BLJM,5)))) +
  # shading tail area
  geom_area(aes(y = ifelse(
    x >= chi2_observed, y, 0)),
    fill = "firebrick", alpha = 0.5) 
  
```

Of course, these calculations can also be performed by using a built-in R function, namely `chisq.test`:

```{r}
counts_BLJM_choice_pairs_vector <- BLJM_associated_counts %>% pull(n)
chisq.test(counts_BLJM_choice_pairs_vector)
```

The common interpretation of our calculations would be to say that the test yielded a significant result, at least at the significance level of $\alpha = 0.5$. In a research paper, we might report these results roughly as follows:

> Observed counts deviated significantly from what is expected if each category (here: pair of music+subject choice) was equally likely ($\chi^2$-test, with $\chi^2 \approx 9.53$, $df = 3$ and $p \approx 0.023$).

Notice that this test is an "omnibus test of difference". We can conclude from a significant test result that the whole vector of observations is unlikely to have been generated by chance. Still, we cannot conclude from this result (without doing anything else) why, where or how the observations deviated from the assumed prediction vector. Looking at the plot of the data in Figure \@ref(fig:ch-03-04-BLJM-count-pairs-plot) above, it seems intuitive to think that Metal is disproportionally disfavored and that the combination of Biology and Jazz looks particularly outliery when compared to the baseline expectation.

#### Pearson's $\chi^2$-test of independence

The previous test of goodness of fit does not allow us to address the lecturer's conjecture that a preference of Metal over Jazz goes with a preference of Logic over Biology. A slightly different kind of $\chi^2$-test is better suited for this. In Pearson's $\chi^2$-test of independence, we look at a two-dimensional table of correlated data observations, like this one:

```{r}
BLJM_table <- BLJM_associated_counts %>% 
  select(-category) %>% 
  pivot_wider(names_from = LB, values_from = n)
BLJM_table
```

For easier computation and compatibility with the function `chisq.test`, we handle the same data but stored as a matrix:

```{r}
counts_BLJM_choice_pairs_matrix <- matrix(
  counts_BLJM_choice_pairs_vector, 
  nrow = 2, 
  byrow = T
)
rownames(counts_BLJM_choice_pairs_matrix) <- c("Jazz", "Metal")
colnames(counts_BLJM_choice_pairs_matrix) <- c("Biology", "Logic")
counts_BLJM_choice_pairs_matrix
```

Pearson's $\chi^2$-test of independence addresses the question of whether two-dimensional tabular count data like the above could plausibly have been generated by a prediction vector $\vec{p}$, which results from the assumption that the realizations of row- and column-choices are [stochastically independent](#Chap-03-01-probability-independence). If row- and column-choices are independent, the probability of seeing an outcome result in cell $ij$ is the probability of realizing row $i$ times the probability of realizing column $j$. So, under an independence assumption, we expect a matrix and a resulting vector of choice proportions like this:

```{r}
# number of observations in total
N <- sum(counts_BLJM_choice_pairs_matrix)
# marginal proportions observed in the data 
# the following is the vector r in the model graph
row_prob <- counts_BLJM_choice_pairs_matrix %>% rowSums() / N
# the following is the vector c in the model graph
col_prob <- counts_BLJM_choice_pairs_matrix %>% colSums() / N
# table of expected observations under independence assumption
# NB: %o% is the outer product of vectors
BLJM_expectation_matrix <- (row_prob %o% col_prob) * N 
BLJM_expectation_matrix
# the following is the vector p in the model graph
BLJM_expectation_vector <- as.vector(BLJM_expectation_matrix)
BLJM_expectation_vector
```

Figure \@ref(fig:ch-03-04-chi2-model-independence) shows a graphical representation of the $\chi^2$-test of independence. The main difference to the previous test of goodness of fit is that we do no longer just fix any-old prediction vector $\vec{p}$, but consider $\vec{p}$ the deterministic results of independence *and* the best estimates (based on the data at hand) of the row- and column probabilities.

```{r ch-03-04-chi2-model-independence, echo = F, fig.cap="Graphical representation of Pearson's $\\chi^2$-test for independence.", out.width = '90%'}
knitr::include_graphics("visuals/chi2-model-independence.png")
```

We can compute the observed $\chi^2$-test statistic and the $p$-value as follows:

```{r}
chi2_observed <- sum(
  (counts_BLJM_choice_pairs_matrix - BLJM_expectation_matrix)^2 / 
    BLJM_expectation_matrix
  )
p_value_BLJM <- 1 - pchisq(q = chi2_observed, df = 1)
round(p_value_BLJM, 5)
```

Figure \@ref(fig:ch-03-04-chi2-plot-independence) shows the sampling distribution, the value of the test statistic for the observed data and the $p$-value.

```{r ch-03-04-chi2-plot-independence, echo = F, fig.cap="Sampling distribution for a Pearson's $\\chi^2$ test of independence ($\\chi^2$-distribution with $1$ degree of freedom), testing a flat baseline null hypothesis based on the BLJM data.", out.width = '90%'}
tibble(
  x = seq(0,4, length.out = 1000) ,
  y = dchisq(x, df = 1)
) %>% 
  ggplot(aes(x = x , y = y)) +
  geom_line() +
  labs(
    x = latex2exp::TeX("$\\chi^2$"),
    y = "density"
  ) +
  # line and label for observed value
  geom_segment(
    aes(
      x = chi2_observed,
      xend = chi2_observed,
      y = 0,
      yend = 3
    ), color = "firebrick"
  ) +
  geom_label(aes(x = chi2_observed, y = 3, label = "observed value\nof test statistic")) +
  # line and label for tail area
  geom_segment(
    aes(
      x = 0.8,
      xend = 1.5,
      y = 0.15,
      yend = 1.2
    ), color = "darkgray"
  ) +
  geom_label(aes(x = 1.5, y = 1.2, label = str_c("tail area =\n", round(p_value_BLJM,5)))) +
  # shading tail area
  geom_area(aes(y = ifelse(
    x >= chi2_observed, y, 0)),
    fill = "firebrick", alpha = 0.5) 
  
```

We can also use the built-in function `chisq.test` in R to obtain this result more efficiently:

```{r}
chisq.test(
  # supply data as a matrix, not as a vector, for a test of independence
  counts_BLJM_choice_pairs_matrix, 
  # do not use the default correction (because we didn't introduce it)
  correct = FALSE
)
```

With a $p$-value of about `r round(p_value_BLJM,4)`, we should conclude that there is no indication of strong evidence *against* the assumption of independence. Consequently, there is no evidence *in favor* of the lecturer's conjecture of dependence of musical and academic preferences. In a research paper, we might report this result as follows:

> A $\chi^2$-test of independence did not yield a significant test result ($\chi^2$-test, with $\chi^2 \approx 0.44$, $df = 1$ and $p \approx 0.5$). Therefore, we cannot claim to have found any evidence for the research hypothesis of dependence.

<!-- exercise 2 -->
<!-- Taken from the prep exam (IDA-prep-exam-01.pdf) -->
<div class = "exercises">
**Exercise 16.5: $\chi^2$-test of independence**

Let us assume that there are two unordered categorical variables $A$ and $B$. Categorical variable $A$ has two levels $a_1$ and $a_2$. Categorical variable $B$ has three levels $b_1$, $b_2$ and $b_3$. Let us further assume that the (marginal) probabilities of a choice from categories $A$ or $B$ is as follows:

$$
P(A=a_i)=\begin{cases}
          0.3 &\textbf{if \(i=1\)} \\
          0.7 &\textbf{if \(i=2\)}
          \end{cases}
\quad P(B=b_i)=\begin{cases}
                0.2 &\textbf{if \(i=1\)}\\
                0.3 &\textbf{if \(i=2\)}\\
                0.5 &\textbf{if \(i=3\)}
               \end{cases}
$$

a. If observations of pairs of instances from categories $A$ and $B$ are stochastically independent, what would the expected joint probability of each pair of potential observations be?

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

<div align="center">

```{r echo=FALSE}
knitr::kable(
  tibble(
    " " = c("$a_1$", "$a_2$"),
    "$b_1$" = c(".3 $\\times$ .2 = .06", ".7 $\\times$ .2 = .14"),
    "$b_2$" = c(".3 $\\times$ .3 = .09", ".7 $\\times$ .3 = .21"),
    "$b_3$" = c(".3 $\\times$ .5 = .15", ".7 $\\times$ .5 = .35")
  ),
  align = 'c',
  escape = F,
  booktabs = T,
  format = "html",
  table.attr = "style='width:70%'"
) 
```

</div>

</div>
</div>

b. Imagine you observe the following table of counts for each pair of instances of categories $A$ and $B$: 

<div align="center">

```{r echo=F}
knitr::kable(
  tibble(
    " " = c("$a_1$", "$a_2$"),
    "$b_1$" = c("1", "19"),
    "$b_2$" = c("26", "4"),
    "$b_3$" = c("3", "47")
  ),
  align = 'c',
  escape = F,
  booktabs = T,
  format = "html",
  table.attr = "style='width:70%'"
)
```

</div>

&emsp;&emsp; Which of the $p$-values given below would you expect to see when feeding this table into a 
&emsp;&emsp; Pearson $\chi^2$-test of independence? (only one correct answer)

<ol type="i" start="1" style="margin-left: 2em;">
  <li>$p \approx 1$</li>
  <li>$p \approx 0.5$</li>
  <li>$p \approx 0$</li>
  <li>I expect no result because the test is not suitable for this kind of data.</li>
</ol>

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

The correct answer is $p \approx 0$.

</div>
</div>

c. Explain the answer you gave in the previous part in at most three concise sentences.

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

As the marginal proportions of observed counts for the table in b. equal the marginal probabilities given above, the joint probability table in a. actually gives the predicted probabilities under the assumption of independence. Comparing prediction against observed proportion (obtained by dividing the table in b. by the total count of 100), we see severe divergences, especially in the middle column.

</div>
</div>

</div>


### *z*-test {#ch-03-05-hypothesis-testing-z-test}

The Central Limit Theorem tells us that, given enough data, we can treat means of repeated samples from any arbitrary probability distribution as approximately normally distributed. Notice in addition that if $X$ and $Y$ are random variables following a normal distribution, then so is $Z = X - Y$ (see also the [chapter on the normal distribution](#app-91-distributions-normal)). It now becomes clear how research questions about means and differences between means (e.g., in the Mental Chronometry experiment) can be addressed, at least approximately: We conduct tests that hinge on a sampling distribution which is a normal distribution (usually a standard normal distribution).

The $z$-test is perhaps the simplest of a family of tests that rely on normality of the sampling distribution. Unfortunately, what makes it so simple is also what makes it inapplicable in a wide range of cases. The $z$-test assumes that a quantity that is normally distributed has an unknown mean (to be inferred by testing), but it also assumes that the *variance is known*. Since we do not know the variance in most cases of practical relevance, the $z$-test needs to be replaced by a more adequate test, usually a test from the $t$-test family, to be discussed below.

We start with the $z$-test nonetheless because of the added benefit to our understanding. Figure \@ref(fig:ch-03-04-z-test-model) shows the model that implicitly underlies a $z$-test. It checks whether the data $\vec{x}$, which are assumed to be normally distributed with known $\sigma$, could have been generated by a hypothesized mean $\mu = \mu_0$. The sampling distribution of the derived test statistic $z$ is a standard normal distribution. 

```{r ch-03-04-z-test-model, echo = F, fig.cap="Graphical representation of a $z$-test.", out.width = '90%'}
knitr::include_graphics("visuals/z-test-model.png")
```

We know that IQ test results are normally distributed around a mean of 100 with a standard deviation of 15. This holds when the sample is representative of the whole population. But suppose we have reason to believe that the sample is from CogSci students. The standard deviation in a sample from CogSci students might still plausibly be fixed to 15, but we'd like to test the assumption that *this* sample was generated by a mean $\mu = 100$, our null hypothesis.

For illustration, suppose we observed the following data set of IQ test results:

```{r}
# fictitious IQ data
IQ_data <- c(87, 91, 93, 97, 100, 101, 103, 104, 
             104, 105, 105, 106, 108, 110, 111, 
             112, 114, 115, 119, 121)
mean(IQ_data)
```

The mean of this data set is `r mean(IQ_data)`. Suspicious!

Following the model in Figure \@ref(fig:ch-03-04-z-test-model), we calculate the value of the test statistic for the observed data.

```{r}
# number of observations
N <- length(IQ_data)
# null hypothesis to test
mu_0 <- 100
# standard deviation (known/assumed as true)
sd <- 15
z_observed <- (mean(IQ_data) - mu_0) / (sd / sqrt(N))
z_observed %>% round(4)
```

We focus on a one-sided $p$-value because our "research" hypothesis is that CogSci students have, on average, a higher IQ. Since we observed a mean of `r mean(IQ_data)` in the data, which is higher than the critical value of 100, we test the null hypothesis $\mu = 100$ against an alternative hypothesis that assumes that the data was generated by a mean *bigger* than 100 (which is exactly our research hypothesis).

As before, we can then compute the $p$-value by checking the area under the sampling distribution, here a standard normal, in the appropriate way. Figure \@ref(fig:ch-03-04-z-test) shows this result graphically.

```{r}
p_value_IQ_data_ztest <- 1 - pnorm(z_observed)
p_value_IQ_data_ztest %>% round(6)
```


```{r ch-03-04-z-test, echo = F, fig.cap="Sampling distribution for a $z$-test, testing the null hypothesis based on the assumption that the IQ-data was generated by $\\mu = 100$ (with assumed/known $\\sigma$).", out.width = '90%'}
tibble(
  x = seq(-4,4, length.out = 1000) ,
  y = dnorm(x)
) %>% 
  ggplot(aes(x = x , y = y)) +
  geom_line() +
  labs(
    x = latex2exp::TeX("$z$"),
    y = "density"
  ) +
  # line and label for observed value
  geom_segment(
    aes(
      x = z_observed,
      xend = z_observed,
      y = 0,
      yend = 0.15
    ), color = "firebrick"
  ) +
  geom_label(aes(x = z_observed, y = 0.16, label = "observed value\nof test statistic")) +
  # line and label for tail area
  geom_segment(aes(x = 2.1, xend = 3, y = 0.01, 
                   yend = 0.05), color = "darkgray") +
  geom_label(aes(x = 3, y = 0.06, label = str_c("tail area =\n", 
                                                round(p_value_IQ_data_ztest,6)))) +
  # shading tail area
  geom_area(aes(y = ifelse(x >= z_observed, y, 0)),
            fill = "firebrick", alpha = 0.5) 
  
```

We can also use a ready-made function for the $z$-test. However, as the $z$-test is so uncommon, it is not built into core R. We need to rely on the `BSDA` package to find the function `z.test`.

```{r}
BSDA::z.test(x = IQ_data, mu = 100, sigma.x = 15, alternative = "greater")
```

The conclusion to be drawn from this test could be formulated in a research report as follows:

> We tested the null hypothesis of a mean equal to 100, assuming a known standard deviation of 15, in a one-sided $z$-test against the alternative hypothesis that the data was generated by a mean greater than 100 (our research hypothesis). The test was not significant ($N = `r length(IQ_data)`$, $z \approx `r z_observed %>% round(4)`$, $p \approx `r p_value_IQ_data_ztest %>% round(5)`$), giving us no indication of strong evidence against the assumption that the mean is at most 100.

### *t*-tests {#ch-03-05-hypothesis-testing-t-test}

In most practical applications where a $z$-test might be useful, the standard deviation is not known. If unknown, it should also not lightly be fixed by clever guess-work. This is where the family of $t$-tests comes in. We will look at two examples of these: the one-sample $t$-test, which compares one set of samples to a fixed mean, and the two-sample $t$-test, which compares the means of two sets of samples.

#### One-sample $t$-test

The simplest example of this family, namely a $t$-test for one metric vector $\vec{x}$ of normally distributed observations, tests the null hypothesis that $\vec{x}$ was generated by some $\mu = \mu_0$ (just like the $z$-test). However, unlike the $z$-test, a one-sample $t$-test does not assume that the standard deviation is known. It rather uses the observed data to obtain an estimate for this parameter. More concretely, a one-sample $t$-test for $\vec{x}$ estimates the standard deviation in the usual way (see Chapter \@ref(Chap-02-03-summary-statistics)):

$$\hat{\sigma}_x = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (x_i - \mu_{\vec{x}})^2}$$

Figure \@ref(fig:ch-03-04-t-test-model-one-population) shows a graphical representation of a one-sample $t$-test model. The light shading of the node for the standard deviation indicates that this parameter is estimated from the observed data. Importantly, the distribution of the test statistic $t$ is no longer well approximated by a normal distribution when the sample size is low. It is better captured by a [Student's $t$ distribution](#app-91-distributions-students-t).

```{r ch-03-04-t-test-model-one-population, echo = F, fig.cap="Graphical representation of the model underlying a frequentist one-sample $t$-test. Notice that the lightly shaded node for the standard deviation represents that the value for this parameter is estimated from the data.", out.width = '60%'}
knitr::include_graphics("visuals/t-test-model-one-population.png")
```

Let's revisit our IQ-data set from above to calculate a $t$-test. Using a $t$-test implies that we are now assuming that the standard deviation is actually unknown. We can calculate the value of the test statistic for the observed data and use this to compute a $p$-value, much like in the case of the $z$-test before.

```{r}
N <- length(IQ_data)
# fix the null hypothesis
mean_0 <- 100
# unlike in a z-test, we use the sample to estimate the SD
sigma_hat <- sd(IQ_data) 
t_observed <- (mean(IQ_data) - mean_0) / sigma_hat * sqrt(N)
t_observed %>% round(4)
```

We calculate the relevant one-sided $p$-value using the cumulative distribution function `pt` of the $t$-distribution.

```{r}
p_value_t_test_IQ <- 1 - pt(t_observed, df = N - 1)
p_value_t_test_IQ %>% round(6)
```


```{r ch-03-04-t-test-one-sample, echo = F, fig.cap="Sampling distribution for a $t$-test, testing the null hypothesis that the IQ-data was generated by $\\mu = 100$ (with unknown $\\sigma$).", out.width = '90%'}
tibble(
  x = seq(-4,4, length.out = 1000) ,
  y = dt(x, df = N-1)
) %>% 
  ggplot(aes(x = x , y = y)) +
  geom_line() +
  labs(
    x = latex2exp::TeX("$t$"),
    y = "density"
  ) +
  # line and label for observed value
  geom_segment(
    aes(
      x = t_observed,
      xend = t_observed,
      y = 0,
      yend = 0.15
    ), color = "firebrick"
  ) +
  geom_label(aes(x = t_observed, y = 0.16, label = "observed value\nof test statistic")) +
  # line and label for tail area
  geom_segment(
    aes(
      x = 2.7,
      xend = 3.3,
      y = 0.005,
      yend = 0.05
    ), color = "darkgray"
  ) +
  geom_label(aes(x = 3.3, y = 0.06, label = str_c("tail area =\n", round(p_value_t_test_IQ,6)))) +
  # shading tail area
  geom_area(aes(y = ifelse(
    x >= t_observed, y, 0)),
    fill = "firebrick", alpha = 0.5) 
  
```

Compare these calculations against the built-in function `t.test`:

```{r}
t.test(x = IQ_data, mu = 100, alternative = "greater")
```

These results could be stated in a research report much like so:

> We tested the null hypothesis of a mean equal to 100, assuming an unknown standard deviation, using a one-sided, one-sample $t$-test against the alternative hypothesis that the data was generated by a mean greater than 100 (our research hypothesis). The significant test result ($N = `r length(IQ_data)`$, $t \approx `r t_observed %>% round(4)`$, $p \approx `r p_value_t_test_IQ %>% round(6)`$) suggests that the data provides strong evidence against the assumption that the mean is not bigger than 100.

Notice that the conclusions we draw from the previous $z$-test and this one-sample $t$-test are quite different. Why is this so? Well, it is because we (cheekily) chose a data set `IQ_data` that was actually *not* generated by a normal distribution with a standard deviation of 15, contrary to what we said about IQ-scores normally having this standard deviation. The assumption about $\sigma$ fed into the $z$-test was (deliberately!) wrong. The result of the $t$-test, at least for this example, is better. The data in `IQ_data` are actually samples from $\text{Normal}(105,10)$. This demonstrates why the one-sample $t$-test is usually preferred over a $z$-test: unshakable, true knowledge of $\sigma$ is very rare.

#### Two-sample $t$-test (for unpaired data with equal variance and unequal sample sizes)

<div style = "float:right; width:16%;">
<img src="visuals/badge-avocado.png" alt="badge-avocado">
</div>

The "mother of all experimental designs" compares two groups of measurements. We give a drug to one group of patients, a placebo to another. We take a metric measure (say, blood sugar level) and ask whether there is a difference between these two groups. Section \@ref(ch-03-04-parameter-estimation) introduced the $T$-Test Model for a Bayesian approach. Here, we look at a corresponding model for a frequentist approach, a so-called two-sample $t$-test. There are different kinds of such two-sample $t$-tests. The differences lie, e.g., in whether we assume that both groups have equal variance, in whether the sample sizes are the same in both groups, or in whether observations are paired (e.g., as in a within-subjects design, where we get two measurements from each participant, one from each condition/group). Here, we focus on unpaired data (as from a between-subjects design), assume equal variance but (possibly) unequal sample sizes. The case we look at is the [avocado data](#app-93-data-sets-avocado), where we want to specifically investigate whether the weekly average price of organically grown avocados is higher than that of conventionally grown avocados.^[Notice that the original avocado data set contains information also about the place of measurement, which would in principle allow us to treat the price measurements as paired samples (one pair for each week and place). For simplicity, but with a note of care that this makes us lose possibly relevant structural information, we here treat the avocado data as if it contained unpaired samples.]

We here consider the preprocessed avocado data set (see Appendix Chapter \@ref(app-93-data-sets-avocado) for details on how this preprocessing was performed).

```{r}
avocado_data <- aida::data_avocado
```

Remember that the distribution of prices looks as follows:

```{r, echo = F}
avocado_data %>% 
  ggplot(aes(x = average_price, fill = type)) +
  geom_density(alpha = 0.3) +
  ylab('') +
  xlab('average weekly price') 
```

A graphical representation of the two-sample $t$-test (for unpaired data with equal variance and unequal sample sizes), which we will apply to this case, is shown in Figure \@ref(fig:ch-03-04-t-test-model-two-populations). The model assumes that we have two vectors of metric measurements $\vec{x}_A$ and $\vec{x}_B$, with length $n_A$ and $n_B$, respectively. These are the price measures for conventionally grown and for organically grown avocados. The model assumes that measures in both $\vec{x}_A$ and $\vec{x}_B$ are i.i.d. samples from a normal distribution. The mean of one group (group $B$ in the graph) is assumed to be some unknown $\mu$. Interestingly, this parameter will cancel out eventually: the approximation of the sampling distribution turns out to be independent of this parameter.^[This is intuitively so because the test statistic is concerned only with the difference between sample means.] The mean of the other group (group $A$ in the graph) is computed as $\mu + \delta$, so with some additive parameter $\delta$ indicating the difference between means of these groups. This $\delta$ is the main parameter of interest for inferences regarding hypotheses concerning differences between groups. Finally, the model assumes that both groups have the same standard deviation, an estimate of which is derived from the data (in a rather convoluted looking formula that is not important for our introductory concerns). As indicated in Figure \@ref(fig:ch-03-04-t-test-model-two-populations), the sampling distribution for this model is an instance of Student's $t$-distribution with mean 0, standard deviation 1 and degrees of freedom $\nu$ given as $n_A + n_B - 2$.

```{r ch-03-04-t-test-model-two-populations, echo = F, fig.cap="Graphical representation of the model underlying a frequentist two-population $t$-test (for unpaired data with equal variance and unequal sample sizes). Notice that the light shading of the node for the standard deviation indicates that the value for this parameter is estimated from the data.", out.width = '90%'}
knitr::include_graphics("visuals/t-test-model-two-populations.png")
```

Figure \@ref(fig:ch-03-04-t-test-model-two-populations) gives us the template to compute the value of the test statistic for the observed data:

```{r}
# fix the null hypothesis: no difference between groups
delta_0 <- 0
# data (group A)
x_A <- avocado_data %>% 
  filter(type == "organic") %>% pull(average_price)
# data (group B)
x_B <- avocado_data %>% 
  filter(type == "conventional") %>% pull(average_price)
# sample mean for organic (group A)
mu_A <- mean(x_A)
# sample mean for conventional (group B)
mu_B <- mean(x_B)
# numbers of observations
n_A <- length(x_A)
n_B <- length(x_B)
# variance estimate
sigma_AB <- sqrt(
  ( ((n_A - 1) * sd(x_A)^2 + (n_B - 1) * sd(x_B)^2 ) / 
      (n_A + n_B - 2) ) * (1/n_A + 1/n_B)
)
t_observed <- (mu_A - mu_B - delta_0) / sigma_AB
t_observed  
```

We can use the value of the test statistic for the observed data to compute a one-sided $p$-value, as before. Notice that we use a one-sided test because we hypothesize that organically grown avocados are more expensive, not just that they have a different price (more expensive or cheaper).

```{r}
p_value_t_test_avocado <- 1 - pt(q = t_observed, df = n_A + n_B - 2)
p_value_t_test_avocado
```

Owing to number imprecision, the calculated $p$-value comes up as a flat zero. We have a lot of data, and the task of defending that conventionally grown avocados are not less expensive than organically grown is very tough. This also shows in the corresponding picture in Figure \@ref(fig:ch-03-04-t-test-two-sample). 


```{r ch-03-04-t-test-two-sample, echo = F, fig.cap="Sampling distribution for a two-sample $t$-test, testing the null hypothesis of no difference between groups, based on the avocado data.", out.width = '90%'}
tibble(
  x = seq(-50,120, length.out = 500) ,
  y = dt(x, df = N-1)
) %>% 
  ggplot(aes(x = x , y = y)) +
  geom_line() +
  labs(
    x = latex2exp::TeX("$t$"),
    y = "density"
  ) +
  # line and label for observed value
  geom_segment(
    aes(
      x = t_observed,
      xend = t_observed,
      y = 0,
      yend = 0.15
    ), color = "firebrick"
  ) +
  geom_label(aes(x = t_observed, y = 0.16, label = "observed value\nof test statistic"))
```

We can also, of course, calculate this test result with the built-in function `t.test`:

```{r}
t.test(
  x = x_A,           # first vector of data measurements
  y = x_B,           # second vector of data measurements
  paired = FALSE,    # measurements are to be treated as unpaired
  var.equal = TRUE,  # we assume equal variance in both groups
  mu = 0             # NH is delta = 0 (name 'mu' is misleading!)
)
```

The result could be reported as follows:

> We conducted a two-sample $t$-test of differences of means (unpaired samples, equal variance, unequal sample sizes) to compare the average weekly price of conventionally grown avocados to that of organically grown avocados. The test result indicates a significant difference for the null hypothesis that conventionally grown avocados are not cheaper ($N_A = `r n_A`$, $N_B = `r n_B`$, $t \approx `r t_observed %>% round(2)`$, $p \approx `r p_value_t_test_avocado`$).

<!-- exercise 3 -->
<div class = "exercises">
**Exercise 16.6: Two-sample $t$-test**

Your fellow student is skeptical of her flatmate's claim that pizzas from place $A$ have a smaller diameter than place $B$ (both pizzerias have just one pizza size, namely $\varnothing\ 32\ cm$). She decides to test that claim with a two-sample $t$-test and sets $H_0: \mu_A = \mu_B$ ($\delta = 0$), $H_a: \mu_A < \mu_B$, $\alpha = 0.05$. She then asks your class to always measure the pizza's diameter if ordered from one of the two places. At the end of the semester, she has the following table:

<div align="center">

```{r echo=FALSE}
knitr::kable(
  tibble(
    " " = c("mean", "standard deviation", "sample size"),
    "Pizzeria $A$" = c("30.9", "2.3", "38"),
    "Pizzeria $B$" = c("31.8", "2", "44"),
  ),
  align = 'c',
  booktabs = T,
  format = "html",
  table.attr = "style='width:70%'"
)
```

</div>

a. How many degrees of freedom $\nu$ are there?

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

$\nu = n_A+n_B-2 = 38+44-2 = 80$ degrees of freedom.

</div>
</div>

b. Given the table above, calculate the test statistic $t$.

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

$$
\hat{\sigma}=\sqrt{\frac{(n_A-1)\hat{\sigma}_A^2+(n_B-1)\hat{\sigma}^2_B}{n_A+n_B-2}(\frac{1}{n_A}+\frac{1}{n_B})}\\
\hat{\sigma}=\sqrt{\frac{37\cdot2.3^2+43\cdot2^2}{80}(\frac{1}{38}+\frac{1}{44})}\approx 0.47\\
t=((\bar{x}_A-\bar{x}_B)-\delta)\cdot\frac{1}{\hat{\sigma}}\\
t=\frac{30.9-31.8}{0.47}\approx -1.91
$$
</div>
</div>

c. Look at this so-called [t table](http://www.ttable.org/) and determine the critical value to be exceeded in order to get a statistically significant result. NB: We are looking for the critical value that is on the *left* side of the distribution. So, in order to have a statistically significant result, the test statistic from b. has to be smaller than the *negated* critical value in the table.

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

The critical value is -1.664.

</div>
</div>

d. Compare the test statistic from b. with the critical value from c. and interpret the result.

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

The calculated test statistic from b. is smaller than the critical value. We therefore know that the $p$-value is statistically significant. The fellow student should reject the null hypothesis of equal pizza diameters.

</div>
</div>

</div>


### ANOVA {#ch-03-05-hypothesis-testing-ANOVA}

ANOVA is short for "analysis of variance".
It's an umbrella term for a number of different models centered around testing the influence of one or several categorical predictors on a metric measurement.
In previous sections, we have summoned regression models for this task.
This is indeed the more modern and preferred approach, especially when the regression modeling also takes random effects (so-called hierarchical modeling) into account.
Nonetheless, it is good to have a basic understanding of ANOVAs, as they are featured prominently in a lot of published research papers, whose findings are still relevant.
Also, in some areas of empirical science, ANOVAs are still commonly used.

Here we are just going to cover the most basic type of ANOVA, which is called a *one-way ANOVA*.
A one-way ANOVA is, in regression jargon, a suitable approach for the case of a single categorical predictor with more than two levels (otherwise a $t$-test would be enough) and a metric dependent variable.
For illustration we will here consider a fictitious case of metric measurement for three groups: A, B, and C.
These groups are levels of a categorical predictor `group`.
We want to address the research question of whether the means of the measurements of groups A, B and C could plausibly be identical.

The main idea behind analysis of variance is *not* to look at the means of measurements to be compared, but rather to compare the *between-group variances* to the *within-group variances*.
Whence the name "analysis of variance".
While mathematically complex, the idea is quite intuitive.
Figure \@ref(fig:ch-05-01-examples-F-score) shows four different (made-up) data sets, each with different measurements for groups A, B and C.
It also shows the "pooled data", i.e., the data from all three groups combined.
What is also shown in each panel is the so-called F-statistic, which is a number derived from a sample in the following way.

We have $k \ge 2$ groups of metric observations. 
For group $1 \le j \le k$, there are $n_j$ observations. 
Let $x_{ij}$ be the observation $1 \le i \le n_j$ for group $1 \le j \le k$. 
Let $\bar{x}_j = \frac{1}{n_j} \sum_{i = 1}^{n_j} x_{ij}$ be the mean of group $j$ and let $\bar{\bar{x}} = \frac{1}{k} \sum_{j=1}^k \frac{1}{n_j} \sum_{i=1}^{n_j} x_{ij}$ be the grand mean of all data points.
The **between-group variance** measures how much, on average, the mean of each group deviates from the grand mean of all data points (where distance is squared distance, as usual):

$$
\hat{\sigma}_{\mathrm{between}} = \frac{\sum_{j=1}^k n_j (\bar{x}_j - \bar{\bar{x}})^2}{k-1}
$$
The **within-group variance** is a measure of the average variance of the data points inside of each group:

$$
\hat{\sigma}_{\mathrm{within}} = \frac{\sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2}{\sum_{i=1}^k (n_i - 1)}
$$

Now, if the means of different groups are rather different from each other, the between-group variance should be high.
But absolute numbers may be misleading, so we need to scale the between-group variance also by how much variance we see, on average, in each group, i.e., the within-group variance.
That is why the $F$-statistic is defined as:

$$
F = \frac{\hat{\sigma}_{\mathrm{between}}}{\mathrm{\hat{\sigma}_{\mathrm{within}}}}
$$
For illustration, Figure \@ref(fig:ch-05-01-examples-F-score) shows four different scenarios with associated measures of $F$.

```{r ch-05-01-examples-F-score, echo = F, fig.cap="Different examples of metric measurements for three groups (A, B, C), shown here together with a plot of the combined (= pooled) data. We see that, as the means of measurements go apart, so does the ratio of between-group variance and within-group variance."}
plot_anova_data <- function(mu_A, mu_B, mu_C, sd = 10, n = 20) {
  # create random data
  fict_data <- tibble(
    condition = c(
      rep("A", n),
      rep("B", n),
      rep("C", n)
    ),
    value = c(
      rnorm(n, mu_A, sd),
      rnorm(n, mu_B, sd),
      rnorm(n, mu_C, sd)
    )
  ) 
  # extract F-statistic
  aov_summary<- aov(value ~ condition, fict_data) %>% summary()
  F_observed <- aov_summary[[1]][["F value"]][1]
  fict_data <- fict_data %>% 
    rbind(
      tibble(
        condition = "pooled",
        value = fict_data$value
      )
    )
  fict_data %>% 
    ggplot(
      aes(x = condition, y = value, color = condition)
    ) +
    geom_point(
      fill = "lightgray",
      size =1.5,
      alpha = 0.2
    ) +
    guides(color = "none") +
    geom_segment(size = 2,
      aes(
        x = condition_number - 0.3,
        xend = condition_number + 0.3,
        y = condition_mean,
        yend = condition_mean
      ),
      data = fict_data %>% group_by(condition) %>% 
        summarise(condition_mean = mean(value)) %>% 
        mutate(condition_number = 1:4)
    ) +
    labs(
      x = "",
      y = ""
    ) +
    geom_label(
      aes(x = 1, y = max(value)-5, label = str_c("F = ", signif(F_observed,3))), 
      color = "black"
    ) + theme(
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank()
    )
}

plot_grid(
  plot_anova_data(48,50,52),
  plot_anova_data(45,50,55),
  plot_anova_data(40,50,60),
  plot_anova_data(30,50,70),
  nrow = 2
)

```

It can be shown that, under the assumption that the $k$ groups have identical means, the sampling distribution of the $F$ statistic follows an [$F$-distribution](#app-91-distributions-F) with appropriate parameters (which is, unsurprisingly, the distribution constructed for exactly this purpose):

$$
F \sim F\mathrm{\text{-}distribution}\left(k - 1, \sum_{i=1}^k (n_i - 1) \right)
$$

The complete frequentist model of a one-way ANOVA is shown in Figure \@ref(fig:ch-05-01-ANOVA-onway-model).
Notice that the null hypothesis of equal means is not shown explicitly, but rather only a single mean $\mu$ is shown, which functions as the mean for all groups.

```{r ch-05-01-ANOVA-onway-model, echo = F, fig.cap="Graphical representation of the model underlying a one-way ANOVA.", out.width = '60%'}
knitr::include_graphics("visuals/anova-oneway-model.png")
```

Let's consider some concrete, but fictitious data for a full example:

```{r}
# fictitious data
x_A <- c(78, 43, 60, 60, 60, 50, 57, 58, 64, 64, 56, 62, 66, 53, 59)
x_B <- c(52, 53, 51, 49, 64, 60, 45, 50, 55, 65, 76, 62, 62, 45)
x_C <- c(78, 66, 74, 57, 75, 64, 64, 53, 63, 60, 79, 68, 68, 47, 63, 67)
# number of observations in each group
n_A <- length(x_A)
n_B <- length(x_B) 
n_C <- length(x_C)
# in tibble form
anova_data <- tibble(
  condition = c(
    rep("A", n_A),
    rep("B", n_B),
    rep("C", n_C)
    ),
  value = c(x_A, x_B, x_C)
)
```

Here's a plot of this data:

```{r, echo = F}
anova_data %>% 
  ggplot(
      aes(x = condition, y = value, color = condition)
    ) +
    geom_point(
      fill = "lightgray",
      size =1.5,
      alpha = 0.4
    ) +
    guides(color = "none") +
    geom_segment(size = 2,
      aes(
        x = condition_number - 0.3,
        xend = condition_number + 0.3,
        y = condition_mean,
        yend = condition_mean
      ),
      data = anova_data %>% group_by(condition) %>% 
        summarise(condition_mean = mean(value)) %>% 
        mutate(condition_number = 1:3)
    ) +
    labs(
      x = "",
      y = ""
    )
```

We want to know whether it is plausible to entertain the idea that the means of these three groups are identical.
We can calculate the one-way ANOVA explicitly as follows, following the calculations described in Figure \@ref(fig:ch-05-01-ANOVA-onway-model):

```{r, echo = T}
# compute grand_mean 
grand_mean <- anova_data %>% pull(value) %>% mean()

# compute degrees of freedom (parameters to F-distribution)
df1 <- 2
df2 <- n_A + n_B + n_C - 3

# between-group variance
between_group_variance <- 1/df1 *
  (
    n_A * (mean(x_A) - grand_mean)^2 +
    n_B * (mean(x_B) - grand_mean)^2 +
    n_C * (mean(x_C) - grand_mean)^2  
  )
  
# within-group variance
within_group_variance <- 1/df2 * 
  (
    sum((x_A - mean(x_A))^2) +
    sum((x_B - mean(x_B))^2) +
    sum((x_C - mean(x_C))^2)
  )
# test statistic of observed data
F_observed <-  between_group_variance / within_group_variance

# retrieving the p-value (using the F-distribution)
p_value_anova <- 1 - pf(F_observed, 2, n_A + n_B + n_C - 3) 
p_value_anova %>% round(4)
```

Compare this to the result of calling R's built-in function `aov`:

```{r}
aov(formula = value ~ condition, anova_data) %>% summary()
```

To report these results, we could use a statement like this:

> Based on a one-way ANOVA, we find evidence against the assumption of equal means across all groups ($F(2, `r df2`) \approx 4.485$, $p \approx 0.0172$).

<!-- We have $k \ge 2$ groups of metric observations. For group $1 \le j \le k$, there are $n_j$ observations. Let $x_{ij}$ be the observation $1 \le i \le n_j$ for group $1 \le j \le k$. Let $\bar{x}_j = \frac{1}{n_j} \sum_{i = 1}^{n_j} x_{ij}$ be the mean of group $j$ and let $\bar{\bar{x}} = \frac{1}{k} \sum_{j=1}^k \frac{1}{n_j} \sum_{i=1}^{n_j} x_{ij}$ be the grand mean of all data points. We would like to show that the total sum of squares can be decomposed into two summands: the within-group sum of squares and the between-group sum of squares: -->

<!-- $$  -->
<!-- \underbrace{ -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{\bar{x}})^2 -->
<!-- }_{\text{Total SS}}  =  -->
<!-- \underbrace{ -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2 -->
<!-- }_{\text{Within-Group SS}} + -->
<!-- \underbrace{ -->
<!--   \sum_{j=1}^k n_j (\bar{x}_j - \bar{\bar{x}})^2 -->
<!-- }_{\text{Between-Group SS}} -->
<!-- $$  -->

<!-- To show this, we first establish a lemma, which will also be useful later: -->

<!-- <div class = "mathstuff"> -->

<!-- ```{lemma, label = "SS-cancellation", name = "Sum of squares cancellation"} -->
<!-- Let $\vec{x}$ be a vector of $n$ real-valued numbers, and let $\bar{x} = \frac{1}{n} \sum_{i=i}^n x_i$ be its mean. The sum of squares around the mean is zero: -->
<!-- $$ -->
<!--   \sum_{i=1}^n (x_i - \bar{x}) = 0 -->
<!-- $$ -->

<!-- ``` -->

<!-- <div class="collapsibleProof"> -->
<!-- <button class="trigger">Proof</button> -->
<!-- <div class="content"> -->

<!-- ```{proof} -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!--   \sum_{i=i}^n (x_i - \bar{x})  -->
<!--   & =  -->
<!--   \sum_{i=i}^n (x_i - \frac{1}{n} \sum_{j=1}^n x_j)  -->
<!--   &&  -->
<!--   [\text{by def. of mean}]  -->
<!--   \\ -->
<!--   & =  -->
<!--   \sum_{i=i}^n x_i - \frac{n}{n} \sum_{j=1}^n x_j)  -->
<!--   &&  -->
<!--   [\text{second summand independent of } i]  -->
<!--   \\  -->
<!--   & = 0 -->
<!--   \\  -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- ``` -->

<!-- &nbsp; -->
<!-- </div> -->
<!-- </div> -->
<!-- </div> -->

<!-- <div class = "mathstuff"> -->

<!-- ```{proposition, label = "ANOVA-SS-decomposition", name = "Sum of squares decomposition (ANOVA)"} -->
<!-- If $x_{ij}$ is observation $1 \le i \le n_j$ for group $1 \le j \le k$, $\bar{x}_j = \frac{1}{n} \sum_{i = 1}^{n_j} x_{ij}$ the mean of group $j$ and $\bar{\bar{x}} = \frac{1}{k} \sum_{j=1}^k \frac{1}{n_j} \sum_{i=1}^{n_j} x_{ij}$ be the grand mean of all data points, then: -->
<!-- $$  -->
<!-- \underbrace{ -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{\bar{x}})^2 -->
<!-- }_{\text{Total SS}}  =  -->
<!-- \underbrace{ -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2 -->
<!-- }_{\text{Within-Group SS}} + -->
<!-- \underbrace{ -->
<!--   \sum_{j=1}^k n_j (\bar{x}_j - \bar{\bar{x}})^2 -->
<!-- }_{\text{Between-Group SS}} -->
<!-- $$  -->

<!-- ``` -->

<!-- <div class="collapsibleProof"> -->
<!-- <button class="trigger">Proof</button> -->
<!-- <div class="content"> -->

<!-- ```{proof} -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!--   &  -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{\bar{x}})^2  -->
<!--   \\ -->
<!--   = &   -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j + \bar{x}_j - \bar{\bar{x}})^2 -->
<!--   &&  -->
<!--   [- \bar{x}_j + \bar{x}_j = 0]  -->
<!--   \\ -->
<!--   = &   -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} \left [ (x_{ij} - \bar{x}_j)^2 + 2(x_{ij} - \bar{x}_j)(\bar{x}_j - \bar{\bar{x}}) + (\bar{x}_j - \bar{\bar{x}})^2 \right ] -->
<!--   &&  -->
<!--   [\text{binomial theorem}]  -->
<!--   \\ -->
<!--   = &   -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2 +  -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} (\bar{x}_j - \bar{\bar{x}})^2 +  -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} 2(x_{ij} - \bar{x}_j)(\bar{x}_j - \bar{\bar{x}}) -->
<!--   &&  -->
<!--   [\text{rearranging} ]  -->
<!--   \\ -->
<!--   = &   -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2 +  -->
<!--   \sum_{j=1}^k n_j (\bar{x}_j - \bar{\bar{x}})^2 +  -->
<!--   2 \sum_{j=1}^k (\bar{x}_j - \bar{\bar{x}}) \underbrace{\sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)}_{\text{=0 by lemma}} -->
<!--   &&  -->
<!--   [\text{independences} ]  -->
<!--   \\ -->
<!--     = &   -->
<!--   \sum_{j=1}^k \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2 +  -->
<!--   \sum_{j=1}^k n_j (\bar{x}_j - \bar{\bar{x}})^2 -->
<!--   &&  -->
<!--   [\text{by lemma} ]  -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- ``` -->

<!-- &nbsp; -->

<!-- </div> -->
<!-- </div> -->
<!-- </div> -->

### Linear regression

Significance testing for linear regression parameters follows the same logic as for other models as well.
In particular, it can be shown that the relevant test statistic for ML-estimates of regression coefficients $\hat\beta_i$, under the assumption that the true model has $\beta_i = 0$, follows a $t$-distribution.
We can run a linear regression model (with a Gaussian noise function) using the built-in function `glm` (for "generalized linear model"):

```{r}
fit_murder_mle <- glm(
  formula = murder_rate ~ low_income,
  data = aida::data_murder
)
```

If we inspect a summary for the model fit, we see the results of a $t$-test, one for each coefficient, based on the null-hypothesis that this coefficient's true value is 0.


```{r}
summary(fit_murder_mle)
```


So, in the case of the `murder_data`, we would conclude that there is strong evidence *against* the assumption that the data could have been generated by a model whose slope parameter for `low_income` is set to 0. 

### Likelihood-Ratio Test {#Chap-05-01-LR-test}

The likelihood-ratio (LR) test is a very popular frequentist method of model comparison.
The LR-test assimilates model comparison to frequentist hypothesis testing. 
It defines a suitable test statistic and supplies an approximation of the sampling distribution. 
The LR-test first and foremost applies to the comparison of **nested models**, but there are results about how approximate results can be obtained when comparing non-nested models with an LR-test [@Vuong1989:Likelihood-Rati]. 

A frequentist model $M_i$ is **nested** inside another frequentist model $M_j$ iff $M_i$ can be obtained from $M_j$ by fixing at least one of $M_j$'s free parameters to a specific value. 
If $M_i$ is nested under $M_j$, $M_i$ is called the **nested model**, and $M_j$ is called the **nesting model** or the **encompassing model**.
Obviously, the nested model is simpler (of lower complexity) than the nesting model.

For example, we had the two-parameter exponential model of forgetting previously in Chapter \@ref(Chap-03-06-model-comparison):

$$
\begin{aligned}
P(D = \langle k, N \rangle \mid \langle a, b\rangle) & = \text{Binom}(k,N, a \exp (-bt)), \ \ \ \ \text{where } a,b>0 
\end{aligned}
$$

We wanted to explain the following "forgetting data":

```{r}
# time after memorization (in seconds)
t <- c(1, 3, 6, 9, 12, 18)
# proportion (out of 100) of correct recall
y <- c(.94, .77, .40, .26, .24, .16)
# number of observed correct recalls (out of 100)
obs <- y * 100
```

An example of a model that is nested under this two-parameter model is the following one-parameter model, which fixes $a = 1.1$.

$$
\begin{aligned}
P(D = \langle k, N \rangle \mid b) & = \text{Binom}(k,N, 1.1 \ \exp (-bt)), \ \ \ \ \text{where } b>0 
\end{aligned}
$$

Here's an ML-estimation for the nested nested model (the best fit for the nesting model `bestExpo` was obtained in Chapter \@ref(Chap-03-06-model-comparison)):

```{r, echo = F}
# generic neg-log-LH function (covers both models)
nLL_generic <- function(par, model_name) {
  w1 <- par[1]
  w2 <- par[2]
  # make sure paramters are in acceptable range
  if (w1 < 0 | w2 < 0 | w1 > 20 | w2 > 20) {
    return(NA)
  }
  # calculate predicted recall rates for given parameters
  if (model_name == "exponential") {
    theta <- w1*exp(-w2*t)  # exponential model
  } else {
    theta <- w1*t^(-w2)     # power model
  }
  # avoid edge cases of infinite log-likelihood
  theta[theta <= 0.0] <- 1.0e-4
  theta[theta >= 1.0] <- 1-1.0e-4
  # return negative log-likelihood of data
  - sum(dbinom(x = obs, prob = theta, size = 100, log = T))
}
# negative log likelihood of exponential model
nLL_exp <- function(par) {nLL_generic(par, "exponential")}
# getting a best fit for the exponential model
bestExpo <- optim(nLL_exp, par = c(1, 0.5))
```

```{r}
nLL_expo_nested <- function(b) {
  # calculate predicted recall rates for given parameters
  theta <- 1.1 * exp(-b * t)  # one-param exponential model 
  # avoid edge cases of infinite log-likelihood
  theta[theta <= 0.0] <- 1.0e-4
  theta[theta >= 1.0] <- 1 - 1.0e-4
  # return negative log-likelihood of data
  - sum(dbinom(x = obs, prob = theta, size = 100, log = T))
}

bestExpo_nested <- optim(
  nLL_expo_nested, 
  par = 0.5, 
  method = "Brent", 
  lower = 0, 
  upper = 20
)
bestExpo_nested
```

The LR-test looks at the likelihood ratio of the nested model $M_0$ over the encompassing model $M_1$ using the following test statistic:

$$\text{LR}(M_1, M_0) = -2\log \left(\frac{P_{M_0}(D_\text{obs} \mid \hat{\theta}_0)}{P_{M_1}(D_\text{obs} \mid \hat{\theta}_1)}\right)$$

We can calculate the value of this test statistic for the current example as follows:

```{r}
LR_observed <- 2 * bestExpo_nested$value - 2 * bestExpo$value
LR_observed
```

If the simpler (nested) model is true, the sampling distribution of this test statistic approximates a $\chi^2$-distribution with $d$ if we have more and more data. 
The degrees of freedom $d$ are given by the difference in free parameters, i.e., the number of parameters the nested model fixes to specific values, but which are free in the nesting model. 

We can therefore calculate the $p$-value for the LR-test for our current example like so:

```{r}
p_value_LR_test <- 1 - pchisq(LR_observed, 1)
p_value_LR_test
```

The $p$-value of this test quantifies the evidence against the assumption that the data was generated by the simpler model.
A significant test result would therefore indicate that it would be surprising if the data was generated by the simpler model. 
This is usually taken as evidence in favor of the more complex, nesting model. 
Given the current $p$-value  $p \approx `r p_value_LR_test %>% round(4)`$, we would conclude that there is no strong evidence against the simpler model.
Often this may lead researchers to favor the nested model due to its simplicity; the data at hand does not seem to warrant the added complexity of the nesting model; the nested model seems to suffice.

<div class = "exercises">
**Exercise 16.7**

TRUE OR FALSE?

a. The nested model usually has more free parameters than the nesting model.
b. When we perform the LR-test, we initially assume that the nested model is more plausible.
c. An LR-test can only compare the nested model with nesting models.
d. If the LR-test result has a $p$-value equal to 1.0, one can conclude that it's a piece of evidence in favor of the simpler model. 

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

a. False
b. True
c. False
d. True

</div>
</div>
</div>

