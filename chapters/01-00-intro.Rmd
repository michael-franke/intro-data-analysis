# (PART) Preliminaries {-}

# General Introduction

<hr>

This chapter lays out the learning goals of this book (Section \@ref(Chap-01-00-intro-learning-goals)) and describes how these goals are to be achieved (Section \@ref(Chap-01-00-intro-course-structure)). Section \@ref(Chap-01-00-intro-tools-methods) details which technical tools and methods are covered in the course, and which are not. There will be some information on the kinds of data sets we will use during the course in Section \@ref(Chap-01-00-intro-data-sets). Finally, Section \@ref(Chap-01-00-intro-installation) provides information about how to install the necessary tools for this course.

## Learning goals {#Chap-01-00-intro-learning-goals}

At the end of this course students should:

- have gained the competence to
  - understand complex data sets,
  - manipulate a data set (using R)*, so as to
  - plot aspects of it in ways that are useful for answering a given research question
- understand the general logic of statistical inference, in particular
  - be able to interpret and apply standard analyses from a frequentist and a Bayesian approach
- be able to independently evaluate statistical analyses based on their adequacy for a given research question and data set
- be able to critically assess the adequacy of analyses commonly found in the literature

Notice that this is, although a lot of hard work already, sill rather modest! It doesn't actually say that we necessarily aim at the competence to *do it* or even to *do it flawlessly*! **Our main goal is understanding**, because that is the foundation of practical success *and* the foundation of an ability to learn more in the future. **We do not teach tricks! We do not share recipes!**

## Course structure {#Chap-01-00-intro-course-structure}

The course consists of three parts. After giving a more detailed overview of the course in this chapter, Part I introduces R, the main programming language that we will use. Part II covers what is often called *descriptive statistics*. It also gives us room to learn more about R when we massage data into shape, compute summary statistics, and plot different data types in different ways. Part III is the main theoretical part. It covers what is often called *inferential statistics*. 

A number of characteristic features distinguish this course from the bulk of its cousins out there. 

1. First, we use a **model-centric approach**, i.e., we are going to explicitly represent and talk about statistical models as a formalized set of the assumptions which underly a specific analysis.
2. Second, we will use a **computational approach**, i.e., we foster an understanding of mathematical notions with computer simulations or other variants of helpful code. 
3. Third, this course takes a **dual approach** because it introduces both the frequentist and the Bayesian approach to statistical inference. We will start off with the Bayesian approach, because it is arguably more intuitive. Yet, a model-centric Bayesian approach also helps with understanding basic concepts from the frequentist paradigm.
4. Fourth, the course focuses on **generalized linear models**, a class of models that have become the new standard for analyses of experimental data in the social and psychological sciences. They are also very useful for data exploration in other domains (such as machine learning).

There are also appendices with additional information:

- Further useful material (textbooks, manuals, etc.) is provided in Appendix \@ref(app-90-further-material).
- Appendix \@ref(app-91-distributions) covers the most important probability distributions used in this book.
- An excursion providing more information about the important Exponential Family of probability distributions and the Maximum Entropy Principle is given in Appendix \@ref(app-92-exponential-family).
- The data sets which reoccur throughout the book as "running examples" are succinctly summarized in Appendix \@ref(app-93-data-sets).

## Tools and topics covered (and not covered) in the course {#Chap-01-00-intro-tools-methods}

The main programming language used in this course is [R](https://www.R-project.org/) [@R2018]. We will make heavy use of the *tidyverse* package [@tidyverse2017], which provides a unified set of functions and conventions that deviate (sometimes: substantially) from basic R. We will also be using the [probabilistic programming language WebPPL](http://webppl.org/) [@dippl], but only "passively" in order to quickly obtain results from probabilistic calculations that we can experiment with directly in the browser. We will not learn to write WebPPL code from scratch.

We will rely on the R package [`brms`](https://github.com/paul-buerkner/brms) [@brms2017] for running Bayesian generalized regression models, which itself relies on the probabilistic programming language [`Stan`](https://mc-stan.org/) [@stan2017]. We will, however, not learn about `Stan` in this course. Instead of `Stan` we will use the package [`greta`](https://CRAN.R-project.org/package=greta) [@greta2019] to write our models and do inference with them. This is because, for current learning purposes, the language in which `greta` formulates its models is much closer to R and so, let's hope, easier to learn.

Section \@ref(Chap-01-00-intro-installation) gives information about how to install these, and other tools necessary for this course.

The main topics that this course will cover are:

+ **data preparation**: how to clean up, and massage a data set into an appropriate shape for plotting and analysis

+ **data visualization**: how to select relevant aspects of data to visualize in informative and useful ways

+ **statistical models**: what that is, and why it's beneficial to think in terms of models, not tests

+ **statistical inference**: what that is, and how it's done in frequentist and Bayesian approaches

+ **hypothesis testing**: how Frequentists and Bayesians test scientific hypotheses

+ **generalized regression**: how to apply generalized regression models to different types of data sets

There is, obviously, a lot that we will *not* cover in this course. We will, for instance, not dwell at any length on the specifics of algorithms for computing statistical inferences or model fits. We will also deal with the history and the philosophy of science of statistics only to the extent that it helps to understand the theoretical notions and practical habits that are important in the context of this course. We will also not do heavy math.

Data analysis can be quite varied because data itself can be quite varied. We try to present some variation, but since this is an introductory course with lots of other ground to cover, we will be slightly conservative in the kind of data that we analyze. There will be, for example, no pictures, sounds, dates or time points in any of the material covered here.

There are at least two different motivations for data analysis, and it is important to keep them apart. This course focuses on **data analysis for explanation**, i.e., routines that help us understand reality through the inspection and massaging of empirical data. We will only glance at the alternative approach, which is **data analysis for prediction**, i.e., using models to predict future observations, as commonly practiced in machine learning and its applications. In sloppy slogan form, this course treats data science for scientific knowledge gain, not as engineers' applications.

## Data sets covered {#Chap-01-00-intro-data-sets}

We want to learn how to do data analysis. This is impossible without laying hands on several data sets. But switching from one data set to another is mentally taxing. It is also difficult to focus and to really care about any old data set. This is why this course relies on a small selection of recurring data sets that are, hopefully, generally interesting for the target audience: students of cognitive science. Appendix \@ref(app-93-data-sets) gives an overview of the most important, recurring data sets used in this course.

Most of the data sets that we will use repeatedly in this class come from various psychological experiments. To make this even more emersive, these experiments are implemented as browser-based experiments, using [_magpie](https://magpie-ea.github.io/magpie-site/index.html). This makes it possible for students of this course to actually conduct the exact experiments whose data we will analyze (and maybe generate some more intuitions and some hypotheses). But it also makes it possible that we will analyze ourselves. That's why part of the exercises for this course will run additional analyses on data collected from the aspiring data analysts themselves. If you want to become an analyst, you should also have undergone analysis yourself, so to speak.

## Installation {#Chap-01-00-intro-installation}

This course relies on a few different pieces of software. Primarily,
we'll be using R, but we'll need installations of Python and C++ in
the background.

There are two options for installing. The simplest method, described
in Section \@ref(Chap-01-00-intro-installation-VirtualBox) is to
install [VirtualBox](https://virtualbox.org) and use our provided
Ubuntu virtual machine (link provided in class), which has the
required software pre-installed and tested. When using this method,
you will be working in a virtualized Linux
environment. Alternatively, you can go through a manual installation
tailored to your own OS, as described in Section
\@ref(Chap-01-00-intro-installation-Manual). Manual installation is
recommended if you do not wish to use a virtualized Linux
environment. The VirtualBox method can be a fall-back option if the manual
installation fails. Finally, Section \@ref(Chap-01-00-intro-installation-Updating)
explains how to update the R package that wraps all R packages needed
for this course and provides some extra convenience functions.

### VirtualBox Setup {#Chap-01-00-intro-installation-VirtualBox}

#### Step 1. Install VirtualBox

Follow the instructions
[here](https://www.virtualbox.org/wiki/Downloads) to download and
install for your platform.

#### Step 2. Download our Ubuntu image

Download the provided VirtualBox Disk Image and move it into a folder
such as "VMs". The link for the VirtualBox Disk Image is provided on StudIP.

#### Step 3. Create a new virtual machine and add the downloaded image

* Open VirtualBox and click New

* Give your new virtual machine a name, e.g. "IDA2019"

* Change the Machine Folder to the folder where you put the disk
  image (e.g. inside the folder "VMs")

* Change the Type to "Linux" and Version to "Ubuntu (64-bit)" and
  click Next to proceed to memory allocation

* Allocate about half of your available memory and click Next to
  proceed to hard disk selection

* Choose "Use an existing virtual hard disk file" and use the file
  selection icon to add our provided disk image

* Click Create

#### Step 4. Give your virtual machine more processing power

* Select your virtual machine on the right panel and click Settings
  on the top

* Navigate to System -> Processor

* Increase the Processor(s) to about half of what your computer can provide

#### Step 5. Boot your virtual machine

* Select your virtual machine and click Start

* The username of the system is "user" and the password is "password"

#### Step 6. Install further packages

The virtual machine includes most of the packages required, but not all. 

* First, run the following commands in 'Terminal':

`sudo apt update`

`sudo apt install r-cran-devtools r-cran-boot r-cran-extradistr r-cran-ggsignif r-cran-naniar`

* Then, open RStudio and run the following command in the R console:

`devtools::install_github("n-kall/IDA2019-package")`

#### Troubleshooting

* If the virtual machine does not boot, you may need to ensure that
  'virtualization' is enabled in your computer's BIOS. Talk to a tutor if you're having difficulties doing so.


### Manual installation {#Chap-01-00-intro-installation-Manual}

If you don't want to use the virtual machine, or it doesn't work for
you, the following six steps describe how to get the main components
installed manually. Depending on your operating system (e.g. macOS,
Linux, Windows), you might need to follow slightly different
instructions, which are specified below. Depending on the exact setup of
your computer, the results may vary. The virtual machine has been
tested with the required software, so if you can, we recommend using
that.

#### Step 1. Install Python

**Windows and macOS:**

We recommend installing miniconda from
[here](https://docs.conda.io/en/latest/miniconda.html)

**Linux:**

You can install miniconda or you can just use the preinstalled Python
(which saves time and space). Make sure you have pip installed,
e.g. for Ubuntu `sudo apt install python3-pip`

#### Step 2. Install the required Python packages

We have provided files that list the required Python packages. They
can be installed automatically with the following commands in the
terminal.

**For Anaconda:**

Copy the text body of [this environment
file](https://raw.githubusercontent.com/n-kall/IDA2019-package/master/environment.yml), save it as 'environment.yml', open a terminal, navigate to the folder which contains the downloaded file and run

`conda env create -f environment.yml`


**For Linux users who are using pip:**

Copy the text body of [this requirements file](https://raw.githubusercontent.com/n-kall/IDA2019-package/master/requirements.txt), save it as 'requirements.txt', and run

`pip3 install -r requirements.txt`


#### Step 3. Install R

**Windows and macOS:**

Download and install R from [here](https://ftp.gwdg.de/pub/misc/cran/)

**Linux users:**

We need to have at least version 3.5 of R. This may be available in
your distribution's repository. If you are using a recent version
of Ubuntu (18.10 or later), you can install R with `sudo apt install
r-base`. Otherwise, follow [these
instructions](https://cran.r-project.org/bin/linux/ubuntu/) for your OS
version.

#### Step 4. Install RStudio

**All platforms:**

Download and install the latest version of RStudio from
[here](https://rstudio.com/products/rstudio/download/)

#### Step 5. Install a C++ toolchains

For the Stan language, which will be interfaced through an R package
called brms, you'll need a working C++ compiler.

**Windows:**

Download and install the latest version of RTools from
[here](https://cran.r-project.org/bin/windows/Rtools/)

**macOS:**

Download and install the latest version of RTools for macOS from
[here](https://github.com/rmacoslib/r-macos-rtools/releases/)

Note: you may need to register for an Apple Developer account (free of
charge).

**Linux (e.g. Ubuntu):**

You can install a compiler and toolchain with `sudo apt install
build-essential`.

#### Step 6. Install R packages {#Chap-01-00-intro-installation-packages}

We have created an R package that, when installed, will prompt the
installation of the packages we will use in this course. In this step,
you'll install this package (and automatically install its
dependencies).

**For Windows and macOS:**

Open RStudio and run the following two commands in the console.

`install.packages("devtools")`

`devtools::install_github("n-kall/IDA2019-package")`

**For Linux (e.g. Ubuntu):**

It's much faster (and less error-prone) if you install devtools from
the app repository via terminal: `apt install r-cran-devtools` and
continue to the second line in the R console. But this will only work
if you are using a recent version of Ubuntu (18.10 or later).

If you had to manually update your R to version in step 3, you'll need
to install the following from a terminal:

`apt install libssl-dev libxml2-dev libcurl4-openssl-dev`

and then install via the the R console:

`install.packages("devtools")`

`devtools::install_github("n-kall/IDA2019-package")`


### Updating the course package {#Chap-01-00-intro-installation-Updating}

Occasionally, we might have to add packages or functionality as we go through this course. In that case, you will have to update the package `IDA2019-package` that takes care of all updates. To update, use the following command in the R console:

`devtools::install_github("n-kall/IDA2019-package")`
