# (PART) Applied (generalized) linear modeling {-}

# Simple linear regression {#Chap-04-01-simple-linear-regression}

<hr>

This chapter introduces the basics of (simple) linear regression modeling with one explanatory variable. 
It covers ordinary least-squares (OLS) regression, a maximum-likelihood approach, and finally a Bayesian approach.
Impatient readers can skip to the Bayesian analyses directly, but understanding the OLS and MLE approaches helps to see the bigger (historical) picture and also helps appreciating some of the formal results pertaining to the Bayesian analysis. 
Finally, the chapter addresses how hypotheses about the values of a regression model's parameters (so-called coefficients) can be addressed.

All concrete calculations in this chapter are based on the same running example using the [murder data set](app-93-data-sets-murder-data). 

```{block, type='infobox'}
The learning goals for this chapter are:

- understand what a simple linear regression model is
- see the conceptual differences between OLS, MLE and Bayesian approaches to simple regression
- be able to find best fitting values for OLS and MLE regression using R's built-in functions
- be able to sample *a posteriori* credible values for a simple regression model using the non-informative standard model 
```

## Ordinary least squares regression

This section introduces an ordinary least squares (OLS) linear regression. The main idea is that we look for the best-fitting line in a (multi-dimensional) cloud of points, where "best-fitting" is defined in terms of a geometrical measure of distance (squared prediction error).

### Prediction without any further information

We are interested in explaining or predicting the murder rates in a city using the [murder data set](app-93-data-sets-murder-data).
Concretely, we are interested in whether knowing a city's unemployment rate (stored in variable `unemployment`) helps make better predictions for that city's murder rate (stored in variable `murder_rate`).

Let's first plot the murder rate for every city (just numbered consecutively):

```{r, echo = F}
murder_data %>%
  ggplot(aes(x = 1:20, y = murder_rate)) +
  geom_point() +
  labs(
    x = "n-th city",
    y = "murder rate"
  ) 
```

Suppose we know the vector $y$ of all observed murder rates but we don't know which murder rate belongs to which city.
We are given a city to guess its murder rate.
But we cannot tell cities apart.
So we must guess one number as a prediction for any of the cities. 
What's a good guess?

Actually, how good a guess is depends on what we want to do with this guess (the utility function of a decision problem).
For now, let's just assume that we have a measure of **prediction error** which we would like to minimize with our guesses.
A common measure of **prediction error** uses intuitions about geometric distance and is defined in terms of the **total sum of squares**, where $y$ is the $n$-dimensional vector of observed murder rates and $\xi$ is a single numeric prediction:

$$
\text{TSS}(\xi) = \sum_{i=1}^n (y_i - \xi)^2
$$

This measure of prediction error is what underlies the ordinary least squares approach to simple regression.

It turns out that the **best prediction** we can make, i.e., the number $\hat{\xi} = \arg \min_{\xi} TSS(\xi)$ for which TSS is minimized, is the mean $\bar{y}$ of the original predictions.
So, given the goal of minimizing TSS, our best guess is the mean of the observed murder rates.

<div class = "exercises">

```{proposition, "TSS-solution-mean", name = "Mean minimizes total sum of squares."}

$$
\arg \min_{\xi} \sum_{i=1}^n (y_i - \xi)^2 = \frac{1}{n} \sum_{i=1}^n y_i = \bar{y}
$$ 

```

<div class="collapsibleSolution">
<button class="trigger">Show proof.</button>
<div class="content">

```{proof}

To find a minimum, consider the first derivative of TSS(\xi) and find its zero points:

$$
\begin{align*}
& f(\xi)   = \sum_{i=1}^n (y_i - \xi)^2 = \sum_{i=1}^n (y_i^2 - 2 y_i \xi + \xi^2) \\
& f'(\xi)  = \sum_{i=1}^n (-2y_i + 2\xi) = 0 \\
\Leftrightarrow &  \sum_{i=1}^n -2y_i  = -2 n \xi \\
\Leftrightarrow &  \xi = \frac{1}{n} \sum_{i=1}^n y_i  = \bar{y} \\
\end{align*}
$$

Indeed, the zero point $\xi = \bar{y}$ is a minimum because its second derivative is positive:

$$
f''(\bar{y}) = 2
$$

```

&nbsp;

</div>
</div>
</div>


The plot below visualizes the prediction we make based on the naive predictor $\hat{y}$.
The black dots show the data points, the red line shows the prediction we make (the mean murder rate), the small hollow dots show the specific predictions for each observed value and the gray lines show the distance between our prediction and the actual data observation.

```{r echo = F}
mean_y <- murder_data %>% pull(murder_rate) %>% mean()
murder_data %>% 
  ggplot(aes(x = 1:20, y = murder_rate)) +
  geom_segment(
    aes(
      x = 1:20, 
      y = murder_rate,
      xend = 1:20, 
      yend = mean_y
    ),
    color = "lightgray"
  ) +
  geom_abline(slope = 0, intercept = mean_y, color = "firebrick") +
  geom_point(aes(y = mean_y), shape = "O", alpha = 0.5) +
  geom_point() +
  labs(
    x = "n-th city",
    y = "murder rate"
  ) 
```

To obtain the TSS for the prediction shown in the plot above, we would need to take each gray line, measure it's distance, square this number and sum over all lines (cities).
In the case at hand, the prediction error we make by assuming just the mean as predictor is:

```{r}
y <- murder_data %>% pull(murder_rate)
n <- length(y)
tss_simple <- sum((y - mean(y))^2)
tss_simple
```

At this stage, a question might arise:
Why square the distances to obtaine the total sum of, well, *squares*?
One intuitive motivation is that we want small deviations from our prediction to have less overall impact than huge deviations.
A technical motivation is that the best solution to OLS estimation corresponds to the best solution under a maximul likelihood approach, if we use a normal distribution as likelihood function.
This is what we will cover in the next section after having introduced the regression model in full.

### Prediction with knowledge of unemployment rate

We might not be very content with this prediction error. Suppose we could use some piece of information about the random city whose murder rate we are trying to predict. E.g., we might happen to know the value of the variable `unemployment`. How could that help us make a better prediction?

There does seem to be some useful information in the unemployment rate, which may lead to better predictions of the murder rate. We see this in a scatter plot:

```{r, echo = F}
murder_data %>% 
  ggplot(aes(x = unemployment, y = murder_rate)) +
  geom_point() +
  labs(y = "murder rate")
```


Let us assume, for the sake of current illustration, that we expect a very particular functional relationship between the variables `murder_rate` and `unemployment`. For some reason or other, we hypothesize that even with 0% unemployment, the murder rate would be positive, namely at 4 murders per million inhabitants. We further hypothesize that with each increase of 1% in the unemployment percentage, the murder rate per million increases by 2. The functional relationship between dependent variable $y$ (= murder rate) and predictor variable $x$ (= unemployment) can then be expressed as a linear function of the following form, where $\xi$ is now a vector of predictions (one prediction $\xi$ for each data observation $y_i$):^[The predictor vector for simple linear regression is often written as $\hat{y}$. The notation in terms of a linear predictor $\xi$ is useful for later extensions to generalized linear regression models.]

$$
\xi_i = 2x_i + 4
$$

Here is a graphical representation of this particular functional relationship assumed in the equation above. Again, the black dots show the data points, the red line the linear function $f(x) = 2x +4$, the small hollow dots show the specific predictions for each observed value $x_i$ and the gray lines show the distance between our prediction and the actual data observation. (Notice that there are data points for which the unemployment rate is the same, but we observed different murder rates.)

```{r, echo = F}
murder_data %>% 
  ggplot(aes(x = unemployment, y = murder_rate)) +
    geom_segment(
    aes(
      x = unemployment, 
      y = murder_rate,
      xend = unemployment, 
      yend = 2*unemployment + 4
    ),
    color = "lightgray"
  ) +
  geom_point() +
  geom_abline(slope = 2, intercept = 4, color = "firebrick") +
  geom_point(aes(y = 2*unemployment + 4), shape = "O", alpha = 0.5) +
  labs(y = "murder rate")
```

We can again quantify our prediction error in terms of a sum of squares like we did before. For the case of a prediction vector $\xi$, the quantity in question is called the **residual sum of squares**.

$$
\text{RSS} = \sum_{i=1}^n (y_i - \xi)^2
$$

Here is how we can calculate RSS in R for the particular vector $\xi_{i} = 2x_i + 4$:

```{r}
y <- murder_data %>% pull(murder_rate)
x <- murder_data %>% pull(unemployment)
predicted_y <- 2 * x + 4
n <- length(y)
rss_guesswork <- sum((y - predicted_y)^2)
rss_guesswork
```

Compared to the previous prediction, which was based on the mean $\bar{y}$ only, this linear function reduces the prediction error (measured here geometrically in terms of a sum of squares).
This alone could be taken as *prima facie* evidence that knowledge of `unemployment`` helps make better predictions about `murder_rate`.


<div class = "exercises">
**Exercise 13.1 [optional]**

Compare RSS and TSS. How / where exactly do these notions differ from each other? Think about which information the difference between the two measures conveys.

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

TSS computes the distance between a data point and the overall mean of all data points, whereas RSS computes the distance between a data point and a predictor value specific to this data point. 
The difference between RSS and TSS tells us how good our prediction is in comparison to a naive prediction (using just the mean). 

</div>
</div>
</div>

### Simple linear regression: general problem formulation

Suppose we have $k$ predictor variables $x_1, \dots , x_k$ and a dependent variable $y$. 
We consider the simple linear relation (where the hat on top of vector $y$ symbolizes that this is a vector of predicted $y$ values): 

$$ \hat{y}_i = \beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki}$$

The parameters $\beta_0, \beta_1, \dots, \beta_k$ of this equation are called **regression coefficients**. In particular, $\beta_0$ is called the **regression intercept** and $\beta_1, \dots, \beta_k$ are **regression slope coefficients**. 
Based on the predictions of a parameter vector $\langle \hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_k\rangle$, we consider the residual sum of squares as a measure of prediction error:

$$\text{RSS}_{\langle {\beta}_0, {\beta}_1, \dots, {\beta}_k\rangle} = \sum_{i = 1}^k [y_i - \hat{y}_i ({\beta}_0, {\beta}_1, \dots, {\beta}_k) ]^2 $$

We would like to find the best parameter values (denoted traditionally by a hat on the parameter's variable: $\hat{\beta}_i$) in the sense of minimizing the residual sum of squares:

$$
\langle \hat{\beta}_0, \hat{\beta}_1, \dots  , \hat{\beta}_k\rangle = \arg \min_{\langle \beta_0, \beta_1, \dots, \beta_k\rangle} \text{RSS}_{\langle {\beta}_0, {\beta}_1, \dots, {\beta}_k\rangle}
$$

<div class = "exercises">
**Exercise 13.2**
How many coefficients does the naive model have? How can they be interpreted?

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">
The naive model has only $\beta_0$ as its coefficient with it being the mean of $y$ values, i.e., $\hat{\beta}_0$ = $\bar{y}$.
</div>
</div>
</div>

In the above example, where we regressed `murder_rate` against `unemployment`, the model has two regression coefficients: an intercept term and a slope for `unemployment`. The optimal solution for these (see next section) delivers the regression line in the graph below:

```{r, echo = F}
lm_fit_murder <- lm(murder_rate ~ unemployment, data = murder_data)
intercept_simple_murder <- lm_fit_murder$coef[1]
slope_simple_murder <- lm_fit_murder$coef[2]
murder_data %>% 
  ggplot(aes(x = unemployment, y = murder_rate)) +
    geom_segment(
    aes(
      x = unemployment, 
      y = murder_rate,
      xend = unemployment, 
      yend = slope_simple_murder * unemployment + intercept_simple_murder
    ),
    color = "lightgray"
  ) +
  geom_point() +
  geom_abline(slope = slope_simple_murder, intercept = intercept_simple_murder, color = "firebrick") +
  geom_point(aes(y = slope_simple_murder * unemployment + intercept_simple_murder), shape = "O", alpha = 0.5) +
  labs(y = "murder rate")
```


The total sum of squares for the best fitting parameters is:

```{r, echo = F}
y <- murder_data %>% pull(murder_rate)
x <- murder_data %>% pull(unemployment)
predicted_y <- slope_simple_murder * x + intercept_simple_murder
n <- length(y)
tss_best <- sum((y - predicted_y)^2)
tss_best
```

### Finding the OLS-solution

In the following, we discuss several methods of finding the best-fitting values for regression coefficients that minimize the residual sum of squares.

#### Finding optimal parameters with `optim`

We can use the `optim` function to find the best-fitting parameter values for simple linear regression. Here is an example based on the murder data. 

```{r, message = T}
# data to be explained / predicted
y <- murder_data %>% pull(murder_rate)
# data to use for prediction / explanation
x <- murder_data %>% pull(unemployment)
# function to calculate residual sum of squares
get_rss = function(y, x, beta_0, beta_1) {
  yPred = beta_0 + x * beta_1
  sum((y-yPred)^2) 
}
# finding best-fitting values for TSS
fit_rss = optim(par = c(0, 1),  # initial parameter values
  fn = function(par) {  # function to minimize
    get_rss(y, x, par[1], par[2])
  }
)
# output the results
message(
  "Best fitting parameter values:",
  "\n\tIntercept: ", fit_rss$par[1] %>% signif(5),
  "\n\tSlope: ", fit_rss$par[2] %>%  signif(5),
  "\nRSS for best fit: ", fit_rss$value %>% signif(5)
)
```

#### Fitting OLS regression lines with `lm`

R also has a built-in function `lm` which fits (simple) linear regression models via RSS minimization. Here is how you call this function for the running example:

```{r}
# fit an OLS regression
fit_lm <- lm(
  # the formula argument specifies dependent and independent variables
  formula = murder_rate ~ unemployment,
  # we also need to say where the data (columns) should come from
  data = murder_data
)
# output the fitted object
fit_lm
```

The output of the fitted object shows the best-fitting values (compare them to what we obtained by hand). It also shows the function call by which this fit was obtained. There is more information in the object `fit_lm` and we will return to this later when we consider hypothesis testing on regression coefficients. But it might be interesting to take a quick preview already:

```{r}
summary(fit_lm)
```

#### Finding optimal parameter values with math

It is also possible to determine the OLS-fits by a mathematical derivation. We start with the case of a simple linear regression with just one predictor variable.

<div class = "exercises">

```{theorem, "OLS-Solution", name = "OLS solution"}
For a simple linear regression model with just one predictor for a data set with $n$ observations, the solution for:

$$\arg \min_{\langle \beta_0, \beta_1\rangle} \sum_{i = 1}^n (y_i - (\beta_0 + \beta_1 x_{i}))^2$$
  
is given by:
  
$$
\begin{aligned}
\hat{\beta_1} &= \frac{Cov(x,y)}{Var(x)} & 
\hat{\beta_0} &= \bar{y} - \hat{\beta}_1 \bar{x} 
\end{aligned}
$$

```


<div class="collapsibleSolution">
<button class="trigger">Show proof.</button>
<div class="content">

```{proof}
*[See e.g., @kirchner2003, pp. 1-3; @olive2017, pp. 57-59]*

Given a set of $n$ observations $(X_i,Y_i)$ (or points on a scatter plot), we want to find the best-fit line,
$$\hat y_i=\hat\beta_0+\hat\beta_1x_i,\tag{1.1.1}$$
such that the sum of squared errors (RSS) in $Y$ is minimized:

$$RSS=\sum_{i=1}^n (y_i - \hat{y}_i)^2 \rightarrow min.\tag{1.1.2}$$

Let the *Residual Sum of Squares (RSS)*  be denoted as $Q$ with,

$$\begin{align}
Q=RSS&=\sum_{i=1}^{n}(y_i-\hat y_i)^2\\ &=\sum_{i=1}^{n}(y_i-\hat\beta_0-\hat\beta_1x_i)^2.
\tag{1.1.3}
\end{align}$$

We want to minimize $Q$ (that is minimizing *RSS*) at the values of $\hat\beta_0$ and $\hat\beta_1$ for which $\frac{\partial Q}{\partial \hat\beta_0}=0$ (1) and $\frac{\partial Q}{\partial \hat\beta_1}=0$ (2), since all partial derivatives equal to 0 at the global minimum.

The first condition (1) is,

$$ \begin{align} \frac{\partial Q}{\partial \hat\beta_0}=\sum_{i=1}^{n}-2(y_i-\hat\beta_0-\hat\beta_1x_i)&= 0\\
&=-\sum_{i=1}^ny_i+\sum_{i=1}^n\hat \beta_0+\sum_{i=1}^n\hat\beta_1x_i\\
&=-\sum_{i=1}^ny_i+n\hat\beta_0+\sum_{i=1}^n\hat\beta_1x_i
\tag{1.1.4}
\end{align}$$


which, if we solve for $\hat\beta_0$, becomes

$$\begin{align}
\hat\beta_0&=\frac{1}{n}\sum_{i=1}^{n}y_i-\frac{1}{n}\hat\beta_1\sum_{i=1}^{n}x_i\\
&=\bar y - \hat\beta_1\bar x,
\tag{1.1.5}
\end{align}$$

which says that the constant $\hat\beta_0$ (the y-intercept) is set such that the line must go through the mean of $x$ and $y$. This makes sense because this point is the "center" of the data cloud.

The solution is indeed a minimum as the second partial derivative is positive:

$\frac{\partial^2 Q}{\partial\hat\beta_0^2}=2n>0. \tag{1.1.6}$

The second condition (2) is,

$$ \begin{align}
\frac{\partial Q}{\partial \hat\beta_1}=\sum_{i=1}^{n}-2x_i(y_i-\hat\beta_0-\hat\beta_1x_i)&=0\\
&=\sum_{i=1}^{n}(-x_iy_i+\hat\beta_0x_i+\hat\beta_1x_i^2)\\
&=-\sum_{i=1}^{n}x_iy_i+\hat\beta_0\sum_{i=1}^{n}x_i+\hat\beta_1\sum_{i=1}^{n}x_i^2
\tag{1.1.7}
\end{align}$$

If we substitute $\hat\beta_0$ by (1.1.5), we get,

$$ \begin{align}
0&=-\sum_{i=1}^{n}x_iy_i+(\bar y - \hat\beta_1\bar x)\sum_{i=1}^{n}x_i+\hat\beta_1\sum_{i=1}^{n}x_i^2\\
&=-\sum_{i=1}^{n}x_iy_i+\bar y\sum_{i=1}^{n}x_i-\hat\beta_1\bar x\sum_{i=1}^{n}x_i+\hat\beta_1\sum_{i=1}^{n}x_i^2
\tag{1.1.8}
\end{align}$$

separating this into two sums,

$$ \sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)-\hat\beta_1\sum_{i=1}^{n}\left(x_i^2-x_i\bar x\right)=0 \tag{1.1.9}$$

becomes,

$$ \hat\beta_1 = \frac{\sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)}{\sum_{i=1}^{n}\left( x_i^2-x_i\bar x\right)} = \frac{\sum_{i=1}^{n}\left( x_iy_i\right)-n\bar x\bar y}{\sum_{i=1}^{n}\left( x_i^2\right)-n \bar x^2} \tag{1.1.10}$$

The model assumes that the deviation from the values from the mean is zero, so that the positive and negative values are in balance, thus

$$ \sum_{i=1}^{n}\left( \bar x^2-x_i\bar x\right)=0, \tag{1.1.11}$$

and

$$ \sum_{i=1}^{n}\left(\bar x \bar y - y_i \bar x\right)=0. \tag{1.1.12}$$

This can be used in order to expand the previous term and finally to rewrite $\hat\beta_1$ as the ratio of $Cov(x,y)$ to $Var(x)$:

$$
\begin{align}
\hat\beta_1&=\frac{\sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)+\sum_{i=1}^{n}\left(\bar x\bar y - y_i \bar x\right)}{\sum_{i=1}^{n}\left( x_i^2-x_i\bar x\right)+\sum_{i=1}^{n}\left( \bar x^2-x_i\bar x\right)}=\frac{\sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)+0}{\sum_{i=1}^{n}\left( x_i^2-x_i\bar x\right)+0}\\
\\
&=\frac{\frac{1}{n}\sum_{i=1}^{n}\left( x_i-\bar x\right) \left(y_i- \bar y \right)}{\frac{1}{n}\sum_{i=1}^{n}\left( x_i-\bar x\right)^2}\\
\\
&=\frac{Cov(x,y)}{Var(x)}.
\tag{1.1.13}
\end{align}$$

The solution is indeed a minimum as the second partial derivative is positive:

$$\frac{\partial^2Q}{\partial \hat\beta_1^2}= 2 \sum_{i=1}^{n}x_i^2 >0. \tag{1.1.14}$$
```

&nbsp;


</div>
</div>
</div>


Let's use these formulas to calculate regression coefficients for the running example as well:

```{r}
tibble(
  beta_1 = cov(x,y) / var(x),
  beta_0 = mean(y) - beta_1 * mean(x)
)
```

A similar result also exists for regression with more than one predictor variable.
For a more compact representation, we will use matrix notation in the following.
Traditionally, we consider a so-called design matrix $X$ of size $n \times k+1$, where $n$ is the number of observations in the data set and $k$ is the number of predictor variables.
The design matrix also includes a column $X_0$ for which $X_{i0}=1$ for all $1 \le i \ne n$ so that the intercept $\beta_0$ can be treated on a par with the other regression coefficients.


$$
\hat{\beta} = \langle \hat{\beta}_0, \hat{\beta}_1, \dots  , \hat{\beta}_k\rangle = \arg \min_{\beta} \sum_{i = 1}^k \left(y_i - \sum_{j=0}^k \left( \beta_j x_{ji} \right) \right)^2
$$
<div class = "exercises">

```{theorem, "OLS-Solution-general", name = "OLS general"}
Let $X$ be the $n \times (k+1)$ regression matrix for a simple linear regression model with $k$ predictor variables for a data set $y$ with $n$ observations. The solution for OLS regression

$$
\hat{\beta} = \langle \hat{\beta}_0, \hat{\beta}_1, \dots  , \hat{\beta}_k\rangle = \arg \min_{\beta} \sum_{i = 1}^k (y_i - \sum_{j=0}^k(\beta_j X_{ij}))^2
$$
  
is given by:
  
$$
\hat{\beta} = (X^T \ X)^{-1}\ X^Ty
$$

```

<div class="collapsibleSolution">
<button class="trigger">Show proof.</button>
<div class="content">

```{proof}
```

With $n$ observations, the vector $y$ of predicted values for given coefficient vector $\beta$ is:

$$
\begin{align*}
y_1&=\beta_0 + \beta_{1} X_{11}+\beta_2 X_{12} + \ldots + \beta_k X_{1k}\\
y_2&=\beta_0 + \beta_{1} X_{21}+\beta_2 X_{22} + \ldots + \beta_k X_{2k}\\
\ldots\\
y_n&=\beta_0 + \beta_{1} X_{n1}+\beta_2 X_{n2}+ \ldots + \beta_k X_{nk}
\end{align*}
$$

This can also be expressed in matrix notation:

$$
\begin{bmatrix} 
  y_1 \\
  y_2 \\ 
  \ldots \\ 
  y_n 
\end{bmatrix} 
= 
\begin{bmatrix}
  1 & X_{11} &  X_{12} & \ldots  & X_{1k} \\
  1 & X_{21} &  X_{22} & \ldots  & X_{2k} \\
  \ldots  &  \ldots  & \ldots  & \ldots  & \ldots \\
  1 & X_{n1} & X_{n2} & \ldots  & X_{nk}
\end{bmatrix} 
\begin{bmatrix}
  \beta_0 \\
  \beta_1 \\
  \ldots \\
  \beta_k 
\end{bmatrix}
$$

And more compactly:

$$
y=X \beta
$$

The OLS estimator is obtained (like in the special case) by minimizing the residual sum of squares (RSS).
The RSS for the multiple linear regression model is

$$
Q=\sum_{i=1}^n \left(y_i-\beta_0 - \beta_1 X_{i1}- \beta_2 X_{i2}-...-\beta_k X_{ik}\right)^2
$$

To find the minimum of $Q$ we calculate the first partial derivative of $Q$ for each $\beta_j$in the expression:

$$\begin{align}
\frac{\partial Q}{\partial\beta_0}&=2\sum_{i=1}^n\left(y_i-\beta_0-\beta_1 X_{i1}-\beta_2 X_{i2}- \ldots -\beta_k X_{ik}\right)(-1)\\
\\
\frac{\partial Q}{\partial\beta_1}&=2\sum_{i=1}^n\left(y_i-\beta_0-\beta_1 X_{i1}-\beta_2 X_{i2}- \ldots -\beta_k X_{ik}\right)(- X_{i1})\\
\\
\frac{\partial Q}{\partial\beta_2}&=2\sum_{i=1}^n\left(y_i-\beta_0-\beta_1 X_{i1}-\beta_2 X_{i2}- \ldots -\beta_k X_{ik}\right)(- X_{i2})\\
 \ldots \\
\frac{\partial Q}{\partial\beta_k}&=2\sum_{i=1}^n\left(y_i-\beta_0-\beta_1 X_{i1}-\beta_2 X_{i2}- \ldots -\beta_k X_{ik}\right)(- X_{ik})
\end{align}$$

For the minimium $\hat{\beta}$ the derivative of each equation must be zero:

$$\begin{align}
&\sum_{i=1}^n\left(y_i-\hat\beta_0-\hat\beta_1 X_{i1}-\hat\beta_2 X_{i2}- \ldots -\hat\beta_k X_{ik}\right) = 0\\
&\sum_{i=1}^n\left(y_i-\hat\beta_0-\hat\beta_1 X_{i1}-\hat\beta_2 X_{i2}- \ldots -\hat\beta_k X_{ik}\right) X_{i1} = 0\\
&\sum_{i=1}^n\left(y_i-\hat\beta_0-\hat\beta_1 X_{i1}-\hat\beta_2 X_{i2}- \ldots -\hat\beta_k X_{ik}\right) X_{i2} = 0\\
& \ldots \\
&\sum_{i=1}^n\left(y_i-\hat\beta_0-\hat\beta_1 X_{i1}-\hat\beta_2 X_{i2}- \ldots -\hat\beta_k X_{ik}\right) X_{ik} = 0
\end{align}$$

Alternatively, we can use matrix notation and combine the above equations into the following form:

$$X'y-X'X\hat\beta=0$$

Rearranging this, the following expression is known as **normal equations**:

$$X'X\hat\beta=X'y$$

Just for illustration, the system of normal equations in expanded matrix notation is:

$$
\begin{bmatrix} 
n & \sum_{i=1}^n X_{i1} & ... & \sum_{i=1}^n X_{ik}\\
\sum_{i=1}^n X_{i1} & \sum_{i=1}^n X_{i1}^2 & ... & \sum_{i=1}^n X_{i1} X_{ik}\\... & ... & ... & ...\\
\sum_{i=1}^n X_{ik} & \sum_{i=1}^n X_{ik} X_{i1} & ... & \sum_{i=1}^n X_{ik}^2
\end{bmatrix}
\begin{bmatrix}
\hat\beta_0 \\
\hat\beta_1 \\
\ldots \\
\hat\beta_k
\end{bmatrix}
=
\begin{bmatrix}
\sum_{i=1}^ny_i\\\sum_{i=1}^n X_{i1}y_i \\
\ldots \\
\sum_{i=1}^nX_{ik}y_i
\end{bmatrix}
$$

The estimator $\hat\beta$ can be obtained by rearranging again:

$$
\mathbf{\hat\beta}=[\mathbf{X'X}]^{-1}\mathbf{X'Y}
$$

Finally, to see that $\hat\beta$ is indeed a global minimizer of the OLS criterion, we check that the second order condition is always a semidefinite positive matrix:

$$\frac{\partial^2 Q}{\partial \mathbf{\hat\beta}^2}=2X'X >0.$$


&nbsp;

</div>
</div>
</div>



The availability of these elegant mathematical solutions for OLS-regression explains why the computation of best-fitting regression coefficients with a built-in function like `lm` is lightning fast: it does not rely on optimization with `optim`, sampling methods or other similar computational approaches. Instead, it instantaneously calculates the analytical solution.

## A maximum-likelihood approach

In order to be able to extend regression modeling to predictor variables other than metric variables (so-called generalized linear regression models), the geometric approach needs to be abandoned in favor of a likelihood-based approach. The likelihood-based approach tries to find coefficients that explain the observed data most plausibly. 

<!-- TODO -->
<!-- https://medium.com/quick-code/maximum-likelihood-estimation-for-regression-65f9c99f815d -->
<!-- Include the second figure if allowed by the author. Caption: In MLE approach, each predicted $y$ value decides mean of the normal distribution. Since the likelihood $f(y_i|\theta)$ is inversely proportional to the residual $\epsilon_i$, $\theta$ should be chosen such that sum of $\epsilon_i$ is minimised. -->

### A likelihood-based model

There are two equivalent formulations of a (simple) linear regression model using a likelihood-based approach. The first is more explicit, showing clearly that the model assumes that for each observation $y_i$ an error term $\epsilon_i$, which is an iid sample from a Normal distribution. (Notice that the likelihood-based model assumes an additional parameter $\sigma$, the standard deviation of the error terms.)

$$ 
\text{likelihood-based regression }
\text{[explicit version]}
$$

$$
\begin{aligned}
y_{\text{pred},i} & = \beta_0 + \beta_1 x_i  \\
y_i & = y_{\text{pred},i} + \epsilon_i \\
\epsilon_i & \sim \text{Normal}(0, \sigma)  \\
\end{aligned}
$$

The second, equivalent version of this writes this more compactly, suppressing the explicit mentioning of iid error terms:

$$ 
\text{likelihood-based regression }
\text{[compact version]}
$$

$$
\begin{aligned}
y_i & \sim \text{Normal}(\beta_0 + \beta_1 x_i, \sigma)
\end{aligned}
$$

### Finding the MLE-solution with `optim`

We can use `optim` to find maximum likelihood estimates:

```{r, message = T}
# data to be explained / predicted
y <- murder_data %>% pull(murder_rate)
# data to use for prediction / explanation
x <- murder_data %>% pull(unemployment)
# function to calculate negative log-likelihood
get_nll = function(y, x, beta_0, beta_1, sd) {
  if (sd <= 0) {return( Inf )}
  yPred = beta_0 + x * beta_1
  nll = -dnorm(y, mean=yPred, sd=sd, log = T)
  sum(nll)
}
# finding MLE
fit_lh = optim(par = c(0, 1, 1), 
  fn = function(par) {
    get_nll(y, x, par[1], par[2], par[3])
  }
)
# output the results
message(
  "Best fitting parameter values:",
  "\n\tIntercept: ", fit_lh$par[1] %>% signif(5),
  "\n\tSlope: ", fit_lh$par[2] %>%  signif(5),
  "\nNegative log-likelihood for best fit: ", fit_lh$value %>% signif(5)
)

```

### Finding the MLE-solution with math

It is no coincidence that these fitted values are (modulo number imprecision) the same as for the geometric OLS approach.
<div class = "exercises">

```{theorem, "MLE-Solution", name = "MLE solution"}
For a simple linear regression model with just one predictor, the solution for:

$$\arg \max_{\langle \beta_0, \beta_1, \sigma \rangle} \prod_{i = 1}^k \text{Normal}(y_i | \mu = \beta_0 + \beta_1 x_{i}, \sigma)$$
  
is the same as for the OLS approach:
  
$$
\begin{aligned}
\hat{\beta_1} &= \frac{Cov(x,y)}{Var(x)} & 
\hat{\beta_0} &= \bar{y} - \hat{\beta}_1 \bar{x} 
\end{aligned}
$$

```

<div class="collapsibleSolution">
<button class="trigger">Show proof.</button>
<div class="content">

```{proof}
*[see e.g., @naveen2019; @croot2010; @eppes2019]*

We consider again the linear regression model of the population with:

$$Y= \beta_0 + \beta_1X + \epsilon.\tag{2.1}$$

This simplifies to the following form on the observed data:

$$y= \beta_0 + \beta_1x + \epsilon.\tag{2.2}$$

Using a sample in order to obtain the **maximum likelihood estimates** the equation simplifies to:

$$y= \hat\beta_0 + \hat\beta_1x. \tag{2.3}$$

**Assumptions** that we make for the model:

- The true underlying distribution of the errors is Gaussian.
- The expected value of the error term is 0.
- The variance of the error term is constant with respect to x.
- The 'lagged' errors are independent of each other, where the error term is normally distributed.

We can thus write:

$$ \epsilon \sim N(0,\sigma^2).\tag{2.4}$$

Since $Y$ is a linear function of $\epsilon$ it will also be normally distributed.

$$f(\epsilon|\beta_0,\beta_1)= \frac{1}{\sqrt{2\pi} \sigma} \exp\left[{-\frac{1}{2}\left(\frac{\epsilon^2}{\sigma^2}\right)}\right]. \tag{2.5}$$

Given the whole data set with $i=1,...,n$ observations, the **likelihood function** ($LH$) is the joint density of all the observations, given a value for the parameters $\beta_0$ and $\beta_1$. Since independence is assumed, this is simply the product of the individual densities from the previous equation.

$$LH(\epsilon_i|\beta_0,\beta_1) =\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi} \sigma} \exp\left[{-\frac{1}{2}\left(\frac{\epsilon_i^2}{\sigma^2}\right)}\right].\tag{2.6}$$

In order to find the maximum of (2.6), we have to find the first derivative.

But as the derivation of a product with a lot of factors is inconvenient, we first take the logarithm of the likelihood, called **log-likelihood**. This is possible as $\log$ is a monotone transformation and the maximum likelihood estimate does not change on log transformation.

The log-likelihood is the sum of the logs of the individual densities:

$$\begin{align}
LL&=\log \left(LH(\epsilon_i|\beta_0,\beta_1)\right)\\
&=-\left( \frac{n}{2}\right) \log(2\pi)-\left( \frac{n}{2}\right) \log(\sigma^2)-\left( \frac{1}{2}\sigma^2\right) \sum_{i=1}^n(\epsilon_i)^2
\tag{2.7}
\end{align}$$

The log-likelihood can be used in order to find the *Maximum Likelihood estimates* $\hat\beta_0$ and $\hat\beta_1$ for the parameters $\beta_0$ and $\beta_1$.


$$\hat\beta_0,\hat\beta_1=\mathrm{argmax}_{\beta_0,\beta_1}LL=\mathrm{argmax}_{\beta_0,\beta_1}\left[-\left( \frac{n}{2}\right) \log(2\pi)-\left( \frac{n}{2}\right) \log(\sigma^2)-\left( \frac{1}{2}\sigma^2\right) \sum_{i=1}^n(\epsilon_i)^2\right].\tag{2.8}$$

Removing the constant terms results in:

$$\hat\beta_0,\hat\beta_1=\mathrm{argmax}_{\beta_0,\beta_1} \sum_{i=1}^n-\epsilon_i^2.\tag{2.9}$$

Substituting $\epsilon$, derived from (2.2), gives:

$$\hat\beta_0,\hat\beta_1=\mathrm{argmax}_{\beta_0,\beta_1} \sum_{i=1}^n-(y-\beta_0-\beta_1x)^2.\tag{2.10}$$

**Conclusion:**

Deriving parameter estimates according to the OLS method:

$$Q_{OLS}=\sum_{i=1}^{n}(y_i-\hat\beta_0-\hat\beta_1x_i)^2 \rightarrow min.$$

Deriving parameter estimates according to the ML method:

$$Q_{ML}=\sum_{i=1}^n-(y_i-\hat\beta_0-\hat\beta_1x_i)^2 \rightarrow max.$$

Maximizing $-z$ is equivalent to minimizing $z$, thus, the best parameter fit under ML is equivalent to the best fit under OLS.
```

&nbsp;

</div>
</div>
</div>


The equivalence also extends to cases with more than one explanatory variable.

<div class = "exercises">
**Exercise 13.3**

Let's assume that following the MLE approach, we obtained $\beta_0 = 1$, $\beta_1 = 2$ and $\sigma = 0.5$. For $x_i = 0$, which $y_i$ value will maximize the likelihood?

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">
Since $y_i \sim \text{Normal} (\beta_0 + \beta_1 x_i , \sigma)$, the likelihood is maximized at the mean of the normal distribution, i.e., $y_i = \beta_0 + \beta_1 x_i = 1$.
</div>
</div>
</div>

### Finding the MLE-solution with `glm`

R also has a built-in way of approaching simple linear regression with a maximum-likelihood approach, namely by using the function `glm` (generalized linear model). Notice that the output looks slightly different from that of `lm`.

```{r}
fit_glm <- glm(murder_rate ~ unemployment, data = murder_data)
fit_glm
```

We might also risk a peek at the summary of `fit_glm`:

```{r}
summary(fit_glm)
```

<div class = "exercises">
**Exercise 13.4**

Compare the output of `fit_lm` on the last page and `fit_glm` on this page. Why are the estimates of coefficients the same?

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">
By default, `glm` assumes a normal link function when it is not specified. `lm` can be seen as a more specific version of `glm` where the link function is fixed to be a normal distribution.
</div>
</div>
</div>

## A Bayesian approach

A Bayesian model for (simple) linear regression looks very much like the previous likelihood-based model, just that it also adds prior information. We have already seen a Bayesian linear regression model in Chapter \@ref(Chap-03-03-models-examples-linear-regression). It is repeated here in Figure \@ref(fig:ch-04-01-Simple-Linear-Regression-repeated).

<!-- TODO -->
<!-- Alternative formulation: In frequentist approach, the best fitting parameters are used to fit a single line. In Bayesian approach, multiple lines are fitted with varying degree of plausibility. -->

<!-- ```{r ch-04-01-Bayesian-linear-regression, echo = F, out.width = '80%', fig.cap="Bayesian linear regression fits multiple lines."} -->
<!-- knitr::include_graphics("visuals/bayesian_linear_reg.png") -->
<!-- ``` -->
<!-- Figure is from ABDA Week 03 bonus I -->


```{r ch-04-01-Simple-Linear-Regression-repeated, echo = F, out.width = '80%', fig.cap="Bayesian Simple Linear Regression Model (repeated from before)."}
knitr::include_graphics("visuals/linear-regression-model.png")
```

### Implementation in `Stan`

Here is an implementation of a Bayesian regression model for the running example murder data using `Stan`:

```{r, echo = F, eval = F}
# data to be explained / predicted
y <- murder_data %>% pull(murder_rate)
# data to use for prediction / explanation
x <- murder_data %>% pull(unemployment)
y_greta     <- as_data(y)
x_greta     <- as_data(x)
# latent variables and priors
intercept <- student(df= 1, mu = 0, sigma = 10)
slope     <- student(df= 1, mu = 0, sigma = 10)
sigma     <- normal(0, 5, truncation = c(0, Inf))
# derived latent variable (linear model)
y_pred <- intercept + slope * x_greta
# likelihood 
distribution(y) <- normal(y_pred, sigma)
# finalize model, register which parameters to monitor
murder_model <- model(intercept, slope, sigma)
```

```{r}
murder_data_4_Stan <- list(
  N = murder_data %>% nrow(),
  x = murder_data %>% pull(unemployment),
  y = murder_data %>% pull(murder_rate)
)
```

```{mystan, eval = F}
data {
int<lower=1> N ;
vector[N] x ;
vector[N] y ;
}
parameters {
real intercept ;
real slope ;
real<lower=0> sigma ;
} 
model {
# priors
intercept ~ student_t(1, 0, 10) ;
slope ~ student_t(1, 0, 10) ;
sigma ~ normal(0, 5) ;

# likelihood
y ~ normal(intercept + slope * x, sigma) ;
}
```

<link rel="stylesheet" href="hljs.css">
<script src="stan.js"></script>
<script>$('pre.mystan code').each(function(i, block) {hljs.highlightBlock(block);});</script>

We can draw samples from the posterior distribution as usual:

```{r, echo = F, eval = F}
# draw samples
draws_murder_data <- greta::mcmc(
  murder_model, 
  n_samples = 2000, 
  chains = 4, 
  warmup = 1000
)
# cast results (type 'mcmc.list') into tidy tibble
tidy_draws_murder_data <- ggmcmc::ggs(draws_murder_data)
```

```{r echo = F, eval = F}
draws_murder_data <- readRDS('models_greta/linear_regression_simple_murder_draws.rds')
tidy_draws_murder_data <- ggmcmc::ggs(draws_murder_data)
```

```{r}
stan_fit_linear_regression <- rstan::stan(
  # where is the Stan code
  file = 'models_stan/simple_linear_regression_model.stan',
  # data to supply to the Stan program
  data = murder_data_4_Stan,
  # how many iterations of MCMC
  iter = 3000,
  # how many warmup steps
  warmup = 500
)

# cast results (type 'mcmc.list') into tidy tibble
tidy_draws_murder_data = ggmcmc::ggs(stan_fit_linear_regression)
```


Here is a plot of the posterior:

```{r}
# plot posterior
tidy_draws_murder_data %>% 
  ggplot(aes(x = value)) +
  geom_density(fill = "lightgray", alpha = 0.5) +
  facet_wrap(~ Parameter, scales = "free")
```


### Using the `brms` package

Instead of hand-coding each Bayesian regression model, we can also use the `brms` package [@brms2017]. The main function of this package is `brm` (short for **B**ayesian **R**egression **M**odel). It behaves very similarly to the `glm` function we saw above.^[Actually, `brm` behaves similarly to the more general `lmer` function from the `lme4` package, which is even more general than `glm`. Both `lmer` and `brm` also cover so-called hierarchical regression models.] Here is an example of the current case study:

```{r, eval = F}
fit_brms_murder <- brm(
  # specify what to explain in terms of what
  #  using the formula syntax
  formula = murder_rate ~ unemployment,
  # which data to use
  data = murder_data
)
```

```{r, echo = F}
# samples computed in models_brms/murder_data.R
fit_brms_murder <- readRDS('models_brms/murder_data.rds')
```


The function `brm` returns a model-fit object, similar to `glm`. We can inspect it to get more information, using the `summary` function:

```{r}
summary(fit_brms_murder)
```

This output tells us about which model we fitted, states some properties of the MCMC sampling routine used to obtain samples from the posterior distribution, and it also gives us information about the posteriors under the heading "Population-Level Effects" we get summary statistics of the set of posterior samples for each regression coefficient, including the mean ("Estimate"), and the 95% inter-quantile range ("l-95%" is the lower bound and "u-95%" is the upper bound).^[Notice that the 95% inter-quantile range is not necessarily the same as the 95% HDI, but for large sample sizes the two will coincide.]

<div class = "exercises">
**Exercise 13.5 [optional]**

By default, `brms` assumes an appropriate prior for coefficients. What do you expect to happen to the estimate of the intercept when using a very strong prior for its value to be around 100?

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">
The estimate will have a value that is bigger than the current one, especially if the number of data points is small.
</div>
</div>
</div>

### Bayesian regression with non-informative standard priors
<!-- TODO: include or not? Currently not populated at all. -->


## Testing coefficients

### Bayesian approach

```{r}
# get means and 95% HDI
Bayes_estimates <- tidy_draws_murder_data %>% 
  group_by(Parameter) %>%
  summarise(
    '|95%' = HDInterval::hdi(value)[1],
    mean = mean(value),
    '95|%' = HDInterval::hdi(value)[2]
  )
Bayes_estimates
```


### Frequentist approach

Figure \@ref(fig:Chap-04-01-simple-linear-regression-frequentist-coefficient-testing) shows a frequentist model for testing the hypotheses that the regression coefficients are equal to zero.

```{r Chap-04-01-simple-linear-regression-frequentist-coefficient-testing, echo = F, fig.cap="Frequentist model for testing whether regression coefficients are plausibly equal to zero."}
knitr::include_graphics("visuals/linear-regression-model-frequentist.png")
```

We use this model to compute the test statics for the obsered data:

```{r}
# observed data
y_obs <- murder_data %>% pull(murder_rate)
x_obs <- murder_data %>% pull(unemployment)
n_obs <- length(y_obs)
# best-fitting coefficients
beta_1_hat <- cov(x_obs, y_obs) / var(x_obs)
beta_0_hat <- mean(y_obs) - beta_1_hat * mean(x_obs)
# calculating t-scores
MSE <- sum((y_obs - beta_0_hat - beta_1_hat * x_obs)^2) / (n_obs-2) 
S_xx <- sum((x_obs - mean(x_obs))^2)
SE_beta_1_hat <- sqrt(MSE  / S_xx)
SE_beta_0_hat <- sqrt(MSE * (1/n_obs + mean(x_obs)^2 / S_xx))
t_slope = (beta_1_hat) / SE_beta_1_hat
t_intercept = beta_0_hat / SE_beta_0_hat
tibble(t_slope, t_intercept)
```

Calculate $p$-values (two sided!) for both of these values:

```{r}
p_value_intercept = pt(t_intercept, df = n_obs -2) + 1-pt(-t_intercept, df = n_obs -2)
p_value_slope     = pt(-t_slope, df = n_obs -2) + 1-pt(t_slope, df = n_obs -2)
tibble(p_value_intercept, p_value_slope)
```

We compare the manual calculation to the that of the built-in functions `lm` and `glm`:

```{r}
summary(lm(murder_rate ~ unemployment, data = murder_data))
```

```{r}
summary(glm(murder_rate ~ unemployment, data = murder_data))
```

<div class = "exercises">
**Exercise 13.6**

Where can one find $p$-values in the above output? What does it mean?

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">
$p$-values can be found under the column 'Pr(>|t|)'. Low $p$-values are evidence against the null hypothesis, which in this case assumes that values of coefficients are $0$. In other words, we have strong evidence to believe that the mean difference between the two groups is not 0.
</div>
</div>
</div>



<!-- ## Snippets -->

<!-- ### Preliminaries -->

<!-- <div class = "grey"> -->
<!-- - *data* for simple linear regression: -->
<!--     - single dependent variable $y$ with observed data points $i: y_1,..., y_n$ -->
<!--     - $k$ predictor variables $x_1, ..., x_k$ and observations $x_{j1}, ..., x_{jn}$ for each $x_j$ -->
<!-- - we consider a *regression model*: -->
<!--     - $y_i \sim Normal(\beta_0 + \beta_1 x_{1i} + ... + \beta_k x_{ki}, \sigma)$ -->
<!--     - $\beta_j$ and $\sigma$ are free parameters, as usual -->
<!-- - further notation: -->
<!--     - $\bar{y} = \frac{1}{n} \sum_{i=1}^n y_i$    -   the mean of data observations  -->
<!--     - $\hat{y}_i = \hat{\beta_0} + \hat{\beta_1} x_1i + ... + \hat{\beta_k} x_{ki}$ - prediction for i-th data point for best fitting parameter values -->
<!-- </div> -->

<!-- --- -->

<!-- ### Definitions -->

<!-- <div class = "grey"> -->
<!-- **Total sum of squares:**  $$TSS = \sum_{i=1}^n (y_i - \bar{y})^2 \tag{0.1}$$ -->
<!-- **Explained sum of squares:**  $$ESS = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2\tag{0.2}$$ -->
<!-- **Residual sum of squares:**  $$RSS = \sum_{i=1}^n (y_i - \hat{y}_i)^2\tag{0.3}$$ -->
<!-- **Likelihood:**  $$LH = \prod_{i=1}^n Normal(\beta_0 + \beta_1 x_{1i} + ... + \beta_k x_{ki}, \sigma)\tag{0.4}$$ -->
<!-- </div> -->

<!-- --- -->

<!-- ### Theorem 1 -->

<!-- #### Theorem 1a - special case: one predictor ($k=1$) -->

<!-- <div class = "blue"> -->
<!-- **Closed-form solution for parameter fit under ordinary least squares loss function (special case where $k=1$).** -->

<!-- *If there is only one predictor variable $(k=1)$, the closed-form solution of predictors in the model that minimize RSS (Residual Sum of Squares) is:* -->


<!-- $$\begin{align} -->
<!-- \hat{\beta_0} &= \bar{y} - \hat{\beta}_1 \bar{x}\textrm{, and}\\ -->
<!-- \\ -->
<!-- \hat{\beta_1} &= \frac{Cov(x,y)}{Var(x)}. -->
<!-- \end{align}$$ -->
<!-- </div> -->

<!-- ##### Proof -->

<!-- *[See e.g., @kirchner2003, pp. 1-3; @olive2017, pp. 57-59]* -->

<!-- Given a set of $n$ observations $(X_i,Y_i)$ (or points on a scatter plot), we want to find the best-fit line,  -->
<!-- $$\hat y_i=\hat\beta_0+\hat\beta_1x_i,\tag{1.1.1}$$ -->
<!-- such that the sum of squared errors (RSS) in $Y$ is minimized: -->

<!-- $$RSS=\sum_{i=1}^n (y_i - \hat{y}_i)^2 \rightarrow min.\tag{1.1.2}$$ -->

<!-- Let the *Residual Sum of Squares (RSS)*  be denoted as $Q$ with, -->

<!-- $$\begin{align} -->
<!-- Q=RSS&=\sum_{i=1}^{n}(y_i-\hat y_i)^2\\ &=\sum_{i=1}^{n}(y_i-\hat\beta_0-\hat\beta_1x_i)^2. -->
<!-- \tag{1.1.3} -->
<!-- \end{align}$$ -->

<!-- We want to minimize $Q$ (that is minimizing *RSS*) at the values of $\hat\beta_0$ and $\hat\beta_1$ for which $\frac{\partial Q}{\partial \hat\beta_0}=0$ (1) and $\frac{\partial Q}{\partial \hat\beta_1}=0$ (2). -->

<!-- The first condition (1) is, -->

<!-- $$ \begin{align} \frac{\partial Q}{\partial \hat\beta_0}=\sum_{i=1}^{n}-2(y_i-\hat\beta_0-\hat\beta_1x_i)&= 0\\ -->
<!-- &=-\sum_{i=1}^ny_i+\sum_{i=1}^n\hat \beta_0+\sum_{i=1}^n\hat\beta_1x_i\\ -->
<!-- &=-\sum_{i=1}^ny_i+n\hat\beta_0+\sum_{i=1}^n\hat\beta_1x_i -->
<!-- \tag{1.1.4} -->
<!-- \end{align}$$ -->


<!-- which, if we solve for $\hat\beta_0$, becomes -->

<!-- $$\begin{align} -->
<!-- \hat\beta_0&=\frac{1}{n}\sum_{i=1}^{n}y_i-\frac{1}{n}\hat\beta_1\sum_{i=1}^{n}x_i\\ -->
<!-- &=\bar y - \hat\beta_1\bar x, -->
<!-- \tag{1.1.5} -->
<!-- \end{align}$$ -->

<!-- which says that the constant $\hat\beta_0$ (the y-intercept) is set such that the line must go through the mean of $x$ and $y$. This makes sense because this point is the "center" of the data cloud. -->

<!-- The solution is indeed a minimum as the second partial derivative is positive: -->

<!-- $\frac{\partial^2 Q}{\partial\hat\beta_0^2}=2n>0. \tag{1.1.6}$ -->

<!-- The second condition (2) is, -->

<!-- $$ \begin{align} -->
<!-- \frac{\partial Q}{\partial \hat\beta_1}=\sum_{i=1}^{n}-2x_i(y_i-\hat\beta_0-\hat\beta_1x_i)&=0\\ -->
<!-- &=\sum_{i=1}^{n}(-x_iy_i+\hat\beta_0x_i+\hat\beta_1x_i^2)\\ -->
<!-- &=-\sum_{i=1}^{n}x_iy_i+\hat\beta_0\sum_{i=1}^{n}x_i+\hat\beta_1\sum_{i=1}^{n}x_i^2 -->
<!-- \tag{1.1.7} -->
<!-- \end{align}$$ -->

<!-- If we substitute the expression by (1.1.5), we get, -->

<!-- $$ \begin{align} -->
<!-- 0&=-\sum_{i=1}^{n}x_iy_i+(\bar y - \hat\beta_1\bar x)\sum_{i=1}^{n}x_i+\hat\beta_1\sum_{i=1}^{n}x_i^2\\ -->
<!-- &=-\sum_{i=1}^{n}x_iy_i+\bar y\sum_{i=1}^{n}x_i-\hat\beta_1\bar x\sum_{i=1}^{n}x_i+\hat\beta_1\sum_{i=1}^{n}x_i^2 -->
<!-- \tag{1.1.8} -->
<!-- \end{align}$$ -->

<!-- separating this into two sums, -->

<!-- $$ \sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)-\hat\beta_1\sum_{i=1}^{n}\left(x_i^2-x_i\bar x\right)=0 \tag{1.1.9}$$ -->

<!-- becomes, -->

<!-- $$ \hat\beta_1 = \frac{\sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)}{\sum_{i=1}^{n}\left( x_i^2-x_i\bar x\right)} = \frac{\sum_{i=1}^{n}\left( x_iy_i\right)-n\bar x\bar y}{\sum_{i=1}^{n}\left( x_i^2\right)-n \bar x^2} \tag{1.1.10}$$ -->

<!-- The model assumes that the deviation from the values from the mean is zero, so that the positive and negative values are in balance, thus -->

<!-- $$ \sum_{i=1}^{n}\left( \bar x^2-x_i\bar x\right)=0, \tag{1.1.11}$$ -->

<!-- and -->

<!-- $$ \sum_{i=1}^{n}\left(\bar x \bar y - y_i \bar x\right)=0. \tag{1.1.12}$$ -->

<!-- This can be used in order to expand the previous term and finally to rewrite $\hat\beta_1$ as the ratio of $Cov(x,y)$ to $Var(x)$: -->

<!-- $$ -->
<!-- \begin{align} -->
<!-- \hat\beta_1&=\frac{\sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)+\sum_{i=1}^{n}\left(\bar x\bar y - y_i \bar x\right)}{\sum_{i=1}^{n}\left( x_i^2-x_i\bar x\right)+\sum_{i=1}^{n}\left( \bar x^2-x_i\bar x\right)}=\frac{\sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)+0}{\sum_{i=1}^{n}\left( x_i^2-x_i\bar x\right)+0}\\ -->
<!-- \\ -->
<!-- &=\frac{\frac{1}{n}\sum_{i=1}^{n}\left( x_i-\bar x\right) \left(y_i- \bar y \right)}{\frac{1}{n}\sum_{i=1}^{n}\left( x_i-\bar x\right)^2}\\ -->
<!-- \\ -->
<!-- &=\frac{Cov(x,y)}{Var(x)}. -->
<!-- \tag{1.1.13} -->
<!-- \end{align}$$ -->

<!-- The solution is indeed a minimum as the second partial derivative is positive: -->

<!-- $$\frac{\partial^2Q}{\partial \hat\beta_1^2}= 2 \sum_{i=1}^{n}x_i^2 >0. \tag{1.1.14}$$ -->

<!-- #### Theorem 1b: Generalization of Theorem 1a to $k >=1$ -->

<!-- *[see e.g., @bremer2012, pp. 21-23; @gonzalez2014, pp. 5-15]* -->

<!-- The model of multiple linear regression is given by the following expression: -->

<!-- $$y=\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_kx_k+\epsilon \tag{1.2.1}$$ -->
<!-- Suppose we have $n$ observations, then we can write: -->
<!-- $$\begin{align} -->
<!-- y_1&=\beta_0+\beta_{1}x_{11}+\beta_2x_{21}+...+\beta_kx_{k1}+\epsilon_1\\ -->
<!-- y_2&=\beta_0+\beta_{1}x_{12}+\beta_2x_{22}+...+\beta_kx_{k2}+\epsilon_2\\ -->
<!-- ...\\ -->
<!-- y_n&=\beta_0+\beta_{1}x_{1n}+\beta_2x_{2n}+...+\beta_kx_{kn}+\epsilon_n -->
<!-- \tag{1.2.2} -->
<!-- \end{align}$$ -->
<!-- The model of multiple linear regression is often expressed in matrix notation: -->

<!-- $$\begin{bmatrix} y_1\\y_2\\...\\y_n \end{bmatrix}= \begin{bmatrix}1&x_{11}& x_{21}&...&x_{k1}\\1&x_{12}& x_{22}&...&x_{k2}\\...& ...&...&...&...\\1&x_{1n}&x_{2n}&...&x_{kn}\end{bmatrix}\begin{bmatrix}\beta_0\\\beta_1\\...\\\beta_n \end{bmatrix}+\begin{bmatrix}\epsilon_1\\\epsilon_2\\...\\\epsilon_n \end{bmatrix} \tag{1.2.3}$$ -->

<!-- Which can be expressed in a compact form as -->

<!-- $$\mathbf{Y=X\beta+\epsilon} \tag{1.2.4}$$ -->
<!-- where $y$ is a vector $n\times 1$, $X$ is a matrix $n \times k$, $\beta$ is a vector $k \times 1$ and $\epsilon$ is a vector $n \times 1$. -->

<!-- The OLS estimator is obtained (like in the special case) by minimizing the residual sum of squares (RSS).  -->

<!-- $$RSS \rightarrow min.$$  -->

<!-- The RSS for the multiple linear regression model is -->

<!-- $$Q=RSS=\sum_{i=1}^n \hat\epsilon_i^2=\sum_{i=1}^n (y_i - \hat{y}_i)^2=\sum_{i=1}^n \left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]^2 \tag{1.2.5}$$ -->

<!-- to apply the least-squares criterion in the model of multiple linear regression, thus to minimize $RSS$, we calculate the first partial derivative from $Q$ with respect to each $\hat\beta_j$in the expression: -->

<!-- $$\begin{align} -->
<!-- \frac{\partial Q}{\partial\hat\beta_0}&=2\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right][-1]\\ -->
<!-- \\ -->
<!-- \frac{\partial Q}{\partial\hat\beta_1}&=2\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right][-x_{1i}]\\ -->
<!-- \\ -->
<!-- \frac{\partial Q}{\partial\hat\beta_2}&=2\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right][-x_{2i}]\\ -->
<!-- ...\\ -->
<!-- \frac{\partial Q}{\partial\hat\beta_k}&=2\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right][-x_{ki}] -->
<!-- \tag{1.2.6} -->
<!-- \end{align}$$ -->

<!-- Then the derivative of each equation is set to zero: -->

<!-- $$\begin{align} -->
<!-- &\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]=0\\ -->
<!-- &\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]x_{1i}=0\\ -->
<!-- &\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]x_{2i}=0\\ -->
<!-- &...\\ -->
<!-- &\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]x_{ki}=0 -->
<!-- \tag{1.2.7} -->
<!-- \end{align}$$ -->

<!-- Alternatively, we can use matrix notation and combine the above equations into the following form: -->

<!-- $$\mathbf{X'Y-X'X\hat\beta=0}.\tag{1.2.8}$$ -->

<!-- Whereby the following expression is known as **normal equations**: -->

<!-- $$\mathbf{X'X\hat\beta=X'Y}.\tag{1.2.9}$$ -->

<!-- The system of normal equations in expanded matrix notation is: -->

<!-- $$\begin{bmatrix} n&\sum_{i=1}^nx_{1i}&...&\sum_{i=1}^nx_{ki}\\ -->
<!-- \sum_{i=1}^nx_{1i}&\sum_{i=1}^nx_{1i}^2&...&\sum_{i=1}^nx_{1i}x_{ki}\\...&...&...&...\\ -->
<!-- \sum_{i=1}^nx_{ki}&\sum_{i=1}^nx_{ki}x_{1i}&...&\sum_{i=1}^nx_{ki}^2\end{bmatrix}\begin{bmatrix}\hat\beta_0\\\hat\beta_1\\...\\\hat\beta_k\end{bmatrix}=\begin{bmatrix}\sum_{i=1}^ny_i\\\sum_{i=1}^nx_{1i}y_i\\...\\\sum_{i=1}^nx_{ki}y_i  -->
<!-- \tag{1.2.10} -->
<!-- \end{bmatrix}$$ -->

<!-- In order to obtain the estimator $\hat\beta$, we have to rearrange (1.2.10) and get the solution: -->

<!-- $$\begin{bmatrix}\hat\beta_0\\\hat\beta_1\\...\\\hat\beta_k\end{bmatrix}=\mathbf{\hat\beta}=[\mathbf{X'X}]^{-1}\mathbf{X'Y}\tag{1.2.11}$$ -->

<!-- Where $\hat\beta$ is a global minimizer of the OLS criterion as the second order condition is always a semidefinite positive matrix. -->

<!-- $$\frac{\partial^2 Q}{\partial \mathbf{\hat\beta}^2}=2X'X >0.$$ -->

<!-- --- -->

<!-- ### Theorem 2 -->

<!-- <div class = "blue"> -->
<!-- **Best fit under OLS is equivalent with best fit under MLE** -->

<!-- *The parameters $\beta_0, ..., \beta_k$ minimize the residual sum of squares (RSS) iff they maximize the (log-)likelihood (LH).* -->

<!-- </div> -->

<!-- ##### Proof -->

<!-- *[see e.g., @naveen2019; @croot2010; @eppes2019]* -->

<!-- #### Maximum Likelihood Estimation -->
<!-- We consider again the linear regression model of the population with: -->

<!-- $$Y= \beta_0 + \beta_1X + \epsilon.\tag{2.1}$$ -->

<!-- This simplifies to the following form on the observed data: -->

<!-- $$y= \beta_0 + \beta_1x + \epsilon.\tag{2.2}$$ -->

<!-- Using a sample in order to obtain the **maximum likelihood estimates** the equation simplifies to: -->

<!-- $$y= \hat\beta_0 + \hat\beta_1x. \tag{2.3}$$ -->

<!-- **Assumptions** that we make for the model: -->

<!-- - True underlying distribution of the errors is Gaussian, -->
<!-- - Expected value of the error term is 0, -->
<!-- - Variance of the error term is constant with respect to x, and -->
<!-- - the 'lagged' errors are independent of each other -->
<!-- where the error term is normally distributed. -->

<!-- We can write: -->

<!-- $$ \epsilon \sim N(0,\sigma^2).\tag{2.4}$$ -->

<!-- Since $Y$ is a linear function of $\epsilon$ it will also be normally distributed. -->

<!-- $$f(\epsilon|\beta_0,\beta_1)= \frac{1}{\sqrt{2\pi} \sigma} \exp\left[{-\frac{1}{2}\left(\frac{\epsilon^2}{\sigma^2}\right)}\right]. \tag{2.5}$$  -->

<!-- Given the whole data set with $i=1,...,n$ observations the **likelihood function** ($LH$) is the joint density of all the observations, given a value for the parameters $\beta_0$ and $\beta_1$. Since independence is assumed, this is simply the product of the individual densities from the previous equation.  -->

<!-- $$LH(\epsilon_i|\beta_0,\beta_1) =\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi} \sigma} \exp\left[{-\frac{1}{2}\left(\frac{\epsilon_i^2}{\sigma^2}\right)}\right].\tag{2.6}$$ -->

<!-- In order to find the maximum of (2.6) we have to find the first derivative. -->

<!-- But as the derivation of a product with a lot of factors is inconvenient, we take first the logarithm of the likelihood, called **log-likelihhod**. This is possible as $\log$ is a monotone transformation and the maximum likelihood estimate does not change on log transformation. -->

<!-- The log-likelihood is the sum of the logs of the individual densities: -->

<!-- $$\begin{align} -->
<!-- LL&=\log \left(LH(\epsilon_i|\beta_0,\beta_1)\right)\\ -->
<!-- &=-\left( \frac{n}{2}\right) \log(2\pi)-\left( \frac{n}{2}\right) \log(\sigma^2)-\left( \frac{1}{2}\sigma^2\right) \sum_{i=1}^n(\epsilon_i)^2 -->
<!-- \tag{2.7} -->
<!-- \end{align}$$ -->

<!-- The log-likelihood can be used in order to find the *Maximum Likelihood estimates* $\hat\beta_0$ and $\hat\beta_1$ for the parameters $\beta_0$ and $\beta_1$. -->


<!-- $$\hat\beta_0,\hat\beta_1=\mathrm{argmax}_{\beta_0,\beta_1}LL=\mathrm{argmax}_{\beta_0,\beta_1}\left[-\left( \frac{n}{2}\right) \log(2\pi)-\left( \frac{n}{2}\right) \log(\sigma^2)-\left( \frac{1}{2}\sigma^2\right) \sum_{i=1}^n(\epsilon_i)^2\right].\tag{2.8}$$ -->

<!-- Removing the constant terms results in: -->

<!-- $$\hat\beta_0,\hat\beta_1=\mathrm{argmax}_{\beta_0,\beta_1} \sum_{i=1}^n-\epsilon_i^2.\tag{2.9}$$ -->

<!-- Substituting $\epsilon$, derived from (2.2), gives: -->

<!-- $$\hat\beta_0,\hat\beta_1=\mathrm{argmax}_{\beta_0,\beta_1} \sum_{i=1}^n-(y-\beta_0-\beta_1x)^2.\tag{2.10}$$ -->

<!-- **Conclusion:** -->

<!-- Deriving parameter estimates according to the OLS method: -->

<!-- $$Q_{OLS}=\sum_{i=1}^{n}(y_i-\hat\beta_0-\hat\beta_1x_i)^2 \rightarrow min.$$ -->

<!-- Deriving parameter estimates according to the ML method: -->

<!-- $$Q_{ML}=\sum_{i=1}^n-(y_i-\hat\beta_0-\hat\beta_1x_i)^2 \rightarrow max.$$ -->

<!-- Maximizing $-z$ is equivalent to minimizing $z$, thus, the best parameter fit under ML is equivalent to the best fit under OLS. -->

<!-- --- -->

<!-- ### Theorem 3 -->

<!-- <div class = "blue"> -->
<!-- **The variance in a regression model can be decompossed by using the notion of sum of squares:** -->

<!-- *The following decomposition holds:* -->

<!-- $$\begin{align} -->
<!-- TSS &= ESS + RSS \textrm{, or}\\ -->
<!-- \sum_{i=1}^n (y_i - \bar{y})^2&=\sum_{i=1}^n (\hat{y}_i - \bar{y})^2+ \sum_{i=1}^n \hat u_i^2  -->
<!-- \end{align}$$ -->

<!-- </div> -->


<!-- with  -->

<!-- $$\hat u_i =y_i-\hat y_i $$ -->

<!-- which can be rewritten as -->

<!-- $$y_i=\hat y_i + \hat u_i \tag{3.1}$$ -->

<!-- #### Proof -->

<!-- [see e.g., @gonzalez2014 pp. 29-31] -->

<!-- $$\begin{align} -->
<!-- TSS=\sum_{i=1}^n (y_i - \bar{y})^2&=\sum_{i=1}^n\left( \hat y_i+\hat u_i - \bar y \right)^2\\ -->
<!-- &= \sum_{i=1}^n \left[(\hat y_i-\bar y)+\hat u_i\right]^2\\ -->
<!-- &= \sum_{i=1}^n (\hat y_i-\bar y)^2 + \sum_{i=1}^n \hat u_i^2+ 2\sum_{i=1}^n(\hat y_i-\bar y)\hat u_i\\ -->
<!-- &= \sum_{i=1}^n (\hat y_i-\bar y)^2 + \sum_{i=1}^n \hat u_i^2\\ -->
<!-- \tag{3.2} -->
<!-- \end{align}$$ -->

<!-- Or, using (3.1) we can write: -->

<!-- $$ -->
<!-- TSS=\sum_{i=1}^n (\hat{y}_i - \bar{y})^2+\sum_{i=1}^n (y_i - \hat{y}_i)^2=ESS+RSS. -->
<!-- \tag{3.3} -->
<!-- $$ -->

<!-- --- -->

<!-- ### Theorem 4 -->

<!-- <div class = "blue"> -->
<!-- **Expressing variance explained $R^2$ in terms of sum of squares** -->

<!-- *The following relationship holds:*  -->

<!--   $$R^2 = 1 - \frac{RSS}{TSS} = \frac{ESS}{TSS}$$ -->
<!-- </div> -->

<!-- #### Proof -->

<!-- [see e.g., @urban2018 pp. 56-72, 101-103 ] -->

<!-- We will first derive the variance explained $R^2$ considering a bivariate model (simple linear regression), afterward some note to the generalized case of multiple regression is made. -->

<!-- The variance explained can be derived in the bivariate model from the regression coefficient $\beta$ and the standard deviation of $X$ and $Y$, whereby the following holds: -->

<!-- $$R^2=\left( \beta \cdot \frac{S_x}{S_y}\right)^2=\beta^2 \cdot \frac{Var(X)}{Var(Y)}.\tag{4.1}$$ -->

<!-- We will show that (4.1) holds in the next steps. -->

<!-- Consider on the one hand the calculation of the correlation coefficient: -->

<!-- $$\begin{align} -->
<!-- r_{xy}&=\frac{cov(X,Y)}{S_x S_y}\\ -->
<!-- \\ -->
<!-- &=\frac{\frac{1}{n}\sum_{i=1}^n(X_i-\bar X)(Y_i-\bar Y)}{\sqrt{\frac{\sum_{i=1}^n(X_i-\bar X)^2}{n}} \sqrt{\frac{\sum_{i=1}^n(Y_i-\bar Y)^2}{n}}} -->
<!-- \tag{4.2} -->
<!-- \end{align}$$ -->

<!-- On the other hand, if we assume $Y$ as dependent and $X$ as an independent variable, we can write: -->

<!-- $$\beta_{yx}=\frac{\sum_{i=1}^{n}X_iY_i}{\sum_{i=1}^n X_i^2}. \tag{4.3}$$ -->

<!-- Or when $Y$ is the independent and $X$ the dependent variable, then: -->

<!-- $$\beta_{xy}=\frac{\sum_{i=1}^{n}Y_iX_i}{\sum_{i=1}^n Y_i^2}.\tag{4.4}$$ -->

<!-- Through dividing equations (4.3) and (4.4) by the number of observations, we get the covariances and variances of $X$ and $Y$: -->

<!-- $$\begin{align} -->
<!-- &\frac{1}{n}\sum_{i=1}^{n}X_iY_i=\frac{1}{n}(X_i-\bar X)(Y_i-\bar Y) = cov(X,Y)\tag{4.5}\\ -->
<!-- &\frac{1}{n}\sum_{i=1}^{n}Y_iX_i=\frac{1}{n}(Y_i-\bar Y)(X_i-\bar X) = cov(Y,X)\tag{4.6}\\ -->
<!-- &\frac{1}{n}\sum_{i=1}^{n}X_i^2=\frac{1}{n}\sum_{i=1}^{n}(X_i-\bar X)^2=S_x^2\tag{4.7}\\ -->
<!-- &\frac{1}{n}\sum_{i=1}^{n}Y_i^2=\frac{1}{n}\sum_{i=1}^{n}(Y_i-\bar Y)^2=S_y^2 -->
<!-- \tag{4.8} -->
<!-- \end{align}$$ -->

<!-- Substituting these results in equations (4.3) and (4.4), we get: -->

<!-- $$\beta_{yx}=\frac{cov(X,Y)}{S_x^2} \textrm{ , and } -->
<!-- \beta_{xy}=\frac{cov(Y,X)}{S_y^2}\tag{4.9}$$ -->

<!-- For calculating the *average* between $\beta_{yx}$ and $\beta_{xy}$ we have to use the *geometric mean*: -->

<!-- $$\bar\beta_{geom}=\sqrt{\frac{cov(X,Y)}{S_x^2}\frac{cov(Y,X)}{S_y^2}}=\frac{cov(X,Y)}{S_x S_y}=r_{xy}\tag{4.10}$$ -->

<!-- Thus, the geometric mean of $\beta_{yx}$ and $\beta_{xy}$ is identical to the correlation coefficient.  -->

<!-- Comparing equations (4.9) and (4.10), -->

<!-- $$\begin{align} -->
<!-- \beta_{yx}&=\frac{cov(X,Y)}{S_x^2}\textrm{ , and}\\ -->
<!-- \\ -->
<!-- r_{yx}&=\frac{cov(X,Y)}{S_x S_y}, -->
<!-- \end{align}$$ -->

<!-- shows that, we can convert $r_{yx}$ in $\beta_{yx}$: -->

<!-- $$\frac{cov(X,Y)}{S_x S_y} \cdot \frac{S_y}{S_x}=\frac{cov(X,Y)}{S_x^2}=\beta_{yx}.\tag{4.11}$$ -->

<!-- Therefore, the relationship stated in (4.1) holds: -->

<!-- $$\beta_{yx}=r_{yx}\cdot \frac{S_y}{S_x}.$$ -->

<!-- Using this definition we can now derive the **variance explained** $R^2$. -->

<!-- We have seen that the following holds: -->

<!-- $$\beta_{yx}=r_{yx}\cdot \frac{S_y}{S_x}=r_{yx}\sqrt{\frac{\frac{1}{n}\sum_{i=1}^{n}(Y_i-\bar Y)^2}{\frac{1}{n}\sum_{i=1}^{n}(X_i-\bar X)^2}}.$$ -->

<!-- Eliminating the number of observations and squaring the equation gives: -->

<!-- $$\beta_{yx}^2=r_{yx}^2\frac{\sum_{i=1}^{n}(Y_i-\bar Y)^2}{\sum_{i=1}^{n}(X_i-\bar X)^2}.\tag{4.12}$$ -->

<!-- The numerator $\sum_{i=1}^{n}(Y_i-\bar Y)^2$ is the total sum of squares (TSS), thus, from (0.1) follows: -->

<!-- $$\sum_{i=1}^{n}(Y_i-\bar Y)^2=\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2+\sum_{i=1}^{n}(Y_i-\hat Y_i)^2.\tag{4.13}$$ -->

<!-- The explained sum of squares (ESS) $\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2$ are statistically explained by the determination of the regression line: -->

<!-- $$ESS=\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2=\beta X_i=\beta(X_i-\bar X).\tag{4.14}$$ -->

<!-- Substituting the result of (4.14) in equation (4.13) gives: -->

<!-- $$\sum_{i=1}^{n}(Y_i-\bar Y)^2=\beta^2\sum_{i=1}^{n}(X_i-\bar X)^2+\sum_{i=1}^{n}(Y_i-\hat Y_i)^2.\tag{4.15}$$ -->

<!-- As we know from (4.12): -->

<!-- $$\beta_{yx}^2=r_{yx}^2\frac{\sum_{i=1}^{n}(Y_i-\bar Y)^2}{\sum_{i=1}^{n}(X_i-\bar X)^2},$$ -->

<!-- we can substitute $\beta$ in equation (4.15) with the result from (4.12) and get: -->

<!-- $$\begin{align} -->
<!-- \sum_{i=1}^{n}(Y_i-\bar Y)^2&=r_{xy}^2\frac{\sum_{i=1}^{n}(Y_i-\bar Y)^2}{\sum_{i=1}^{n}(X_i-\bar X)^2}\sum_{i=1}^{n}(X_i-\bar X)^2+\sum_{i=1}^{n}(Y_i-\hat Y_i)^2\\ -->
<!-- \\ -->
<!-- &=r_{xy}^2 \sum_{i=1}^{n}(Y_i-\bar Y)^2+\sum_{i=1}^{n}(Y_i-\hat Y_i)^2\tag{4.16}\\ -->
<!-- \\ -->
<!-- \frac{\sum_{i=1}^{n}(Y_i-\bar Y)^2-\sum_{i=1}^{n}(Y_i-\hat Y_i)^2}{\sum_{i=1}^{n}(Y_i-\bar Y)^2}&=r_{xy}^2 \textrm{ ,which is}\\ -->
<!-- \\ -->
<!-- 1-\frac{\sum_{i=1}^{n}(Y_i-\hat Y_i)^2}{\sum_{i=1}^{n}(Y_i-\bar Y)^2}=1-\frac{RSS}{TSS}&=r_{xy}^2. -->
<!-- \tag{4.17} -->
<!-- \end{align}$$ -->

<!-- Similarly, according to (0.2) the explained sum of squares (ESS) are the deviation between total (TSS) and residual sum of squares (RSS): -->

<!-- $$\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2 = \sum_{i=1}^{n}( Y_i-\bar Y)^2-\sum_{i=1}^{n}(Y_i-\hat Y)^2,$$ -->

<!-- therefore, we can write equation (4.17) as well as: -->

<!-- $$\begin{align} -->
<!-- \frac{\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2}{\sum_{i=1}^{n}(Y_i-\bar Y)^2}&=r_{xy}^2\\ -->
<!-- \\ -->
<!-- \frac{ESS}{RSS}&=r_{xy}^2=R^2. -->
<!-- \tag{4.18} -->
<!-- \end{align}$$ -->

<!-- **Final remarks:** -->
<!-- In the bivariate model (simple linear regression) the variance explained is equivalent to the squared bivariate correlation coefficient (Bravais Pearson).  -->
<!-- In multiple regression model the relation: -->

<!-- $$\frac{ESS}{TSS}=R^2$$ -->

<!-- holds as well. Here the variance explained is equivalent to the squared multiple correlation coefficient between the estimated and observed Y-value ($r_{\hat Y Y}$). -->

<!-- In the multiple linear regression model, the variance explained is also dependent on the number of X-variables. Therefore an adjusted $R^2$ measure should be used. -->
