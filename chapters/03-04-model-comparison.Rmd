# Model Comparison {#Chap-03-06-model-comparison}

<hr>

<div style = "float:right; width:40%;">
<img src="visuals/badge-model-comparison.png" alt="badge model comparison">  
</div>  

Parameter estimation (the topic of the last chapter) asks: given a single model and the data, what are good (e.g., credible) values of the model's parameters? 
Model comparison (the topic of this chapter) asks: based on the data at hand, which of several models is better? Or even: *how much* better is this model compared to another, given the data?

The pivotal criterion by which to compare models is how well a model explains the observed data. A good explanation of observed data $D$ is one that makes $D$ unsurprising. Intuitively, we long for an explanation for things that puzzle us. A good explanation is a way of looking at the world in which puzzles disappear, in which all observations make sense, in which what we have seen would have been quite expectable after all. Consequently, the pivotal quantity for comparing models is how likely $D$ is given a model $M_i$: $P(D \mid M_i)$.

But there is more to a good explanation, also intuitively. All else equal, a good explanation is simple. If theories $A$ and $B$ both explain the facts equally well, but $A$ does so with less "mental machinery", most people would choose the more economical explanation $A$. 

In this chapter, we will look at two common methods of comparing models: the Akaike information criterion (AIC) and Bayes factors.
AICs are a non-Bayesian method in the sense that it does not require (or ignores) a model's priors over parameter values.
Bayes factors are a the flagship Bayesian method for model comparison.
There are many other approaches to model comparison (e.g., other kinds of information criteria, or methods based on cross-validation, other varieties of). 
Our goal is not to be exhaustive, but to introduce the main ideas of model comparison and showcase a reasonable selection of representative approaches.

```{block, type='infobox'}
The learning goals for this chapter are:

- understand the differences between estimation and model comparison
- understand and apply the two covered methods:
  - Akaike information criterion
  - Bayes factor
- become familiar with the pro's and con's of each of these methods
- [optional] get acquainted with some methods for computing Bayes factors
```


## Case study: recall models {#Chap-03-06-model-comparison-case-study}

As a running example for this chapter, we borrow from @Myung2003:Tutorial-on-Max and consider a fictitious data set of recall rates and two models to explain this data. 

As for the data, for each time point (in seconds) $t \in \{1, 3, 6, 9, 12, 18\}$, we have 100 (binary) observations of whether a previously memorized item was recalled correctly.


```{r}
# time after memorization (in seconds)
t <- c(1, 3, 6, 9, 12, 18)
# proportion (out of 100) of correct recall
y <- c(.94, .77, .40, .26, .24, .16)
# number of observed correct recalls (out of 100)
obs <- y * 100
```

A visual representation of this data set is here:

```{r echo = F}
tibble(
  t, obs
) %>% 
  ggplot(aes(x = t, y = y)) +
  geom_point(size = 3, color = "darkgreen") +
  labs(
    x = "time (in seconds)",
    y = "proportion of correct recall",
    title = "(fictitious) recall data"
  ) +
  ylim(c(0,1))
```

We are interested in comparing two theoretically different models for this data. Models differ in their assumption about the functional relationship between recall probability and time. The **exponential model** assumes that the recall probability $\theta_t$ at time $t$ is an exponential decay function with parameters $a$ and $b$:

$$\theta_t(a, b) = a \exp (-bt), \ \ \ \ \text{where } a,b>0 $$

Taking the binary nature of data (recalled / not recalled) into account, this results in the following likelihood function for the exponential model:

$$
\begin{aligned}
P(k \mid a, b, N , M_{\text{exp}}) & = \text{Binom}(k,N, a \exp (-bt)), \ \ \ \ \text{where } a,b>0 
\end{aligned}
$$

In contrast, the **power model** assumes that the relationship is that of a power function:

$$\theta_t(c, d) = ct^{-d}, \ \ \ \ \text{where } c,d>0 $$

The resulting likelihood function for the power model is:

$$
\begin{aligned}
P(k \mid a, b, N , M_{\text{pow}}) & = \text{Binom}(k,N, c\ t^{-d}), \ \ \ \ \text{where } c,d>0 
\end{aligned}
$$

These models therefore make different (parameterized) predictions about the time course of forgetting/recall. Figure \@ref(fig:Chap-03-06-model-comparison-model-predictions) shows the predictions of each model for $\theta_t$ for different parameter values:

```{r Chap-03-06-model-comparison-model-predictions, echo = F, fig.height=12, fig.cap = "Examples of predictions of the exponential and the power model of forgetting for different values of each model's parameters."}
forgetData = tibble(t = t, obs = obs, y = y)
expo = function(x, c, d) return( c* exp(-x*d) )
power = function(x, a, b) return( a*x^(-b) )
myCols = project_colors[1:3]

forgetPlotExpo = ggplot(data.frame(x = c(1,20)), aes(x)) +
         stat_function(fun = function(x) expo(x, 1,1), aes(color = "a,b=1")) +
         stat_function(fun = function(x) expo(x, 2,2), aes(color = "a,b=2")) +
         stat_function(fun = function(x) expo(x, 1,0.2), aes(color = "a=1,b=0.1")) +
         scale_colour_manual("Function", breaks = c("a,b=1", "a,b=2", "a=1,b=0.1"), values = myCols) +
          ggtitle("exponential model") + geom_point(data = forgetData, aes(x = t, y = y)) + ylab("recall prob.") + xlab("time t")
forgetPlotPower = ggplot(data.frame(x = c(1,20)), aes(x)) +
         stat_function(fun = function(x) power(x, 1,1), aes(color = "c,d=1")) +
         stat_function(fun = function(x) power(x, 2,2), aes(color = "c,d=2")) +
         stat_function(fun = function(x) power(x, 2,1), aes(color = "c=2, d=1")) +
         scale_colour_manual("Function", breaks = c("c,d=1", "c,d=2", "c=2, d=1"), values = myCols) +
          ggtitle("power model") + geom_point(data = forgetData, aes(x = t, y = y)) + ylab("recall prob.") + xlab("time t")
cowplot::plot_grid(forgetPlotExpo, forgetPlotPower, nrow = 2)
```

The research question of relevance is: which of these two models is a better model for the observed data?
We are going to look at the Akaike information criterion (AIC) first, which only considers the models' likelihood functions and is therefore a non-Bayesian method.
We will see that AIC scores are easy to compute, but give numbers that are hard to interpret or only approximation of quantities that have a clear interpretation.
Then we look at a Bayesian method, using Bayes factors, which does take priors over model parameters into account.
We will see that Bayes factors are much harder to compute, but do directly calculate quantities that are intuitively interpretable.
We will also see that AIC scores only very indirectly take a model's complexity into account.

## Akaike Information Criterion {#Chap-03-06-model-comparison-AIC}

A wide-spread non-Bayesian approach to model comparison is to use the **Akaike information criterion (AIC)**. The AIC is the most common instance of a class of measures for model comparison known as *information criteria*, which all draw on information-theoretic notions to compare how good each model is.

If $M_i$ is a model, specified here only by its likelihood function $P(D \mid \theta_i, M_i)$, with $k_i$ model parameters in parameter vector $\theta_i$, and if $D_\text{obs}$ is the observed data, then the AIC score of model $M_i$ given $D_\text{obs}$ is defined as:

$$
\begin{aligned}
\text{AIC}(M_i, D_\text{obs}) & = 2k_i - 2\log P(D_\text{obs} \mid \hat{\theta_i}, M_i)
\end{aligned}
$$
Here, $\hat{\theta}_i = \arg \max_{\theta_i} P(D_\text{obs} \mid \theta_i, M_i)$ is the best-fitting parameter vector, i.e., the maximum likelihood estimate (MLE), and $k$ is the number of free parameters in model $M_i$.

The lower an AIC score, the better the model (in comparison to other models for the same data $D_\text{obs}$). All else equal, the higher the number of free parameters $k_i$, the worse the model's AIC score. The first summand in the definition above can, therefore, be conceived of as a measure of **model complexity**. As for the second summand, think of $- \log P(D_\text{obs} \mid \hat{\theta}_i, M_i)$ as a measure of (information-theoretic) surprisal: how surprising is the observed data $D_\text{obs}$ from the point of view of model $M$ under the most favorable circumstances (that is, the MLE of $\theta_i$). The higher the probability $P(D_\text{obs} \mid \hat{\theta}_i, M_i)$, the better the model $M_i$'s AIC score, all else equal.

To apply AIC-based model comparison to the recall models, we first need to compute the MLE of each model (see Chapter \@ref(ch-03-04-parameter-estimation-conjugacy)). Here are functions that return the negative log-likelihood of each model, for any (suitable) pair of parameter values:

```{r}
# generic neg-log-LH function (covers both models)
nLL_generic <- function(par, model_name) {
  w1 <- par[1]
  w2 <- par[2]
  # make sure paramters are in acceptable range
  if (w1 < 0 | w2 < 0 | w1 > 20 | w2 > 20) {
    return(NA)
  }
  # calculate predicted recall rates for given parameters
  if (model_name == "exponential") {
    theta <- w1*exp(-w2*t)  # exponential model
  } else {
    theta <- w1*t^(-w2)     # power model
  }
  # avoid edge cases of infinite log-likelihood
  theta[theta <= 0.0] <- 1.0e-4
  theta[theta >= 1.0] <- 1-1.0e-4
  # return negative log-likelihood of data
  - sum(dbinom(x = obs, prob = theta, size = 100, log = T))
}
# negative log likelihood of exponential model
nLL_exp <- function(par) {nLL_generic(par, "exponential")}
# negative log likelihood of power model
nLL_pow <- function(par) {nLL_generic(par, "power")}
```

These functions are then optimized with R's built-in function `optim`. The results are shown in the table below.

```{r}
# getting the best fitting values
bestExpo <- optim(nLL_exp, par = c(1, 0.5))
bestPow  <- optim(nLL_pow, par = c(0.5, 0.2))
MLEstimates = data.frame(model = rep(c("exponential", "power"), each = 2),
                         parameter = c("a", "b", "c", "d"),
                         value = c(bestExpo$par, bestPow$par))
MLEstimates
```

The MLE-predictions of each model are shown in Figure \@ref(fig:Chap-03-06-model-comparison-MLE-fits) below, alongside the observed data.

```{r Chap-03-06-model-comparison-MLE-fits, echo = F, fig.cap = "Predictions of the exponential and the power model under best-fitting parameter values."}
a <- bestExpo$par[1]
b <- bestExpo$par[2]
c <- bestPow$par[1]
d <- bestPow$par[2]
forgetPlotBest <- ggplot(data.frame(x = c(1,20)), aes(x)) +
  stat_function(fun = function(x) expo(x, a, b), aes(color = "exponential")) +
  stat_function(fun = function(x) power(x, c, d), aes(color = "power")) +
  scale_colour_manual(
    "Function", 
    breaks = c("exponential", "power"), 
    values = project_colors[1:2]
  ) +
  ggtitle("MLE fits") + geom_point(data = forgetData, aes(x = t, y = y)) +
  labs(
    x = "time",
    y = "recall rate"
  )
forgetPlotBest
```
By visual inspection of Figure \@ref(fig:Chap-03-06-model-comparison-MLE-fits) alone, it is impossible to say with confidence which model is better. Numbers might help see more fine-grained differences.
So, let's look at the log-likelihood and the corresponding probability of the data for each model under each model's best fitting parameter values. 

```{r}
predExp <- expo(t,a,b)
predPow <- power(t,c,d)
modelStats <- tibble(
  model = c("expo", "power"),
  `log likelihood` = round(c(-bestExpo$value, -bestPow$value),3),
  probability = signif(exp(c(-bestExpo$value, -bestPow$value)),3),
  # sum of squared errors
  SS = round(c( sum((predExp-y)^2), sum((predPow-y)^2)),3)
)
modelStats
```

The exponential model has a higher log-likelihood, a higher probability, and a lower sum of squares. This suggests that the exponential model is better. 

The AIC-score of these models is a direct function of the negative log-likelihood. Since both models have the same number of parameters, we arrive at the same verdict as before: based on comparison of AIC-scores, the exponential model is the better model.

```{r}
get_AIC <- function(optim_fit) {
  2 * length(optim_fit$par) + 2 * optim_fit$value
}
AIC_scores <- tibble(
  AIC_exponential = get_AIC(bestExpo),
  AIC_power = get_AIC(bestPow)
)
AIC_scores
```

How should we interpret the difference in AIC-scores? Some suggest that differences in AIC-scores larger than 10 should be treated as implying that the weaker model has practically no empirical support [@BurnhamAnderson2002:Model-Selection]. Adopting such a criterion, we would therefore favor the exponential model based on the data observed. 

But we could also try to walk a more nuanced, more quantitative road.
We would ideally want to know the *absolute probability* of $M_i$ given the data: $P(M_i \mid D)$. 
Unfortunately, to calculate this (by Bayes rule), we would need to normalize by quantifying over *all* models. Alternatively, we look at the relative probability of a small selection of models.
Indeed, we can look at relative AIC-scores in terms of so-called **Akaike weights** [@WagenmakersFarrell2004:AIC-model-selec;@BurnhamAnderson2002:Model-Selection] to derive an approximation of $P(M_i \mid D)$, at least for the case where we only consider a small set of candidate models.
So, if we want to compare models $M_1, \dots, M_n$ and $\text{AIC}(M_i, D)$ is the AIC-score of model $M_i$ for observed data $D$, then the **Akaike weight of model $M_i$** is defined as:

$$
\begin{aligned}
w_{\text{AIC}}(M_i, D) & = \frac{\exp (- 0.5 * \Delta_{\text{AIC}}(M_i,D) )} {\sum_{j=1}^k\exp (- 0.5 * \Delta_{\text{AIC}}(M_j,D) )}\, \ \ \ \ \text{where} \\
\Delta_{\text{AIC}}(M_i,D) & = \text{AIC}(M_i, D) - \min_j \text{AIC}(M_j, D)
\end{aligned}
$$

Akaike weights are relative and normalized measures, and may serve as an approximate measure of a model's posterior probability given the data:

$$ P(M_i \mid D) \approx w_{\text{AIC}}(M_i, D) $$ 

For the running example at hand, this would mean that we could conclude that the posterior probability of the exponential model is approximately:

```{r}
delta_AIC_power <- AIC_scores$AIC_power - AIC_scores$AIC_exponential
delta_AIC_exponential <- 0
Akaike_weight_exponential <- exp(-0.5 * delta_AIC_exponential) /
  (exp(-0.5 * delta_AIC_exponential) + exp(-0.5 * delta_AIC_power))
Akaike_weight_exponential
```

We can interpret this numerical result as indicating that, given a universe in which only the exponential and the power model exist, the posterior probability of the exponential model is almost 1 (assuming, implicitly, that both models are equally likely _a priori_).
We would conclude from this approximate quantitative assessment that the empirical evidence supplied by the given data in favor of the exponential model is very strong.

Our approximation is better the more data we have. We will see a method below, the Bayesian method using Bayes factors, which computes $P(M_i \mid D)$ in a non-approximate way.

<div class = "exercises">
**Exercise 11.1**

1. Describe what the following variables represent in the AIC formula:
$$
\begin{aligned}
\text{AIC}(M_i, D_\text{obs}) & = 2k_i - 2\log P(D_\text{obs} \mid \hat{\theta_i}, M_i)
\end{aligned}
$$

a. $k_i$ stands for: 
b. $\hat{\theta_i}$ stands for:
c. $P(D_\text{obs} \mid \hat{\theta_i}, M_i)$ stands for:

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

a. the number of free parameters in model $M_{i}$;
b. the parameter vector obtained by maximum likelihood estimation for model $M_{i}$ and data $D_{\text{obs}}$;
c. the likelihood of the data $D_{\text{obs}}$ under the best fitting parameters of a model $M_{i}$.

</div>
</div>

2. Do you see that there is something "circular" in the definition of AICs? (Hint: What do we use the data $D_{\text{obs}}$ for?)

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

We use the same data twice! We use $D_{\text{obs}}$ to find the best fitting parameter values, and then we ask how likely $D_{\text{obs}}$. If model comparison is about how well a model explains the data, then this is a rather circular measure: we quantify how well a model explains or predicts a data set after having "trained / optimized" the model for exactly this data set. 

</div>
</div>

</div>


## Bayes factors {#Chap-03-06-model-comparison-BF}

At the end of the previous section, we saw that we can use the AIC-approach to calculate an approximate value of the posterior probability $P(M_{i} \mid D)$ for model $M_{i}$ is given data $D$. The Bayes factor approach is similar to this, but avoids taking priors over models into the equation by focusing on _the extent to which data $D$ changes our beliefs about the which model is more likely_. 

Take two Bayesian models:

- $M_1$ has prior $P(\theta_1 \mid M_1)$ and likelihood $P(D \mid \theta_1, M_1)$
- $M_2$ has prior $P(\theta_2 \mid M_2)$ and likelihood $P(D \mid \theta_2, M_2)$
    
Using Bayes rule, we compute the posterior odds of models (given the data) as the product of the likelihood ratio and the prior odds.

$$\underbrace{\frac{P(M_1 \mid D)}{P(M_2 \mid D)}}_{\text{posterior odds}} = \underbrace{\frac{P(D \mid M_1)}{P(D \mid M_2)}}_{\text{Bayes factor}} \ \underbrace{\frac{P(M_1)}{P(M_2)}}_{\text{prior odds}}$$

The likelihood ratio is also called the **Bayes factor**. Formally, the Bayes factor is the factor by which a rational agent changes her prior odds in the light of observed data to arrive at the posterior odds. More intuitively, the Bayes factor quantifies the strength of evidence given by the data about the models of interest. It expresses this evidence in terms of the models' relative prior predictive accuracy. To see the latter, let's expand the Bayes factor as what it actually is: the ratio of marginal likelihoods.

$$
\frac{P(D \mid M_1)}{P(D \mid M_2)} = \frac{\int P(\theta_1 \mid M_1) \ P(D \mid \theta_1, M_1) \text{ d}\theta_1}{\int P(\theta_2 \mid M_2) \ P(D \mid \theta_2, M_2) \text{ d}\theta_2}
$$

Three insights are to be gained from this expansion. Firstly, the Bayes factor is a measure of how well each model would have predicted the data *ex ante*, i.e., before having seen any data. In this way, it is diametrically opposed to a concept like AIC, which relies on models' maximum likelihood fits (therefore *using the data*, so being *ex post*). 

Secondly, the marginal likelihood of a model is exactly the quantity that we identified (in the context of parameter estimation) as being very hard to compute, especially for complex models. The fact that marginal likelihoods are hard to compute was the reason that we methods like MCMC sampling are useful, since they give posterior samples _without_ requiring the calculation of marginal likelihoods. 
It follows that Bayes factors can be very difficult to compute in general. 
However, for many prominent models, it is possible to calculate Bayes factors analytically if the right kinds of priors are specified [@RouderSpeckman2009:Bayesian-t-test;@RouderMorey2012:Default-Bayes-F;@GronauLy2019:Informed-Bayesi].
We will see an example of this in Chapter \@ref(ch-03-07-hypothesis-testing-Bayes).
Also, as we will see in the following there are very clever approaches to computing Bayes factors in special cases and good algorithms for approximating marginal likelihoods also for complex models.

Thirdly, Bayes factor model comparison implicitly (and quite vigorously) punishes model complexity, but in a more sophisticated manner than just counting free parameters. To appreciate this intuitively, imagine a model with a large parameter set and a very diffuse, uninformative prior that spreads its probability over a wide range of parameter values. Since Bayes factors are computed based on *ex ante* predictions, a diffuse model is punished for its imprecision of prior predictions because we integrate over all parameters (weighted by priors) and their associated likelihood. 

As for notation, we write:

$$\text{BF}_{12} = \frac{P(D \mid M_1)}{P(D \mid M_2)}$$
for the Bayes factor in favor of model $M_1$ over model $M_2$. This quantity can take on positive values, which are often translated into natural language as follows:

$BF_{12}$ | interpretation
:---:|:---:|
1 | irrelevant data
1 - 3 | hardly worth ink or breath
3 - 6 | anecdotal
6 - 10 | now we're talking: substantial
10 - 30 | strong
30 - 100 | very strong
100 + | decisive (bye, bye $M_2$!)

As $\text{BF}_{12} = \text{BF}_{21}^{-1}$ it suffices to give this translation into natural language only for values $\ge 1$.

There are at least two general approaches to calculating or approximating Bayes factors, paired here with a (non-exhaustive) list of example methods:

1. get each model's marginal likelihood 
    - grid approximation (see Section \@ref(Chap-03-06-model-comparison-BF-grid))
    - by Monte Carlo sampling (see Section \@ref(Chap-03-06-model-comparison-BF-naiveMC))
    - bridge sampling (see Section \@ref(Chap-03-06-model-comparison-BF-bridge))
2. get Bayes factor directly
    - Savage-Dickey method (see Section \@ref(ch-03-07-hypothesis-testing-Bayes-Savage-Dickey))
    - using encompassing models (see Section \@ref(ch-03-07-hypothesis-testing-Bayes-encompassing-models))

### Grid approximation {#Chap-03-06-model-comparison-BF-grid}

We can use _grid approximation_ to approximate a model's marginal likelihood if the model are small enough, say, no more than 4-5 free parameters. 
Grid approximation considers discrete values for each parameter evenly spaced over the whole range of plausible parameter values, thereby approximating the integral in the definition of marginal likelihoods. 

Let's calculate an example for the comparison of the exponential and the power model of forgetting.
To begin with, we need to define a prior over parameters to obtain Bayesian versions of the exponential and power model.
Here, we assume flat priors over a reasonable range of parameter values for simplicity. For the exponential model, we choose:

$$
\begin{aligned}
P(k \mid a, b, N, M_{\text{exp}}) & = \text{Binom}(k,N, a \exp (-bt_i)) \\
P(a \mid M_{\text{exp}}) & = \text{Uniform}(a, 0, 1.5) \\
P(b \mid M_{\text{exp}}) & = \text{Uniform}(b, 0, 1.5) 
\end{aligned}
$$

The (Bayesian) power model is given by:


$$
\begin{aligned}
P(k \mid a, b, N, M_{\text{pow}}) & = \text{Binom}(k,N, c\ t_i^{-d}) \\
P(c \mid M_{\text{pow}}) & = \text{Uniform}(c, 0, 1.5) \\
P(d \mid M_{\text{pow}}) & = \text{Uniform}(d, 0, 1.5) 
\end{aligned}
$$

We can also express these models in code, like so:

```{r}
# prior exponential model
priorExp <- function(a, b){
  dunif(a, 0, 1.5) * dunif(b, 0, 1.5)
}
# likelihood function exponential model
lhExp <- function(a, b){
  theta <- a*exp(-b*t)
  theta[theta <= 0.0] <- 1.0e-5
  theta[theta >= 1.0] <- 1-1.0e-5
  prod(dbinom(x = obs, prob = theta, size = 100))
}

# prior power model
priorPow <- function(c, d){
  dunif(c, 0, 1.5) * dunif(d, 0, 1.5)
}
# likelihood function power model
lhPow <- function(c, d){
  theta <- c*t^(-d)
  theta[theta <= 0.0] <- 1.0e-5
  theta[theta >= 1.0] <- 1-1.0e-5
  prod(dbinom(x = obs, prob = theta, size = 100))
}
```

To approximate each model's marginal likelihood via grid approximation, we consider equally spaced values for both parameters (a tighly knit grid), asses the prior and likelihood for each parameter pair and finally take the sum over all of the visited values:

```{r}
# make sure the functions accept vector input
lhExp <- Vectorize(lhExp)
lhPow <- Vectorize(lhPow)

# define the step size of the grid
stepsize <- 0.01
# calculate the marginal likelihood
marg_lh <- expand.grid(
  x = seq(0.005, 1.495, by = stepsize),
  y = seq(0.005, 1.495, by = stepsize)
) %>% 
  mutate(
    lhExp = lhExp(x,y), priExp = 1 / length(x),  # uniform priors!
    lhPow = lhPow(x,y), priPow = 1 / length(x)
  )
# output result
str_c(
  "BF in favor of exponential model: ", 
  with(marg_lh, sum(priExp*lhExp)/ sum(priPow*lhPow)) %>% round(2)
)
```

Based on this computation, we would be entitled to conclude that the data provides overwhelming evidence in favor of the exponential model. The result tells us that a rational agent should adjust her prior odds by a factor of more than 1000 in favor of the exponential model when updating her beliefs with the data. In other words, the data tilt our beliefs very strongly towards the exponential model, no matter what we believed initially. In this sense, the data provides strong evidence for the exponential model. 

### Naive Monte Carlo {#Chap-03-06-model-comparison-BF-naiveMC}

For simple models (with maybe 4-5 free parameters), we can also use naive Monte Carlo sampling to approximate Bayes factors. In particular, we can approximate the marginal likelihood by taking samples from the prior, calculating the likelihood of the data for each sampled parameter tuple, and then averaging over all calculated likelihoods:

$$P(D, M_i) = \int P(D \mid \theta, M_i) \ P(\theta \mid M_i) \ \text{d}\theta \approx \frac{1}{n} \sum^{n}_{\theta_j \sim P(\theta \mid M_i)} P(D \mid \theta_j, M_i)$$

Here is a calculation using one million samples from the prior of each model:

```{r}
nSamples <- 1000000
# sample from the prior
a <- runif(nSamples, 0, 1.5)
b <- runif(nSamples, 0, 1.5)
# calculate likelihood of data for each sample
lhExpVec <- lhExp(a,b)
lhPowVec <- lhPow(a,b)
# compute marginal likelihoods
str_c(
 "BF in favor of exponential model: ", 
 round(mean(lhExpVec) / mean(lhPowVec),2)
)
```

We can also check the time course of our MC-estimate by a plot like that in Figure \@ref(fig:Chap-03-06-model-comparison-MC-estimate-time).
The plot shows the current estimate of the Bayes factor on the $y$-axis after having taken the number of samples given on the $x$-axis.
We see that the initial calculations (after only a 10,000 samples) is far off, but that the approximation finally gets reasonably close to the value calculated by grid approximation, which is shown as the red line.

```{r Chap-03-06-model-comparison-MC-estimate-time, echo = F, fig.cap = "Temporal development (as more samples come in) of the Monte Carlo estimate of the Bayes factor in favor of the exponential model over the power model of forgetting. The red horizontal line indicates the Bayes factor estimate obtained previously via grid approximation."}
BFVec <- map_dbl(
  # start at 10.000 and then inspect every 500 samples
  seq(10000, nSamples, by = 500), 
  function(i){
    # what's the BF-estimate at that point in time?
    sum(lhExpVec[1:i]) / sum(lhPowVec[1:i])
  } 
)

tibble(
  i = seq(10000, nSamples, by = 500), 
  BF = BFVec
) %>% 
ggplot(aes(x = i, y = BF)) +
  geom_line() + 
  geom_hline(
    yintercept = 1221, 
    color = "firebrick"
  ) + 
  xlab("number of samples")
```

<div class = "exercises">
**Exercise 11.3**

Which statements concerning Bayes Factors (BF) are correct?

a. The Bayes Factor shows the absolute probability of a particular model to be a good explanation of the observed data.
b. If $BF_{12} = 11$, one should conclude that there is strong evidence in favor of $M_1$.
c. Grid approximation allows us to compare no more than five models simultaneously.
d. With the Naive Monte Carlo method, we can only approximate the BF for models with continuous parameters. 
e. BF computation penalizes more complex models.

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

Statements b. and e. are correct.

</div>
</div>
</div>


### Excursion: Bridge sampling {#Chap-03-06-model-comparison-BF-grid}

For more complex models (e.g., high-dimensional/hierarchical parameter spaces), naive Monte Carlo methods can be highly inefficient. If random sampling of parameter values from the priors is unlikely to deliver values for which the likelihood of the data is reasonably high, most naive MC samples will contribute very little information to the overall estimate of the marginal likelihood. For this reason, there are better sampling-based procedures which preferentially sample *a posteriori* credible parameter values (given the data) and use clever math to compensate for using the wrong distribution to sample from. This is the main idea behind approaches like [importance sampling](https://en.wikipedia.org/wiki/Importance_sampling). A very promising approach is in particular **bridge sampling**, which also has its own R package [@GronauSarafoglou2017:A-tutorial-on-b].

We will not go into the formal details of this method, but just showcase here an application of the `bridgesampling` package.
This approach requires samples from the posterior, which we can obtain using Stan (see Section \@ref(ch-03-03-estimation-Stan)).
Towards this end, we first assemble the data for input to the Stan program in a list:

```{r}
forgetting_data <- list(
  N = 100,
  k = obs,
  t = t
)
```

The models are implemented in the Stan. We here only show the exponential model.

```{mystan, eval = F}
data {
  int<lower=1> N ;
  int<lower=0,upper=N> k[6] ;
  int<lower=0> t[6];
}
parameters {
  real<lower=0,upper=1.5> a ;
  real<lower=0,upper=1.5> b ;
} 
model {
  // likelihood
  for (i in 1:6) {
    target += binomial_lpmf(k[i] | N,  a * exp(-b*t[i])) ;
  }
}
```

We then use Stan to obtain samples from the posterior in the usual way. To get reliable estimates of Bayes factors via bridge sampling we should take a much larger number of samples than we usually would for a reliable estimation of, say, the posterior means and credible intervals.

```{r}
stan_fit_expon <- rstan::stan(
  # where is the Stan code
  file = 'models_stan/model_comp_exponential_forgetting.stan',
  # data to supply to the Stan program
  data = forgetting_data,
  # how many iterations of MCMC
  iter = 20000,
  # how many warmup steps
  warmup = 2000
)
```

```{r}
stan_fit_power <- rstan::stan(
  # where is the Stan code
  file = 'models_stan/model_comp_power_forgetting.stan',
  # data to supply to the Stan program
  data = forgetting_data,
  # how many iterations of MCMC
  iter = 20000,
  # how many warmup steps
  warmup = 2000
)
```

The `bridgesampling` package can then be used to calculate each model's marginal likelihood.

```{r }
expon_bridge <- bridgesampling::bridge_sampler(stan_fit_expon, silent = T)
power_bridge <- bridgesampling::bridge_sampler(stan_fit_power, silent = T)
```

We then obtain an estimate of the Bayes factor in favor of the exponential model with this function:

```{r }
bridgesampling::bf(expon_bridge, power_bridge)
```
