
# Open science practices {#app-94-open-science}

**[Main author of chapter: Özge Özenoglu]**

This chapter is dedicated to reflecting on open science practices regarding data collection, analysis and sharing the results with the scientific community. To motivate why this is necessary, Section \@ref(app-94-replication-crisis) unravels how publication bias, questionable research practices, inflated error rates, and the lack of transparency threaten the credibility of psychological science. This section will be somewhat pessimistic. However, there is no need to despair! Section \@ref(app-94-remedies) provides an overview of measures to solve (or at least reduce) the problems from the previous section. Finally, Section \@ref(app-94-recap) summarizes what everybody can do to embrace transparent, open, and replicable research.

## Psychology's replication crisis {#app-94-replication-crisis}

What happens with a scientific discipline if it predominantly fails to replicate^[Direct replication is the repetition of an experiment as close as possible to the original methodology. A replication attempt is declared to be "successful" if it was capable of obtaining the same results as the original experiment on the new data set. Replicability is often interchangeably used with reproducibility, which is the ability to obtain the same results as the original experiment on the original data set.] previous discoveries? This question frequently arose after a groundbreaking project revealed that psychology is facing a replication crisis. In 2011, the @OpenScienceCollab2015 launched a large-scale project – the so-called “Reproducibility Project” – in which they attempted 100 direct replications of experimental and correlational studies in psychology. The results are worrisome: 97% of the original studies reported statistically significant results, whereas the initiative could merely replicate 36% of the results.^[There are several methods to assess the replicability of a study result. The Reproducibility Project separately evaluated the replication success – among others – based on (a) statistical significance ($p < 0.05$) (b) inclusion of original effect sizes within the 95% CI of replication effect sizes (c) magnitude between effect sizes (d) subjective assessment (“Did it replicate?”). Here, replication success is only evaluated based on statistical significance.] This low replicability rate, however, does not imply that about two-thirds of the discoveries are wrong. It emphasizes that research outcomes should not be taken at face value but scrutinized by the scientific community. Most of all, the results show that scientists should take action to increase the replicability of their studies. This urgent need is further fueled by the discovery that the prevalence of low replicability rates diminishes the public’s trust [e.g., @WingenBerkessel2019] and, in the long run, might undermine the credibility of psychology as a science. 

In order to know how to increase a study’s replicability, it is crucial to investigate what causes replications to fail. Essentially, failing to replicate the significant results of the original study has three roots: The original study yielded a false-positive, the replication study yielded a false-negative, or too divergent methodologies led to two different outcomes [@OpenScienceCollab2015]. We focus here on false-positives and diverging methodologies. We only briefly touch on false-negatives in the replication study when we talk about low statistical power.

### Publication bias, QRP's, and false-positives

Weighing evidence in favor of verifying preconceptions and beliefs rather than falsifying them is a cognitive bias (confirmation bias). This natural form of reasoning can be a considerable challenge in doing proper research, as the full amount of information should be taken into account and not just those consistent with prior beliefs. Confirmation bias also manifests itself in a tendency to see patterns in the data and perceive meaning, when there is only noise (apophenia) and overestimating the prediction of an event after it occurred, typically expressed as “I knew it all along!” (hindsight bias). 

These biases further pave the way for a skewed incentive structure that prefers confirmation over inconclusiveness or contradiction. In psychological science, there is a vast prevalence of publications that report significant ($p < 0.05$) and novel findings in contrast to null-results [e.g., @Sterling1959] or replication studies [e.g., @MakelPlucker2012]. This substantial publication bias towards positive and novel results may initially seem entirely plausible. Journals might want to publish flashy headlines that catch the reader’s attention rather than “wasting” resources for studies that remain inconclusive. Furthermore, scientific articles that report significant outcomes are more likely to be cited [@DuyxUrlings2017] and thus may increase the journal’s impact factor (JIF). Replication studies might not be incentivized because they are considered tedious and redundant. Why publish results that don't make new contributions to science?^[This way of thinking might be the reason why direct replications were virtually replaced by *conceptual replications*, which is the replication of a basic idea from previous research, albeit with different experimental methods to preserve novelty.]

Publication bias operates at the expense of replicability and thus the reliability of science. The pressure of generating significant results can further fuel the researcher’s bias [@Fanelli2010]. Increasing cognitive biases towards the desired positive result could therefore lead researchers to draw false conclusions. To cope with this “Publish or Perish” mindset, researchers may increasingly engage in **questionable research practices** (QRP’s) as a way of somehow obtaining a $p$-value less than the significance level $\alpha$. QRP’s fall into the grey area of research and might be the norm in psychological science. Commonly researchers “$p$-hack” their way to a significant $p$-value by analyzing the data multiple ways through exploiting the flexibility in data collection and data analysis (researcher degrees of freedom). This exploratory behavior is frequently followed by selective reporting of what “worked”, so-called cherry-picking. Such $p$-hacking also takes on the form of unreported omission of statistical outliers and conditions, post hoc decisions to analyze a subgroup, or to change statistical analyses. Furthermore, researchers make rounding errors by reporting their results to cross the significance threshold (.049 becomes .04), they randomly stop collecting data when the desired $p$-value of under .05 pops up, or they present exploratory hypotheses^[An exploratory hypothesis is one that was generated after the data was inspected, that is, *explored* for unanticipated patterns. In contrast, a confirmatory hypothesis is one that is defined *before* data collection. Both types are essential for scientific progress: Theories are generated by exploring the data and tested as confirmatory hypotheses in a new experiment. However, it gets fraudulent when serendipitous findings are passed off as being predicted in advance.] as being confirmatory (HARKing, Hypothesizing After the Results are Known).

Diederik Stapel, a former professor of social psychology at Tilburg University, writes in his book *Faking Science: A True Story of Academic Fraud* about his scientific misconduct. He shows how easy it is to not just fool the scientific community (in a discipline where transparency is not common practice) but also oneself:

> I did a lot of experiments, but not all of them worked. […] But when I really believed in something […] I found it hard to give up, and tried one more time. If it seemed logical, it must be true. […] You can always make another couple of little adjustments to improve the results. […] I ran some extra statistical analyses looking for a pattern that would tell me what had gone wrong. When I found something strange, I changed the experiment and ran it again, until it worked [@Stapel2014, pp. 100--101].

If the publication of a long-standing study determines whether researchers get funding or a job, it is perfectly understandable why they consciously or subconsciously engage in such practices. However, exploiting researcher degrees of freedom by engaging in QRP’s poses a significant threat to the validity of the scientific discovery by blatantly inflating the probability of false-positives. By not correcting the significance threshold accordingly, many analyses are likely to be statistically significant just by chance, and reporting solely those that “worked” additionally paints a distorted picture on the confidence of the finding. 

```{block, type='infobox'}

<div style = "float:right; width:15%;">
  <img src="visuals/green-jelly-bean.png">
</div>

**Example.** Let's illustrate $p$-hacking based on a [popular comic by xkcd](https://www.explainxkcd.com/wiki/index.php/882:_Significant). In the comic, two researchers investigate whether eating jelly beans causes acne. A $p$-value larger than the conventional $\alpha$-threshold doesn't allow them to reject the null hypothesis of no effect. Well, it must be one particular color that is associated with acne. The researchers now individually test the 20 different colors of jelly beans. Indeed, numerous tests later, they obtain the significant $p$-value that they have probably been waiting for. The verdict: Green jelly beans are associated with acne! Of course, this finding leads to a big headline in the newspaper. The article reports that there is only a 5% chance that the finding is due to coincidence.

However, the probability that the finding is a fluke is about 13 times higher than anticipated and reported in the paper. Let's check what happened here:

In the first experiment (without color distinctions), there was a $5\%$ chance of rejecting $H_0$, and consequently a $95\%$ chance of failing to reject $H_0$. Since the $\alpha$-level is the upper bound on a false-positive outcome, the confidence in the finding reported in the newspaper would have been true if the researchers had kept it with just one hypothesis test. However, by taking the 20 different colors into account, the probability of obtaining a non-significant $p$-value in each of the 20 tests dropped from $95\%$ to $0.95^{20} \approx 35.85\%$, leaving room for a $64.15\%$ chance that at least one test yielded a false-positive. The probability of at least one false-positive due to conducting multiple hypothesis tests on the same data set is called the *family-wise error rate* (FWER). Formally, it can be calculated like so:

$$\alpha_{FWER} = 1 - (1 - \alpha)^n,$$

where $\alpha$ denotes the significance level for each individual test, which is conventionally set to $\alpha = 0.05$, and $n$ the total number of hypothesis tests.

Conducting multiple tests on the same data set and not correcting the family-wise error rate accordingly, therefore makes it more likely that a study finds a statistically significant result by coincidence.
```

To investigate how prevalent QRP’s are in psychological science, Leslie John et al. [@JohnLoewenstein2012] surveyed over 2000 psychologists regarding their engagement in QRP’s. They found that 66.5% of the respondents admitted that they failed to report all dependent measures, 58% collected more data after seeing whether the results were significant, 50% selectively reported studies that “worked”, and 43.4% excluded data after looking at the impact of doing so. Based on the self-admission estimate, they derived a prevalence estimate of 100% for each mentioned QRP. These numbers once more reinforce the suspicion that QRP’s are the norm in psychology. Together with the fact that these practices can blatantly inflate the false-positive rates, one might conclude that much of the psychological literature cannot be successfully replicated and thus *might* be wrong.

### Low statistical power

Another factor that can account for unreliable discoveries in the scientific literature is the persistence of highly underpowered studies in psychology [e.g., @Cohen1962; @SedlmeierGigerenzer1989; @BakkerVanDijk2012; @SzucsIoannidis2017]. A study’s statistical power is the probability of correctly rejecting a false null hypothesis, i.e., the ideal in NHST. Defined as $1 − \beta$, power is directly related to the probability of encountering a false-negative, meaning that low-powered studies are less likely to reject $H_0$ when it is in fact false. Figure \@ref(fig:ch-94-error-dists) shows the relationship between $\alpha$-errors and $\beta$-errors (slightly adapted from a previous figure in Chapter \@ref(ch-03-04-hypothesis-significance-errors)), as well as the power to correctly rejecting $H_0$.

```{r ch-94-error-dists, echo = F, fig.cap="Relationship between power, $\\alpha$ and $\\beta$-errors."}
plot_data %>% 
  ggplot(aes(x = x, y = value, color = name)) + 
  # null hypothesis segment & label
  geom_segment(
    aes(
      x = 0,
      xend = 0,
      y = -0.02,
      yend = dnorm(0, sd = 2)
    ),
    color = "darkgray"
  ) + 
  geom_label(
    aes(
      x = 0,
      y = -0.025,
      label = "H_0"
    ),
    color = "darkgray",
    label.size = 0,
    size = 6
  ) +
  # alternative hypothesis segment & label
  geom_segment(
    aes(
      x = 4,
      xend = 4,
      y = -0.02,
      yend = dnorm(0, sd = 2)
    ),
    color = "darkgray"
  ) + 
  geom_label(
    aes(
      x = 4,
      y = -0.025,
      label = "H_a"
    ),
    color = "darkgray",
    label.size = 0,
    size = 6
  ) +
  # power area
  geom_area(
    data = filter(
      plot_data, name == "alternative", 
      x >= qnorm(0.95, sd = 2)
    ), 
    fill = "firebrick",
    color = "firebrick",
    alpha = 0.3
  ) +
  # alpha error area
  geom_area(
    data = filter(
      plot_data, name == "null", 
      x >= qnorm(0.95, sd = 2)
    ), 
    fill = project_colors[2],
    color = project_colors[2],
    alpha = 0.3
  ) +
  # beta error area
  geom_area(
    data = filter(
      plot_data, name == "alternative", 
      x <= qnorm(0.95, sd = 2)
    ), 
    fill = project_colors[1],
    color = project_colors[1],
    alpha = 0.3
  ) +
  # main density lines
  geom_line(size = 2) +
  # Power explanation
  annotate(
    geom ='text', 
    x = 10, 
    y = 0.2, 
    label = ("Power"), 
    parse = TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
    geom_segment(
    aes(
      x = 4.3,
      xend = 10,
      y = 0.105,
      yend = 0.19
    ),
    color = "black"
  ) +
  # alpha error explanation
  annotate(
    geom = 'text', 
    x = 10, 
    y = 0.1, 
    label = TeX("$\\alpha$-error", output ='character'), 
    parse = TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = 4.3,
      xend = 10,
      y = 0.005,
      yend = 0.09
    ),
    color = "black"
  ) +
  # beta error explanation
  annotate(
    geom = 'text', 
    x = -6, 
    y = 0.1, 
    label = TeX("$\\beta$-error", output = 'character'), 
    parse = TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = -0.3,
      xend = -6,
      y = 0.005,
      yend = 0.09
    ),
    color = "black"
  ) +
  # general cosmetics
  guides(color = F) +
  labs(x = "", y = "") +
  theme(
      axis.text = element_blank(),
      axis.ticks = element_blank()
  )
```

It may be tempting to conclude that a statistically significant result of an underpowered study is "more convincing". However, low statistical power also decreases the probability that a significant result reflects a true effect (that is, that the detected difference is really present in the population). This probability is referred to as the *Positive Predictive Value* (PPV). The PPV is defined as $$PPV = \frac{(1 - \beta) \cdot R}{(1 - \beta) \cdot R + \alpha},$$ where $1 − \beta$ is the statistical power, $R$ is the pre-study odds (the odds of the prevalence of an effect before conducting the experiment), and $\alpha$ is the type I error. Choosing the conventional $\alpha$ of 5% and assuming $R$ to be 25%, the PPV for a statistically significant result of a study with 80% power - which is deemed acceptable - is 0.8. If the power is reduced to 35%, the PPV is 0.64. A 64% chance that a discovered effect is true implies that there is a 36% chance that a false discovery was made. Therefore, low-powered studies are more likely to obtain flawed and unreliable outcomes, which contribute to the poor replicability of discoveries in the scientific record.

Another consequence of underpowered studies is the overestimation of effect sizes^[The effect size quantifies the difference between two variables (e.g., the difference in means) or the strength of the relationship between two variables (e.g., correlation coefficient). While statistical significance attests that there is an effect, the effect size tells us something about the magnitude of the effect (also called *practical* significance).] and a higher probability of an effect size in the wrong direction. These errors are referred to as **Type M (Magnitude)** and **Type S (Sign) errors**, respectively [@GelmanCarlin2014]. If, for example, the true effect size (which is unknown in reality) between group $A$ and $B$ is 20 ms, finding a significant effect size of 50 ms would overestimate the true effect size by a factor of 2.5. If we observe an effect size of -50 ms, we would even wrongly assume that group $B$ performs faster than group $A$.

The statistical power, as well as Type S and Type M error rates can be easily estimated by simulation. Recall the example from Chapter \@ref(ch-03-05-hypothesis-testing-t-test), where we investigated whether the distribution of IQ's from a sample of CogSci students could have been generated by an average IQ of 100, i.e., $H_0: \mu_{CogSci} = 100 \ (\delta = 0)$. This time, we're doing a two-tailed $t$-test, where the alternative hypothesis states that there is a difference in means without assigning relevance to the direction of the difference, i.e., $H_a: \mu_{CogSci} \neq 100 \ (\delta \neq 0)$. We plan on recruiting 25 CogScis and set $\alpha = 0.05$. 

Before we start with the real experiment, we check its power, Type S, and Type M error rates by hypothetically running the same experiment 10000 times in the WebPPL code box below. From the previous literature, we estimate the true effect size to be 1 (CogScis have an average IQ of 101) and the standard deviation to be 15. Since we want to know how many times we correctly reject the null hypothesis of equal means, we set the estimated true effect size as ground truth (`delta` variable) and sample from $Normal(100 + \delta, 15)$. Variable `t_crit` stores the demarcation point for statistical significance in a $t$-distribution with `n` - 1  degrees of freedom.

We address the following questions:

* If the true effect size is 1, what is the probability of correctly rejecting the null hypothesis of equal means (= an effect size of 0)?
* If the true effect size is 1, what is the probability that a significant result will reflect a negative effect size (that is, an average IQ of less than 100)?
* If the true effect size is 1 and we obtain a statistically significant result, what is the ratio of the estimated effect size to the true effect size (exaggeration ratio)?

Play around with the parameter values to get a feeling of how power can be increased. Remember to change the `t_crit` variable when choosing a different sample size. The critical $t$-value can be easily looked up in a $t$-table or computed with the respective quantile function in R (e.g, `qt(c(0.025,0.975), 13)` for a two-sided test with $\alpha = 0.05$ and $n = 14$). For $n \geq 30$, the $t$-distribution approximates the standard normal distribution.

<pre class="webppl">
var delta = 1;         // true effect size between mu_CogSci and mu_0
var sigma = 15;        // standard deviation
var n = 25;            // sample size per experiment
var t_crit = 2.063899; // +- critical t-value for n-1 degrees of freedom
var n_sim = 10000;     // number of simulations (1 simulation = 1 experiment)
///fold:

var se = sigma/Math.sqrt(n); // standard error

// Effect size estimates:

/* In each simulation, drep(n_sim) takes n samples from a normal distribution 
centered around the true mean and returns a vector of the effect sizes */
var drep = function(n_sim) {
  if(n_sim == 1) {
    var sample = repeat(n, function(){gaussian({mu: 100 + delta, sigma: sigma})});
    var effect_size = [_.mean(sample)-100];
    return effect_size;
  } else {
    var sample = repeat(n, function(){gaussian({mu: 100 + delta, sigma: sigma})});
    var effect_size = [_.mean(sample)-100];
    return effect_size.concat(drep(n_sim-1));
  }
}

// vector of all effect sizes
var ES = drep(n_sim);

// Power:

/* get_signif(n_sim) takes the number of simulations and returns a vector of only 
significant effect sizes. It calculates the absolute observed t-value, i.e., 
|effect size / standard error| and compares it with the critical t-value. If the 
absolute observed t-value is greater than or equal to the critical t-value, the 
difference in means is statistically significant.

Note that we take the absolute t-value since we're conducting a two-sided t-test and
therefore also have to consider values that are in the lower tail of the sampling 
distribution. */
var get_signif = function(n_sim) {
  if(n_sim == 1) {
   var t_obs = Math.abs(ES[0]/se);
    if(t_obs >= t_crit) {
      return [ES[0]];
    } else {
      return [];
    }
  } else {
    var t_obs = Math.abs(ES[n_sim-1]/se);
     if(t_obs >= t_crit) {
       return [ES[n_sim-1]].concat(get_signif(n_sim-1));
    } else {
      return [].concat(get_signif(n_sim-1));
    }
  }
}

// vector of only significant effect size estimates
var signif_ES = get_signif(n_sim);

// proportion of times where the null hypothesis would have been correctly rejected
var power = signif_ES.length/n_sim;

// Type S error:

/* get_neg_ES(n_sim) takes the number of simulations and returns a vector of 
significant effect sizes that are negative. */
var get_neg_ES = function(n_sim){
  if(n_sim == 1){
    if(signif_ES[n_sim-1] < 0){
      return [signif_ES[n_sim-1]];
    } else {
      return [];
    }
  } else {
    if(signif_ES[n_sim-1] < 0){
      return [signif_ES[n_sim-1]].concat(get_neg_ES(n_sim-1));
    } else {
      return [].concat(get_neg_ES(n_sim-1));
    }
  }
}

// vector of only significant effect size estimates that are negative
var neg_ES = get_neg_ES(n_sim);

/* If at least one simulation yielded statistical significance, calculate the
proportion of significant+negative effect sizes to all significant effect sizes. */
var type_s = function(){
 if(signif_ES.length == 0) {
      return "No significant effect size";
    } else {
      return neg_ES.length/signif_ES.length;
    } 
}

// proportion of significant results with a negative effect size
var s = type_s();

// Type M error:

// take the absolute value of all significant effect sizes
var absolute_ES = _.map(signif_ES, Math.abs);

/* If at least one simulation yielded statistical significance, calculate the 
ratio of the average absolute effect size to the true effect size. */
var type_m = function(){
 if(signif_ES.length == 0) {
      return "No significant effect size";
    } else {
      return _.mean(absolute_ES)/delta; 
    } 
}

// exaggeration ratio
var m = type_m();

// Results:

// print results
display("Power: " + power + 
        "\nType II error: " + (1-power) + 
        "\nType S error: " + s +
        "\nType M error: " + m
       );

// print interpretation depending on results
if(power != 0) {
  if(_.round(m,1) == 1) {
    display(
      "Interpretation:\n" +
      // Power
      "If the true effect size is " + delta + ", there is a " + 
      _.round((power*100),1) + "% chance of detecting a significant \ndifference. "+
      // Type S error
      "If a significant difference is detected, there is a " + 
      _.round((s*100),1) +
      "% chance \nthat the effect size estimate is negative. " + 
      // Type M error
      "Further, the absolute estimated effect size is expected to be about the " +
      "same as the true effect size of " + delta + "."
    );
  } else {
    display(
      "Interpretation:\n" +
      // Power
      "If the true effect size is " + delta + ", there is a " + 
      _.round((power*100),1) + "% chance of detecting a significant \ndifference. "+
      // Type S error
      "If a significant difference is detected, there is a " + 
      _.round((s*100),1) +
      "% chance \nthat the effect size estimate is negative. " + 
      // Typ M error
      "Further, the absolute estimated effect size is expected to be " +
      _.round(m,1) + " times too high."
    );
  }
} else {
  display(
    "Interpretation:\n" +
    // Power
    "If the true effect size is " + delta + ", there is no " +
    "chance of detecting a significant \ndifference at all. " + 
    "Since Type S and Type M errors are contingent on a \nsignificant " + 
    "result, there is no chance of having them in this case."
   );
}
///
</pre>

<pre class=" CodeMirror-line " role="presentation">
</pre>

<script>
// find all <pre> elements and set up the editor on them
var preEls = Array.prototype.slice.call(document.getElementsByClassName("webppl"));
preEls.map(function(el) { console.log(el); editor.setup(el, {language: 'webppl'}); });
</script>

As the power of replication studies is typically based on the reported effect size of the original study, an inflated effect size also renders the power of the replication study to be much lower than anticipated. Hence, an underpowered study may additionally increase the replications’ probability of encountering a type II error, which may lead replicators to misinterpret the statistical significance of the original study as being a false-positive. Besides being self-defeating for authors of the original study, this may compromise the veracity of the cumulative knowledge base that direct replications aim to build.

### Lack of transparency

When it comes to the reporting of methodologies, there seem to be disagreements within the scientific community. In his *new Etiquette for Replication*, Daniel Kahneman [-@Kahneman2014] called for new standards for conducting direct replication studies. Concretely, replicators should be obliged to consult the authors of the original study – otherwise, the replication should not be valid. According to him, the described methodologies in psychology papers are too vague to permit direct replications. He argues that “[…] behavior is easily affected by seemingly irrelevant factors” and that paraphrasing experimental instructions discards crucial information, as “[…] their wording and even the font in which they are printed are known to be significant”. Kahneman’s proposed rules for the interaction between authors and replicators led to heated discussions within the discipline. Chris Chambers [-@Chambers2017, pp. 52--55] refers to several responses to Kahneman, among others, from psychologist Andrew Wilson. In his blog post, titled *Psychology's real replication problem: our Methods sections*, he takes an unequivocal stand on rejecting rigid standards for replication studies:

> If you can't stand the replication heat, get out of the empirical kitchen because publishing your work means you think it's ready for prime time, and if other people can't make it work based on your published methods then that's your problem and not theirs [@Wilson2014].

Of course, there are also voices between those extremes that, even if they disagree with Kahneman’s proposal, agree that there are shortcomings in reporting methodologies. So why are method sections not as informative as they should be? A reason might be that the trend towards disregarding direct replications – due to lacking incentives – decreases the importance of detailed descriptions about the experimental design or data analyses. Furthermore, editors may favor brief method descriptions due to a lack of space in the paper. To minimize a variation in methodologies that might account for different outcomes, it is essential that journal policies change accordingly.

In addition to detailed reporting of methodologies, further materials such as scripts and raw data are known to facilitate replication efforts. In an attempt to retrieve data from previous studies, @HardwickeIoannidis2018 encountered that almost 40% of the authors did not respond to their request in any form, followed by almost 30% not willing to share their data. The reluctance to share data for reanalysis can be related to weaker evidence and more errors in reporting statistical results [@WichertsBakker2011]. This finding further intensifies the need for assessing the veracity of the reported results by reanalyzing the raw data, i.e., checking its computational reproducibility. However, computational replication attempts can hardly be conducted without transparency of the original study. To end this vicious circle and make sharing common practice, journals could establish mandatory sharing policies or provide incentives for open practices.

## Possible remedies {#app-94-remedies}

The intertwined connections between contributing factors show how quickly the reproducibility and replicability of a study can be compromised. In order to safeguard research integrity, it is therefore necessary that many factors are addressed. Over the past years, the severity of the consequences of irreplicable research has spawned many solution attempts. The following provides an overview of the most effective measures so far.

### Improve scientific rigor

#### Preregistration

A preregistration is a protocolled research plan *prior* to data collection – including hypotheses, methodology, research design, and statistical analyses. With this commitment, researcher degrees of freedom are drastically constrained, so that (sub)conscious engagement in QRP’s like $p$-hacking or HARKing is limited. Such practices are limited because preregistered studies draw a clear line between exploratory and confirmatory analyses. The confirmatory analyses regarding the prespecified hypothesis (i.e., the prediction) are protocolled in advance so that exploring the data and then selectively reporting is not possible without being detectable. 

Some may perceive preregistrations as a limitation on "scientific creativity". However, preregistrations are not intended to deter researchers from exploring their data. Examining the data is the core of generating new hypotheses for further *confirmatory* analyses, which is ultimately vital for scientific progress. Nevertheless, to counteract a potential inflation of false-positive rates, any $p$-values calculated by a test deceived after data inspection need to be marked as post hoc.

You do not need to write a preregistration from scratch. In fact, websites such as the [Open Science Framework (OSF)](https://osf.io/) or [AsPredicted](https://www.aspredicted.org/) offer useful templates, only needed to be filled out as detailed as possible. Below is a list of what your preregistration should ideally include. For more information on each point listed, please take a look at the [preregistration template](https://docs.google.com/document/d/1DaNmJEtBy04bq1l5OxS4JAscdZEkUGATURWwnBKLYxk/edit?pli=1) provided by the OSF.

* **Study information:** title, authors, description (optional), hypotheses

* **Design plan:** study type, blinding, study design, randomization (optional)

* **Sampling plan:** existing data, explanation of existing data (optional), data collection procedures, sample size, sample size rationale (optional), stopping rule (optional)

* **Analysis plan:** statistical models, transformations (optional), inference criteria (optional), data exclusion (optional), missing data (optional), exploratory analyses (optional)

#### Know your stats {#ch-05-01-frequentist-know-your-stats}

**1. Learn more about statistical frameworks**

A good departure point for increasing research quality is to learn more about statistical frameworks, such as Null Hypothesis Significance Testing. A common misinterpreted and misused concept in NHST is the $p$-value itself. Therefore, when adopting a frequentist approach to hypothesis testing, it is important to intensively engage with the purpose and interpretation of the $p$-value. In the exercise box below (repeated with minor changes from before), you can test your knowledge about it. Please re-visit Chapter \@ref(ch-03-05-hypothesis-p-values) if you feel like brushing up on the topic. 

<div class = "exercises">
**Self-test on $p$-values**

Which statement(s) about $p$-values is/are true? The $p$-value is...

a. ...the probability that the null hypothesis $H_0$ is true.
b. ...the probability that the alternative hypothesis $H_a$ is true.
c. ...the probability, derived from the assumption that $H_0$ is true, of obtaining an outcome for the chosen test statistic that is the exact same as the observed outcome.
d. ...a measure of evidence in favor of $H_0$.
e. ...the probability, derived from the assumption that $H_0$ is true, of obtaining an outcome for the chosen test statistic that is the same as the observed outcome or more extreme towards $H_a$.
f. ...the probability of a Type-I error.
g. ...a measure of evidence against $H_0$.

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

Statements e. and g. are correct. 
   
</div>
</div>
</div>

Another statistical framework worth considering is a [Bayesian approach to hypothesis testing](#ch-03-07-hypothesis-testing-Bayes). Unlike frequentist statistics, Bayesian data analysis does not bake in binary decision routines, such as to reject or not to reject a given null hypothesis. Instead, evidence is *quantified*. Assigning less importance to a single value that decides whether a study is published or not might reduce the pressure to pass the threshold at all costs. Moreover, Bayesian statistics provide a broader spectrum for drawing conclusions from the results. The posterior tells us how probable a hypothesis is given the data, $P(H|D)$. In contrast, frequentist hypothesis testing gives the probability of the observed data or more extreme data (denoted by the asterisk) if the null hypothesis were true, $P(D^*|H_0)$. This not only lacks information about the correctness of the alternative hypothesis (which is often the research question) but also that of the null hypothesis itself. While a non-significant $p$-value is no go-ahead for accepting $H_0$, Bayes factors can in fact give evidence *in favor* of $H_0$ by comparing both hypotheses in light of the data.

**2. Control for the error rate**

The most obvious way to reduce the probability of a false-positive is to lower the significance threshold. Indeed, many researchers argue that a 1 in 20 chance of obtaining a false-positive is too high and call for lowering the conventional $\alpha$ threshold from 0.05 to 0.005 [@BenjaminBerger2017]. This call sparked a hot debate in the scientific community. @LakensAdolfi2018 argue that switching to a lower threshold might even have negative consequences on the feasibility of replication studies. Researchers should instead justify the choice of the threshold individually. Further resources on the "$\alpha$-debate" are linked in Section \@ref(app-94-resources).

Regardless of whether you agree with "Team Redefine $\alpha$" or not, it is crucial to further correct the threshold if at least two analyses were conducted on the same data set. Probably the simplest and most conservative method to adjust the family-wise error rate to 0.05 is to use a *Bonferroni* correction, where the per-contrast $\alpha$-level is divided by the number $n$ of tests on the same data set. In the fishy jelly bean study from the previous section, the researchers could have corrected the error rate as follows:

$$
\alpha_{corrected} = \frac{\alpha}{n} = \frac{0.05}{20} = 0.0025
$$

This new cut-off point at 0.0025 ensures an upper bound of 5% for a false-positive result despite multiple testing:

$$
\alpha_{FWER} = 1-(1-0.0025)^{20} \approx 0.049
$$

So, in order to claim a statistically significant link between acne and the jelly bean flavor at hand, the $p$-value in this particular test has to be smaller than 0.0025. 

**3. Increase statistical power**

Power is mainly improved by increasing the underlying effect size, decreasing the standard deviation, increasing the $\alpha$-level, and increasing the sample size (see figure \@ref(fig:ch-94-improve-power)). However, not all factors are feasible, practical, or common. The true effect size is unknown in reality and needs to be estimated when calculating the power. Since it is not influenceable, the true effect size is as big as it gets. The standard deviation can theoretically be reduced (e.g., by controlling for measurement error), but only up to a point. A higher demarcation point for statistical significance simultaneously entails a higher risk of making a false discovery and is thus not desirable at all. 

This leaves us with the sample size. Intuitively put, increasing the number of observations in the sample will provide more information about the population we want to generalize to (think Law of Large Numbers). 100 CogSci students are more representative of the whole CogSci population than only 25 students. Therefore, the more data we collect, the likelier we get to the true average IQ.
Concretely, a larger sample size will decrease the standard error of the mean (SEM), which is the standard deviation of the sample means. With a smaller standard error, smaller effect sizes are detectable.
<!-- ÖÖ: possibly mention relation between sample size, SEM, and SD? -->

The minimum required sample size to detect a given effect size is often calculated with a so-called *power analysis* (see Section \@ref(ch-03-04-hypothesis-significance-errors)). If the power analysis yields a sample size estimate that is difficult to realize, it may be an indicator that the effect size is so minor that the study could even not be worth the effort. If authors still want to conduct their studies, inviting many more participants and thereby prolonging the experiment can conflict with time and financial limitations. According to the motto “where there's a will, there's a way”, authors can team up with other labs to achieve the calculated sample size.

Useful resources and software for power analyses are linked in Section \@ref(app-94-resources).

```{r ch-94-improve-power, echo=FALSE, fig.height=10, fig.cap='Factors that affect the power of a study. **A|** An example of low statistical power. **B|** A larger effect size makes differences easier to detect. **C|** Less variability in the groups makes smaller differences detectable. **D|** A higher $\\alpha$-level increases the probability of rejecting $H_0$.'}
plot_data1 <- tibble(
  x = seq(-8, 12, length.out = 1000),
  null = dnorm(x, sd = 2),
  alternative = dnorm(x, mean = 1, sd = 2)
  ) %>% 
  pivot_longer(cols = 2:3)

a <- plot_data1 %>% 
  ggplot(aes(x = x, y = value, color = name)) + 
  # null hypothesis segment & label
  geom_segment(
    aes(
      x = 0,
      xend = 0,
      y = -0.02,
      yend = dnorm(0, sd = 2)
      ),
    color = "darkgray"
  ) + 
  geom_label(
    aes(
      x = -0.2,
      y = -0.025,
      label = "H_0"
    ),
    color = "darkgray",
    label.size = 0,
    size = 4
  ) +
  # alternative hypothesis segment & label
  geom_segment(
    aes(
      x = 1,
      xend = 1,
      y = -0.02,
      yend = dnorm(0, sd = 2)
      ),
    color = "darkgray"
  ) + 
  geom_label(
    aes(
      x = 1.2,
      y = -0.025,
      label = "H_a"
      ),
    color = "darkgray",
    label.size = 0,
    size = 4
  ) +
  # power area
  geom_area(
    data = filter(
      plot_data1, name == "alternative", 
      x >= qnorm(0.95, sd = 2)
      ), 
    fill = "firebrick",
    color = "firebrick",
    alpha = 0.3
  ) +
  # alpha error area
  geom_area(
    data = filter(
      plot_data1, name == "null", 
      x >= qnorm(0.95, sd = 2)
      ), 
    fill = project_colors[2],
    color = project_colors[2],
    alpha = 0.3
  ) +
  # beta error area
  geom_area(
    data = filter(
      plot_data1, name == "alternative", 
      x <= qnorm(0.95, sd = 2)
      ), 
    fill = project_colors[1],
    color = project_colors[1],
    alpha = 0.3
  ) +
  # main density lines
  geom_line(size = 2) +
  # Power explanation
  annotate(
    geom = 'text', 
    x = 10, 
    y = 0.1595, 
    label = ("Power"), 
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = 3.4,
      xend = 10,
      y = 0.075,
      yend = 0.1475
      ),
    color = "black"
  ) +
  # alpha error explanation
  annotate(
    geom = 'text', 
    x = 10, 
    y = 0.09, 
    label = TeX("$\\alpha$-error", output = 'character'), 
    parse = TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = 3.4,
      xend = 10,
      y = 0.0075,
      yend = 0.08
      ),
    color = "black"
  ) +
  # beta error explanation
  annotate(
    geom = 'text', 
    x = -6, 
    y = 0.1, 
    label = TeX("$\\beta$-error", output = 'character'), 
    parse = TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = -0.5,
      xend = -6,
      y = 0.05,
      yend = 0.09
      ),
    color = "black"
  ) +
  # general cosmetics
  guides(color = F) +
  labs(x = "", y = "") +
  annotate(geom = "text", label = "A", x = -8, y = 0.2, fontface = "bold", size = 5) + 
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )


plot_data2 <- tibble(
  x = seq(-8, 12, length.out = 1000),
  null = dnorm(x, sd = 2),
  alternative = dnorm(x, mean = 4, sd = 2)
  ) %>% 
  pivot_longer(cols = 2:3)

b <- plot_data2 %>% 
  ggplot(aes(x = x, y = value, color = name)) + 
  # null hypothesis segment & label
  geom_segment(
    aes(
      x = 0,
      xend = 0,
      y = -0.02,
      yend = dnorm(0, sd = 2)
      ),
    color = "darkgray"
    ) + 
  geom_label(
    aes(
      x = 0,
      y = -0.025,
      label = "H_0"
      ),
    color = "darkgray",
    label.size = 0,
    size = 4
    ) +
  # alternative hypothesis segment & label
  geom_segment(
    aes(
      x = 4,
      xend = 4,
      y = -0.02,
      yend = dnorm(0, sd = 2)
      ),
    color = "darkgray"
  ) + 
  geom_label(
    aes(
      x = 4,
      y = -0.025,
      label = "H_a"
      ),
    color = "darkgray",
    label.size = 0,
    size = 4
  ) +
  # power area
  geom_area(
    data = filter(
      plot_data2, name == "alternative", 
      x >= qnorm(0.95, sd = 2)
      ), 
    fill = "firebrick",
    color = "firebrick",
    alpha = 0.3
  ) +
  # alpha error area
  geom_area(
    data = filter(
      plot_data2, name == "null", 
      x >= qnorm(0.95, sd = 2)
      ), 
    fill = project_colors[2],
    color = project_colors[2],
    alpha = 0.3
  ) +
  # beta error area
  geom_area(
    data = filter(
      plot_data2, name == "alternative", 
      x <= qnorm(0.95, sd = 2)
      ), 
    fill = project_colors[1],
    color = project_colors[1],
    alpha = 0.3
  ) +
  # main density lines
  geom_line(size = 2) +
  # Power explanation
  annotate(
    geom = 'text', 
    x = 10, 
    y = 0.203, 
    label = ("Power"), 
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = 4.3,
      xend = 10,
      y = 0.105,
      yend = 0.19
      ),
    color = "black"
  ) +
  # alpha error explanation
  annotate(
    geom = 'text', 
    x = 10, 
    y = 0.1, 
    label = TeX("$\\alpha$-error", output = 'character'), 
    parse = TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = 4.3,
      xend = 10,
      y = 0.005,
      yend = 0.09
      ),
    color = "black"
  ) +
  # beta error explanation
  annotate(
    geom='text', 
    x = -6, 
    y = 0.1, 
    label = TeX("$\\beta$-error", output = 'character'), 
    parse = TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = 2,
      xend = -6,
      y = 0.05,
      yend = 0.09
      ),
    color = "black"
  ) +
  # general cosmetics
  guides(color = F) +
  labs(x = "", y = "") +
  annotate(geom = "text", label = "B", x = -8, y = 0.2, fontface = "bold", size = 5) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

plot_data3 <- tibble(
  x = seq(-8, 12, length.out = 1000),
  null = dnorm(x, sd = 0.5),
  alternative = dnorm(x, mean = 1, sd = 0.5)
  ) %>% 
  pivot_longer(cols = 2:3)

c <- plot_data3 %>% 
  ggplot(aes(x = x, y = value, color = name)) + 
  # null hypothesis segment & label
  geom_segment(
    aes(
      x = 0,
      xend = 0,
      y = -0.08,
      yend = dnorm(0, sd = 0.5)
      ),
    color = "darkgray"
  ) + 
  geom_label(
    aes(
      x = -0.2,
      y = -0.1,
      label = "H_0"
      ),
    color = "darkgray",
    label.size = 0,
    size = 4
  ) +
  # alternative hypothesis segment & label
  geom_segment(
    aes(
      x = 1,
      xend = 1,
      y = -0.08,
      yend = dnorm(0, sd = 0.5)
      ),
    color = "darkgray"
  ) + 
  geom_label(
    aes(
      x = 1.2,
      y = -0.1,
      label = "H_a"
      ),
    color = "darkgray",
    label.size = 0,
    size = 4
  ) +
  # power area
  geom_area(
    data = filter(
      plot_data3, name == "alternative", 
      x >= qnorm(0.95, sd = 0.5)
      ), 
    fill = "firebrick",
    color = "firebrick",
    alpha = 0.3
  ) +
  # alpha error area
  geom_area(
    data = filter(
      plot_data3, name == "null", 
      x >= qnorm(0.95, sd = 0.5)
      ), 
    fill = project_colors[2],
    color = project_colors[2],
    alpha = 0.3
  ) +
  # beta error area
  geom_area(
    data = filter(
      plot_data3, name == "alternative", 
      x <= qnorm(0.95, sd = 0.5)
      ), 
    fill = project_colors[1],
    color = project_colors[1],
    alpha = 0.3
  ) +
  # main density lines
  geom_line(size = 2) +
  # Power explanation
  annotate(
    geom = 'text', 
    x = 5, 
    y = 0.810, 
    label = ("Power"), 
    size = 5,
    color = "black",
    label.size = 0
  ) +
    geom_segment(
    aes(
      x = 1.2,
      xend = 5,
      y = 0.46,
      yend = 0.76
      ),
    color = "black"
  ) +
  # alpha error explanation
  annotate(
    geom = 'text', 
    x = 5, 
    y = 0.4, 
    label = TeX("$\\alpha$-error", output = 'character'), 
    parse = TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = 1,
      xend = 5,
      y = 0.06,
      yend = 0.36
      ),
    color = "black"
  ) +
  # beta error explanation
  annotate(
    geom = 'text', 
    x = -5, 
    y = 0.4, 
    label = TeX("$\\beta$-error", output = 'character'), 
    parse = TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = 0.5,
      xend = -5,
      y = 0.1,
      yend = 0.36
      ),
    color = "black"
  ) +
  # general cosmetics
  guides(color = F) +
  labs(x = "", y = "") +
  annotate(geom = "text", label = "C", x = -8, y = 0.8, fontface = "bold", size = 5) + 
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

d <- plot_data1 %>% 
  ggplot(aes(x = x, y = value, color = name)) + 
    # null hypothesis segment & label
  geom_segment(
    aes(
      x = 0,
      xend = 0,
      y = -0.02,
      yend = dnorm(0, sd = 2)
      ),
    color = "darkgray"
  ) + 
  geom_label(
    aes(
      x = -0.2,
      y = -0.025,
      label = "H_0"
      ),
    color = "darkgray",
    label.size = 0,
    size = 4
  ) +
  # alternative hypothesis segment & label
  geom_segment(
    aes(
      x = 1,
      xend = 1,
      y = -0.02,
      yend = dnorm(0, sd = 2)
      ),
    color = "darkgray"
  ) + 
  geom_label(
    aes(
      x = 1.2,
      y = -0.025,
      label = "H_a"
      ),
    color = "darkgray",
    label.size = 0,
    size = 4
  ) +
  # power area
  geom_area(
    data = filter(
      plot_data1, name == "alternative", 
      x >= qnorm(0.90, sd = 2)
      ), 
    fill = "firebrick",
    color = "firebrick",
    alpha = 0.3
  ) +
  # alpha error area
  geom_area(
    data = filter(
      plot_data1, name == "null", 
      x >= qnorm(0.90, sd = 2)
      ), 
    fill = project_colors[2],
    color = project_colors[2],
    alpha = 0.3
  ) +
  # beta error area
  geom_area(
    data = filter(
      plot_data1, name == "alternative", 
      x <= qnorm(0.90, sd = 2)
      ), 
    fill = project_colors[1],
    color = project_colors[1],
    alpha = 0.3
  ) +
  # main density lines
  geom_line(size = 2) +
  # Power explanation
  annotate(
    geom = 'text', 
    x = 10, 
    y = 0.203, 
    label = ("Power"), 
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = 2.8,
      xend = 10,
      y = 0.11,
      yend = 0.19
      ),
    color = "black"
  ) +
  # alpha error explanation
  annotate(
    geom = 'text', 
    x = 10, 
    y = 0.1, 
    label = TeX("$\\alpha$-error", output = 'character'), 
    parse = TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = 2.8,
      xend = 10,
      y = 0.025,
      yend = 0.09
      ),
    color = "black"
  ) +
  # beta error explanation
  annotate(
    geom = 'text', 
    x = -6, 
    y = 0.1, 
    label = TeX("$\\beta$-error", output = 'character'), 
    parse = TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = -0.5,
      xend = -6,
      y = 0.05,
      yend = 0.09
      ),
    color = "black"
  ) +
  # general cosmetics
  guides(color = F) +
  labs(x = "", y = "") +
  annotate(geom = "text", label = "D", x = -8, y = 0.2, fontface = "bold", size = 5) + 
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

cowplot::plot_grid(a,b,c,d, ncol = 1)
```

**4. Correlation $\neq$ Causation**

As already outlined in Chapter \@ref(Chap-02-03-summary-statistics-2D), a positive or negative correlation must not be interpreted as a causal relationship between two measurements $A$ and $B$. In fact, there can also be a third variable $C$ that caused both $A$ and $B$. Let's illustrate erroneous conflation of correlation and causation with an example. It is known that a rapid drop in barometric pressure ($A$) is followed by stormy weather ($B$). This co-occurrence, however, cannot be interpreted as $A$ causing $B$, i.e., the barometric pressure causing a storm. Rather, there is a third factor $C$, that causes both the drop in barometric pressure and stormy weather, namely the falling air pressure that precedes both events.

An observed correlation between two measurements, attributed to either a third (confounding) variable or to random chance, is called a *spurious relationship* or *spurious correlation*. Note that spurious correlations are often not readily identifiable, which is why we never can surely speak of causation. This realization is important to keep in mind when interpreting the results of a study.

### Realigning incentive structures

#### Registered Reports

A Registered Report (RR) is a preregistration that is integrated into the publication process. Here, peer review splits into two stages: one before data collection and one after conducting the experiment. Initially, the manuscript is triaged by an editorial team for rigor, feasibility, and suitability of the research proposal. If approved, the manuscript passes to the first stage of peer review, where the study design, methods, and proposed analyses are assessed in more depth. In addition to other methodological aspects, reviewers assess if proposed studies are adequately powered, requiring the *a priori* statistical power to be at least 90%. In the first stage, peer reviewers preliminary accept the submitted manuscript, ask the researchers to revise or reject it right away. If the manuscript passes the first stage, it is “in principle accepted” (IPA) and guaranteed to be published – irrespective of the results. After data collection and analyses, the manuscript is completed with the results section and discussion and then submitted to stage two peer review. To be admissible, authors are often required to make their anonymized raw data and study materials publicly available and provide a link within the stage two manuscript. In this last stage, the adherence to the manuscript approved in stage one is assessed. If the results are reported thoroughly, and potential deviations from the IPA report are justified and transparent, the paper is published.

Registered Reports eliminate publication bias. By accepting the manuscript in stage one of peer review, the respective journal guarantees the publication of the study, irrespective of whether the outcomes are null results or significant. With results-blind peer review, fewer papers with non-significant results should land in the file drawers^[This is a reference to the "file drawer problem", a term coined in the late seventies. It aptly describes publication bias to the effect that researchers lock non-significant studies away in their file drawers, knowing that there is no prospect of ever being published and thus seen and acknowledged by the scientific community.] and instead be added to the scientific literature. By eliminating publication bias, Registered Reports aim to take off the pressure from researchers to chase significant $p$-values and shift the focus to valuing good quality research that complies with the [hypothetico-deductive model of the scientific method](https://en.wikipedia.org/wiki/Hypothetico-deductive_model).

Two recent studies suggest that Registered Reports are indeed effective in realigning incentive structures. @ScheelSchijen2020 found that 56% of the Registered Reports did not yield statistically significant results. In a study by @AllenMehler2019, the percentage was even higher. The results vary tremendously from the authors’ estimation of 5%-20% of null results in traditional literature. If many journals and authors promote this publication format, it is conceivable that the skewed incentive structure can be normalized entirely. Indeed, the number of journals offering Registered Reports is increasing exponentially. What started with three journals in 2013 – with *Cortex* as the first journal to implement them – has been steadily growing ever since, with currently [273 participating journals](https://www.cos.io/initiatives/registered-reports) at the time of writing (end of 2020).

With realigning incentive structures, it may be reasonable that also a journal’s prestige is evaluated differently. Instead of depending on citations, a journal’s impact could be assessed on how much it promotes open science practices (see [TOP Factor](https://www.cos.io/top)) and replicable research (see [Replicability-Index](https://replicationindex.com/about/)). 

#### Replication initiatives

Direct replications are the cornerstone of science. They assure the validity and credibility of scientific discoveries on which further research can build upon. However, a neophiliac incentive system makes this scrutiny unlikely to be published and may lead researchers to see no sense in attempting direct replications of previous findings. Here are two solution attempts journals may adopt to stop neophilia in favor of preserving psychology's "self-correcting system":

**1. Pottery Barn Rule**

The "Pottery Barn Rule" is a solution attempt proposed by psychologist Sanjay Srivastava [-@Srivastava2012]. According to the motto “you break it, you buy it”, he proposes that once a journal has published a study, it should also be accountable to publish direct replications of the findings. Importantly, this commitment also includes failed replications and replication attempts despite a flawed methodology of the original study. An example of the implementation of such a concept is the Psychology and Neuroscience Section of *Royal Society Open Science*. 

**2. Registered Replication Reports**

A Registered Replication Report (RRR) is a publication format that is entirely dedicated to direct replications. It consists of  multi-lab direct replication studies that aim to precisely estimate effect sizes in studies whose results are highly influential in their field or attracted media attention. The *Association for Psychological Science* (APS) is the first organization to implement and encourage this extension of classic Registered Reports.

### Promote transparency

#### Open Science Badges

To provide an incentive for open science practices, more and more journals are adopting [badges](https://www.cos.io/initiatives/badges). By sharing data, providing materials, or preregistering, the authors “earn” the corresponding badge, which will be depicted in the respective article. The three types of badges are shown in figure \@ref(fig:os-badges). 

```{r os-badges, echo = F, out.width = '40%', fig.cap="Badges to incentivize preregistration, as well as data and material sharing."}
knitr::include_graphics("visuals/open-science-badges.png")
```

This simple solution already shows initial indications of a positive impact. @KidwellLazarević2016 found that implementing badges increases data and material sharing. Although the authors did not include the impact of the preregistration badge in their assessment, it is conceivable that the possibility of earning the badge will result in more preregistrations – or at least awareness of their existence.

#### TOP Guidelines

The **T**ransparency and **O**penness **P**romotion Guidelines comprise open science standards for eight categories. Journals that promote open science practices can implement one or more guidelines into their policy, thereby choosing the level of stringency. For example, if a journal adopts level III (the most stringent level) of the "Data" standard, it requires authors to make their data retrievable for readers. In addition, the journal undertakes to independently reproduce the results before publishing the paper. 

A full list of the TOP Guidelines can be found on the [website of the Center for Open Science](https://www.cos.io/initiatives/top-guidelines).

#### Disclosure statements

Sometimes it doesn't need much to cause a great effect. In fact, it can only take 21 words to enhance integrity in research papers:

> We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study.

This 21-word solution by @SimmonsNelson2012 requests from researchers to honestly report everything in order to render this statement true. For example, if there were data exclusions in a reaction time study, the disclosure statement could be supplemented with a sentence like: "We excluded every individual trial faster than 100 ms and slower than 1500 ms." 

If such disclosure statements were the norm or mandatory, authors who want to conceal details of data collection and analysis would have to actually lie about their disclosure. And deliberate lying probably exceeds the "grey area" in research. 

```{block, type='infobox'}
**Excursion: Improve usability** 

Sharing analysis scripts and (raw) data is certainly a good starting point to facilitate replication efforts. However, it is also important to ensure that code and data are correctly interpreted by the reader. The following three best practices will help improve the usability of your files.

**Add a README file.** A README file is usually the first place to go when exploring a project repository (e.g., on [OSF](https://osf.io/) or [GitHub](https://github.com/)). Ideally, it should give methodological information, a walkthrough on the files and the folder structure of the project, and contact information (especially the full name and a permanent email address). It is also useful to document the software (preferably open source), versioning, and packages needed for analysis.

**Provide metadata.** Metadata, that is, "data about data", refers to structural, descriptive, or administrative properties of the gathered data. It gives, to name a few, information about who collected the data, the time period, and the location of data collection. Furthermore, it refers to the actual content of the data, e.g., what the column names mean and what types of measurements are stored. A detailed description of all measured variables is also called a "Codebook".

**Comment your code.** Reading into someone else's code is hard enough. It gets more time-consuming and nerve-racking if the code is also not supplemented with proper comments. Let the reader know what your code does by providing useful comments and descriptive variable names. This also has the benefit that you have a better understanding of the analysis script when you come back to it at a later point in time.
```

## Chapter summary {#app-94-recap}

In this chapter, we peeked into the abyss of psychology's replication crisis and learned how biases towards significant outcomes fuel the engagement in questionable research practices, ultimately compromising the validity of the finding. We also learned that underpowered studies will not only hardly find true-positives, but also render detected effects useless. Lastly, the lack of transparency, be it in reported methods or keeping data and analysis code private, may account for moderators between the original and replication study. 

Later, we looked at several promising solution attempts and initial prospects of improvement. It is important to note though, that there is no single panacea for all obstacles to good scientific practices. It is rather a *combination* of several measures, such as preregistration/Registered Reports, badges, TOP guidelines, and replication initiatives. The existence and increased implementation of such measures manifest that standards in psychological research are about to change. The replication crisis is followed by a credibility revolution, prompting psychology to reward what should be rewarded - scientific rigor and integrity. 

```{block, type='recap'}

Our little journey through the replication crisis ends here. Here are the most important points to keep in mind:

**Limit exploitation of researcher degrees of freedom**

* Be aware of cognitive biases, such as confirmation bias, apophenia, and hindsight bias
* Raise awareness and be aware of QRP's and their implications for the validity of research outcomes
* Preregister your study

**Limit statistical fallacies**

* Learn more about Null Hypothesis Significance Testing 
* Consider adopting a Bayesian approach to hypothesis testing
* Control for error rates
* Make sure that your study has sufficient power (conduct a power analysis)
* Keep in mind that correlation does not imply causation

**Eliminate publication bias**

* Consider proposing an upcoming study as a Registered Report
* Replicate!

**Embrace transparency**

* Make your raw data, study materials, and analysis scripts available, along with
  - README files
  - Metadata/Codebooks
  - Descriptive comments and variable names in analysis scripts
* You did not engage in selective reporting? Awesome! Let the reader know by explicitly writing a disclosure statement (e.g., 21-word solution)
```

## Further resources {#app-94-resources}

**Incentives**

More information on Registered Reports: 

* https://www.cos.io/initiatives/registered-reports

Assessing the effectiveness of Registered Reports:

* Chambers, C. D., Tzavella, L. (2020). Registered Reports: Past, Present and Future. https://doi.org/10.31222/osf.io/43298

* Hardwicke, T. E., Ioannidis, J. P. A. (2018). Mapping the universe of registered reports. *Nature Human Behaviour*, 2, 793-796. https://doi.org/10.1038/s41562-018-0444-y

Workflow of Registered Replication Reports (example AMPPS): 

* https://www.psychologicalscience.org/publications/ampps/rrr-guidelines

Brian Nosek on the importance of replication:

* https://www.youtube.com/watch?v=wsRmyW8GmJs

**Statistics**

<u>Researcher degrees of freedom and QRP's</u>

Two papers on the reasons behind false discoveries:

* Simmons, J. P., Nelson, L. D., Simonsohn, U. (2011). False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. *Psychological Science*, 22(11), 1359-1366. https://doi.org/10.1177/0956797611417632

* Ioannidis, J. P. A. (2005) Why Most Published Research Findings Are False. *PLoS Medicine*, 2(8), e124. https://doi.org/10.1371/journal.pmed.0020124

More on the $\alpha$-debate: 

* Morey, R. D. (2018, Jan 1). Redefining statistical significance: the statistical arguments [blog post]. Retrieved from https://medium.com/@richarddmorey/redefining-statistical-significance-the-statistical-arguments-ae9007bc1f91

* de Ruiter, J. (2019). Redefine or justify? Comments on the alpha debate. *Psychonomic Bulletin & Review*, 26, 430-433. https://doi.org/10.3758/s13423-018-1523-9

Overview of methods to adjust the family-wise error rate: 

* Chen, S.-Y., Feng, Z., Yi, X. (2017). A general introduction to adjustment for multiple comparisons. *Journal Of Thoracic Disease*, 9(6), 1725-1729. https://doi.org/10.21037/jtd.2017.05.34

<u>Statistical power</u>

An introductory video on statistical power by the OSF:

* https://www.youtube.com/watch?v=-ZU7fbvSJ60

A video on consequences of low statistical power by the OSF: 

* https://www.youtube.com/watch?v=7daQRvRO-NE&t=20s

An R package to assess Type S and Type M error rates: 

* https://cran.r-project.org/web/packages/retrodesign/index.html

Correcting for inflated effect sizes fueled by publication bias:

* Simonsohn, U., Nelson, L. D., Simmons, J. P. (2014). $p$-Curve and Effect Size: Correcting for Publication Bias Using Only Significant Results. *Perspectives on Psychological Science*, 9(6), 666-681. https://doi.org/10.1177/1745691614553988

Frequentist power analysis in R: 

* https://www.statmethods.net/stats/power.html

G*Power - A Software for frequentist power analysis: 

* https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower.html

A blogpost on Bayesian power analysis: 

* https://solomonkurz.netlify.app/post/bayesian-power-analysis-part-i/

**Transparency**

A survey that aimed to gain insight into why authors keep their data private:

* Houtkoop, B. L., Chambers, C., Macleod, M., Bishop, D. V. M., Nichols, T. E., Wagenmakers, E.-J. (2018). Data Sharing in Psychology: A Survey on Barriers and Preconditions. *Advances in Methods and Practices in Psychological Science*, 1(1), 70-85. https://doi.org/10.1177/2515245917751886

More information on open science badges: 

* https://www.cos.io/initiatives/badges

More information on TOP Guidelines: 

* https://www.cos.io/initiatives/top-guidelines

A platform for disclosure statements: 

* https://psychdisclosure.org/

Guidelines to write README files and Metadata: 

* https://data.research.cornell.edu/content/readme

More on the hows and whys of sharing:

* Klein, O., Hardwicke, T. E., Aust, F., Breuer, J., Danielsson, H., Hoeflich Mohr, A., IJzerman, H., Nilsonne, G., Vanpaemel, W., Frank, M. C. (2018). A practical guide for transparency in psychological science. *Collabra: Psychology*, 4(1), 20.
https://doi.org/10.1525/collabra.158

**Miscellaneous**

A comic about the replication crisis: 

* https://thenib.com/repeat-after-me/?t=defau

A youtube-playlist of 5-10 minute videos on open science:

* https://www.youtube.com/watch?v=1rFWeTryiW4&list=PLtAL5tCifMi5zG70dslERYcGApAQcvj1s

A video on norms in science:

* https://www.youtube.com/watch?v=00btFojQPiU&t=36s

Highly recommended resources that provide a broad overview of the replication crisis, contributing factors, and solution attempts:

* Chambers, C. (2017). *The Seven Deadly Sins of Psychology: A Manifesto for Reforming the Culture of Scientific Practice*. Princeton University Press. https://doi.org/10.1515/9781400884940

* Munafò, M. R., Nosek, B. A., Bishop, D. V., Button, K. S., Chambers, C. D., Du Sert, N. P., ... Ioannidis, J. P. (2017). A manifesto for reproducible science. *Nature Human Behaviour*, 1(1), 0021. https://doi.org/10.1038/s41562-016-0021
