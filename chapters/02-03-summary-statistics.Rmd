
# Summary statistics {#Chap-02-03-summary-statistics}

<hr>

<div style = "float:right; width:45%;">
<img src="visuals/badge-summary-statistics.png" alt="badge summary statistics">  
</div>  

A **summary statistics** is a single number that represents one aspect of a possibly much more complex chunk of data. This single number might, for example, indicate the maximum or minimum value of a vector of a billion observations. The large data set (1 billion observations) is reduced to a single number, which represents one aspect of that data. Summary statistics are, as a general (but violable) rule, many-to-one (surjections). They compress complex information into a simpler, compressed representation.

Summary statistics are useful for understanding the data at hand, for communication about a data set, but also for subsequent statistical analyses. As we will see later on, many statistical tests look at a summary statistic $x$, which is a single value derived from data set $D$, and compare $x$ to an expectation of what $x$ should be like if the process that generated $D$ really had a particular property. For the moment, however, we use summary statistics only to get comfortable with data: understanding it better and gaining competence to manipulate it.

Chapter \@ref(Chap-02-03-summary-statistics-counts) first uses the [Bio-Logic Jazz-Metal data set](#app-93-data-sets-BLJM) to look at a very intuitive class of summary statistics for categorical data, namely counts and proportions.
Chapter \@ref(Chap-02-03-summary-statistics-1D) introduces summary statistics for simple, one-dimensional vectors with numeric information. In doing so, we will also learn about *nested tibbles*.
Chapter \@ref(Chap-02-03-summary-statistics-2D) looks at measures of the relation between two numerical vectors, namely *covariance* and *correlation*. These last two chapters use the [avocado data set](#app-93-data-sets-avocado).


```{block, type='infobox'}
The learning goals for this chapter are:

- become able to compute counts and frequencies for categorical data
- understand and be able to compute summary statistics for one-dimensional metric data
  - measures of central tendency
    - mean, mode, median
  - measures of dispersion
    - variance, standard deviation, quantiles
  - non-parametric estimates of confidence
    - bootstrapped CI of mean
- understand and be able to compute for two-dimensional metric data
  - covariance
  - Bravais-Pearson correlation
```




## Counts and proportions {#Chap-02-03-summary-statistics-counts}

<div style = "float:right; width:24%;">
<img src="visuals/badge-BLJM.png" alt="badge-BLJM">  
</div>  

Very familiar instances of summary statistics are counts and frequencies. While there is no conceptual difficulty in understanding these numerical measures, we have yet to see how to obtain counts for categorical data in R. The [Bio-Logic Jazz-Metal data set](app-93-data-sets-BLJM) provides nice material for doing so. If unfamiliar with the data and the experiment that generated it, please have a look at Appendix Chapter \@ref(app-93-data-sets-BLJM).

### Loading and inspecting the data

We load the preprocessed data immediately (see Appendix Chapter \@ref(app-93-data-sets-BLJM) for details on how this preprocessing was performed).

<!-- TODO uniform convention 'data' first? or source identifier first? -->

```{r echo = F}
data_BLJM_processed <- read_csv("data_sets/bio-logic-jazz-metal-data-processed.csv",
                                col_types = cols(
                                  submission_id = col_double(),
                                  condition = col_character(),
                                  response = col_character()
                                ))
```

```{r, eval = F}
data_BLJM_processed <- read_csv(url('https://raw.githubusercontent.com/michael-franke/intro-data-analysis/master/data_sets/bio-logic-jazz-metal-data-processed.csv'))
```

The preprocessed data lists, for each participant (in column `submission_id`) the binary choice (in column `response`) given for a particular condition (in column `condition`).

```{r}
head(data_BLJM_processed)
```

### Obtaining counts with `n`, `count` and `tally`

To obtain counts, the `dplyr` package offers functions `n`, `count` and `tally`, among others.^[Useful base R functions for obtaining counts are `table` and `prop.table`.] The function `n` does not take arguments and is useful for counting rows. It works inside of `summarise` and `mutate` and is usually applied to grouped data sets. For example, we can get a count of how many observations the data in `data_BLJM_processed` contains for each condition by first grouping by variable `condition` and then calling `n` (without arguments) inside of `summarise`:

```{r}
data_BLJM_processed %>% 
  group_by(condition) %>% 
  summarise(nr_observation_per_condition = n()) %>% 
  ungroup()
```

Notice that calling `n` without grouping just gives you the number of rows in the data set:

```{r}
data_BLJM_processed %>% summarize(n_rows = n())
```

This can also be obtained simply by (although in a different output format!):

```{r}
nrow(data_BLJM_processed)
```

Counting can be helpful also when getting acquainted with a data set or when checking whether the data is complete. For example, we can verify that every participant in the experiment contributed three data points like so:

```{r}
data_BLJM_processed %>% 
  group_by(submission_id) %>% 
  summarise(nr_data_points = n())
```

The functions `tally` and `count` are essentially just convenience wrappers around `n`. While `tally` expects that the data is already grouped in the relevant way, `count` takes a column specification as an argument and does the grouping (and final ungrouping) implicitly.

For instance, the following code blocks produce the same output, one using `n`, the other using `count`, namely the total number of times a particular response has been given in a particular condition:

```{r}
data_BLJM_processed %>% 
  group_by(condition, response) %>% 
  summarise(n = n())
```


```{r}
data_BLJM_processed %>% 
  # function`count` is masked by another package, must call explicitly
  dplyr::count(condition, response)
```

So, these counts suggest that there is an overall preference for mountains over beaches, Jazz over Metal and Biology over Logic. Who would have known!?

These counts are overall numbers. They do not tell us anything about any potentially interesting relationship between preferences. So, let's have a closer look at the number of people who selected which music-subject pair. We collect these counts in variable `BLJM_associated_counts`. We first need to pivot the data, using `pivot_wider`, to make sure each participant's choices are associated with each other, and then take the counts of interest:

```{r}
BLJM_associated_counts <- data_BLJM_processed %>% 
  select(submission_id, condition, response) %>% 
  pivot_wider(names_from = condition, values_from = response) %>% 
  # drop the Beach-vs-Mountain condition
  select(-BM) %>% 
  dplyr::count(JM,LB) 
BLJM_associated_counts
```

We can also produce a table of proportions from this, simply by dividing the column called `n` by the total number of observations, i.e., by `sum(n)`. We can also flip the table around into a more convenient (though messy) representation:

```{r}
BLJM_associated_counts %>% 
  # look at relative frequency, not total counts
  mutate(n = n / sum(n)) %>% 
  pivot_wider(names_from = LB, values_from = n)
```

Eye-balling this table of relative frequencies, we might indeed hypothesize that preference for musical style is not independent of preference for an academic subject. The impression is corroborated by looking at the plot in Figure \@ref(fig:chap-02-03-BLJM-proportions). More on this later!

```{r chap-02-03-BLJM-proportions, fig.cap="Proportions of jointly choosing a musical style and an academic subfield in the Bio-Logic Jazz-Metal data set.", echo = F}
BLJM_associated_counts %>% 
  ggplot(aes(x = LB, y = n / sum(n), color = JM, shape = JM, group = JM)) +
  geom_point(size = 3) + geom_line() +
  labs(
    title = "Proportion of choices of each music+subject pair",
    x = "",
    y = "")
```

## Central tendency and dispersion {#Chap-02-03-summary-statistics-1D}

This section will look at two types of summary statistics: measures of central tendency and measures of dispersion.

**Measures of central tendency** map a vector of observations onto a single number that represents, roughly put, "the center". Since what counts as a "center" is ambiguous, there are several measures of central tendencies. Different measures of central tendencies can be more or less adequate for one purpose or another. The type of variable (nominal, ordinal or metric, for instance) will also influence the choice of measure. We will visit three prominent measures of central tendency here: *(arithmetic) mean*, *median* and *mode*.

**Measures of dispersion** indicate how much the observations are spread out around, let's say, "a center". We will visit three prominent measures of dispersion: the *variance*, the *standard deviation* and *quantiles*. 

To illustrate these ideas, consider the case of a numeric vector of observations. Central tendency and dispersion together describe a (numeric) vector by giving indicative information about the point around which the observations spread, and how far away from that middle point they tend to lie. Fictitious examples of observation vectors with higher or lower central tendency and higher or lower dispersion are given in Figure \@ref(fig:ch-02-03-dispersion-central-tendency).

```{r ch-02-03-dispersion-central-tendency, echo = F, , fig.cap="Fictitious data points with higher/lower central tendencies and higher/lower dispersion. NB: points are 'jittered' along the vertical dimension for better visibility; only the horizontal dimension is relevant here."}

n_samples = 300

samples <- tibble(
  x = c(rnorm(n = n_samples, mean = 0, sd = 1),
        rnorm(n = n_samples, mean = 0, sd = 3),
        rnorm(n = n_samples, mean = 5, sd = 1),
        rnorm(n = n_samples, mean = 5, sd = 3)),
  cent_tend    = rep(c("lower central tendency","higher central tendency"), each = 2*n_samples),
  dispersion   = rep(rep(c("lower dispersion"," higher dispersion"), times = 2), each = n_samples)
) %>% 
  group_by(cent_tend, dispersion) %>% 
  mutate(
    mean_emp = mean(x) %>% round(3),
    sd_emp = sd(x) %>% round(3)
  )

samples %>% 
  ggplot(aes(x = x, y = 1)) +
  geom_jitter(alpha = 0.5, color = "skyblue") +  facet_grid(cent_tend ~ dispersion) +
  xlab('') +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank())

```

### The data for the remainder of the chapter

<div style = "float:right; width:24%;">
<img src="visuals/badge-avocado.png" alt="badge-avocado">
</div>

In the remainder of this chapter, we will use the [avocado data set](#app-93-data-sets-avocado), a very simple and theory-free example in which we can explore two metric variables: the average price at which avocados were sold during specific intervals of time and the total amount of avocados sold. Please check Appendix Chapter \@ref(app-93-data-sets-avocado) for more information on this data set.

We load the data into a variable named `avocado_data` but also immediately rename some of the columns to have more convenient handles (see also Appendix Chapter \@ref(app-93-data-sets-avocado)):

```{r, echo = F}
avocado_data <- read_csv('data_sets/avocado.csv', 
                         col_types = cols(
                           X1 = col_double(),
                           Date = col_date(format = ""),
                           AveragePrice = col_double(),
                           `Total Volume` = col_double(),
                           `4046` = col_double(),
                           `4225` = col_double(),
                           `4770` = col_double(),
                           `Total Bags` = col_double(),
                           `Small Bags` = col_double(),
                           `Large Bags` = col_double(),
                           `XLarge Bags` = col_double(),
                           type = col_character(),
                           year = col_double(),
                           region = col_character()
                         )) %>% 
  # remove currently irrelevant columns
  select( -X1 , - contains("Bags"), - year, - region) %>% 
  # rename variables of interest for convenience
  rename(
    total_volume_sold = `Total Volume`,
    average_price = `AveragePrice`,
    small  = '4046',
    medium = '4225',
    large  = '4770',
  )
```

```{r, eval = F}
avocado_data <- read_csv(url('https://raw.githubusercontent.com/michael-franke/intro-data-analysis/master/data_sets/avocado.csv')) %>% 
  # remove currently irrelevant columns
  select( -X1 , - contains("Bags"), - year, - region) %>% 
  # rename variables of interest for convenience
  rename(
    total_volume_sold = `Total Volume`,
    average_price = `AveragePrice`,
    small  = '4046',
    medium = '4225',
    large  = '4770',
  )
```

We can then take a glimpse:

```{r}
avocado_data
```

The columns that will interest us the most in this chapter are:

+ `average_price` - average price of a single avocado
+ `total_volume_sold` - total number of avocados sold
+ `type` - whether the price/amount is for a conventional or an organic avocado

In particular, we will look at summary statistics for the `average_price` and `total_volume_sold`, either for the whole data set or independently for each type of avocado. Notice that both of these variables are numeric. They are vectors of numbers, each representing an observation.


### Measures of central tendency

#### The (arithmetic) mean

If $\vec{x} = \langle x_1, \dots , x_n \rangle$ is a vector of $n$ observations with $x_i \in \mathbb{R}$ for all $1 \le i \le n$, the (arithmetic) **mean** of $x$, written $\mu_{\vec{x}}$, is defined as 

$$\mu_{\vec{x}} = \frac{1}{n}\sum_{i=1}^n x_i\,.$$

The arithmetic mean can be understood intuitively as **the center of gravity**. If we place a marble on a wooden board for every $x_i$ such that every marble is equally heavy and the differences between all data measurements are identical to the distances between the marbles, the arithmetic mean is where you can balance the board with the tip of your finger. 

<div class="infobox">
**Example.** The mean of the vector $\vec{x} = \langle 0, 3, 6, 7\rangle$ is $\mu_{\vec{x}} = \frac{0 + 3 + 6 + 7}{4} = \frac{16}{4} = 4\,.$ The black dots in the graph below show the data observations, and the red cross indicates the mean. Notice that the mean is clearly *not* the mid-point between the maximum and the minimum (which here would be 3.5).

```{r, echo = F, fig.height=1.5}
tibble(x = c(0, 3, 6, 7 ), mean = mean(x)) %>% 
  ggplot(aes(x = x, y = 0), size = 5) +
  ylim(-0.1,0.1) +
  ylab('') + xlab('') +
  geom_point() + 
  theme(
    axis.title.y = element_blank(),
    axis.text.y  = element_blank(),
    axis.ticks.y = element_blank(),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank()
  ) +
  geom_point(aes(x = mean, y = 0), color = "firebrick", shape = "X", size = 5) +
  scale_x_continuous(breaks=seq(0, 7, 1))



```

</div>

To calculate the mean of a large vector, R has a built-in function `mean`, which we have in fact used frequently before. Let's use it to calculate the mean of the variable `average_price` for different types of avocados:

```{r}
avocado_data %>% 
  group_by(type) %>% 
  summarise(
    mean_price = mean(average_price)
  )
```

Unsurprisingly, the overall mean of the observed prices is (numerically) higher for organic avocados than for conventional ones.

```{block2, type='infobox'}
<div style = "float:right; width:20%;">
  <img src="visuals/fool.png" alt="kid-fool">
  </div>
  **Excursion.** It is also possible to conceptualize the arithmetic mean as the **expected value** when sampling from the observed data. This is useful for linking the mean of a data sample to the expected value of a random variable, a concept we will introduce in Chapter \@ref(Chap-03-01-probability). Suppose you have gathered the data $\vec{x} = \langle 0, 3, 6, 7\rangle$. What is the expected value that you think you will obtain if you sample from this data vector once? -- Wait! What does that even mean? Expected value? Sampling once?
  
  Suppose that some joker from around town invites you for a game. The game goes like this. The joker puts a ball in an urn, one for each data observation. The joker writes the observed value on the ball corresponding to that value. You pay the joker a certain amount of money to be allowed to draw one ball from the urn. The balls are indistinguishable, and the process of drawing is entirely fair. You receive the number corresponding to the ball you drew paid out in silver coins. (For simplicity, we assume that all numbers are non-negative, but that is not crucial. If a negative number is drawn, you just have to pay the joker that amount.)

How many silver coins would you maximally pay to play one round? Well, of course, no more than four (unless you value gaming on top of silver)! This is because 4 is the expected value of drawing once. This, in turn, is because every ball has a chance of $0.25$ of being drawn. So you can expect to earn 0 silver with a 25% chance, 3 with a 25% chance, 6 with a 25% chance and 7 with a 25% chance. In this sense, the mean is the expected value of sampling once from the observed data.
```

#### The median

If $\vec{x} = \langle x_1, \dots , x_n \rangle$ is a vector of $n$ data observations from an at least ordinal measure and if $\vec{x}$ is ordered such that for all $1 \le i < n$ we have $x_i \le x_{i+1}$, the **median** is the value $x_i$ such that the number of data observations that are bigger or equal to $x_i$ and the number of data observations that are smaller or equal to $x_i$ are equal. Notice that this definition may yield no unique median. In that case, different alternative strategies are used, depending on the data type at hand (ordinal or metric). (See also the example below.) The median corresponds to the 50% quartile, a concept introduced below.

<div class="infobox">
**Example.** The median of the vector $\vec{x} = \langle 1=0, 3, 6, 7\rangle$ does not exist by the definition given above. However, for metric measures, where distances between measurements are meaningful, it is customary to take the two values "closest to where the median should be" and average them. In the example at hand, this would be $\frac{3 + 6}{2} = 4.5$. The plot below shows the data points in black, the mean as a red cross (as before) and the median as a blue circle.

```{r, echo = F, fig.height=1.5}
tibble(x = c(0, 3, 6, 7 ), mean = mean(x), median = median(x)) %>% 
  ggplot(aes(x = x, y = 0), size = 5) +
  ylim(-0.1,0.1) +
  ylab('') + xlab('') +
  geom_point() + 
  theme(
    axis.title.y = element_blank(),
    axis.text.y  = element_blank(),
    axis.ticks.y = element_blank(),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank()
  ) +
  geom_point(aes(x = mean, y = 0), color = "firebrick", shape = "X", size = 5) +
  geom_point(aes(x = median, y = 0), color = "skyblue", shape = "O", size = 5) +
  scale_x_continuous(breaks=seq(0, 7, 1))



```

</div>

The function `median` from base R computes the median of a vector. It also takes an ordered factor as an argument.

```{r}
median(c(0, 3, 6, 7 ))
```

To please the avocados, let's also calculate the median price of both types of avocados and compare these to the means we calculated earlier already:

```{r}
avocado_data %>% 
  group_by(type) %>% 
  summarise(
    mean_price   = mean(average_price),
    median_price = median(average_price)
  )
```

<!-- TODO compare mean and median -> homework -->
<!-- TODO: describe "how to mislead with statistics" problem in an "excursion box" -->

#### The mode

The **mode** is the value that occurred most frequently in the data. While the mean is only applicable to metric variables, and the median only to variables that are at least ordinal, the mode is only reasonable for variables that have a finite set of different possible observations (nominal or ordinal). 

There is no built-in function in R to return the mode of a (suitable) vector, but it is easily retrieved by obtaining counts.

<!-- HW: write a function that returns the mode -->

### Measures of dispersion

#### Variance

The variance is a widely used and very useful measure of dispersion for metric data. The variance $\text{Var}(\vec{x})$ of a vector of metric observations $\vec{x}$ of length $n$ is defined as the average of the squared distances from the mean:

$$\text{Var}(\vec{x}) = \frac{1}{n} \sum_{i=1}^n (x_i - \mu_{\vec{x}})^2$$

<div class="infobox">
**Example.** The variance of the vector $\vec{x} \ \langle 0, 3, 6, 7 \rangle$ is computed as:

$$\text{Var}(\vec{x}) = \frac{1}{4} \ \left ( (0-4)^2 + (3-4)^2 + (6-4)^2 + (7-4)^2 \right ) = $$
$$ \frac{1}{4} \ (16 + 1 + 4 + 9) = \frac{30}{4} = 7.5$$

</div>


Figure \@ref(fig:ch-02-03-variance-rectangles) shows a geometric interpretation of variance for the running example of the vector $\vec{x} \ \langle 0, 3, 6, 7 \rangle$. 

```{r ch-02-03-variance-rectangles, echo = F, fig.cap="Geometrical interpretation of variance. Four data points (black dots) and their mean (red cross) are shown, together with the squares whose sides are the differences between the observed data points and the mean. The numbers in white give the area of each square, which is also indicated by the coloring of each rectangle."}
tibble(
  x = c(0,3,7,6), 
  y =0,
  xmin = map_dbl(x, function(i) {min(i, mean(x))}),
  ymin = y,
  xmax = map_dbl(x, function(i) {max(i, mean(x))}),
  ymax = abs(x - mean(x)) * c(1,-1,1,-1),
  rect_area = abs(xmax - xmin) * abs(ymax - ymin),
  x_label_pos = c(2, 3.5, 5, 5.5),
  y_label_pos = c(2, -0.5, -1, 1.5)
) %>% 
  ggplot() +
  geom_rect(
    aes(
      xmin = xmin,
      ymin = ymin,
      xmax = xmax,
      ymax = ymax,
      fill = - rect_area
    )
  ) +
  geom_point(aes(x = x, y = 0), size = 5) +
  geom_point(aes(x = mean(x), y = 0), shape = "X", color = "firebrick", size = 5) +
  ylab('') +
  geom_text(aes(x = x_label_pos, y = y_label_pos, label = rect_area), color = "white") +
  ggtitle("Geometric visualization of variance") +
  theme(
    legend.pos = "none"
  )
```

We can calculate the variance in R explicitly:

```{r}
x <- c(0, 3, 6, 7)
sum((x - mean(x))^2) / length(x)
```

There is also a built-in function `var` from base R. Using this we get a different result though:

```{r}
x <- c(0, 3, 6, 7)
var(x)
```

This is because `var` computes the variance by a slightly different formula to obtain an **unbiased estimator** of the variance for the case that the mean is not known but also estimated from the data. The formula for the unbiased estimator that R uses simply replaces the $n$ in the denominator by $n-1$:^[For the current purpose, it is not important what biased or unbiased estimator is and why this subtle change in the formula matters. We will come back to the issue of estimation in Chapter \@ref(ch-03-04-parameter-inference).]

$$\text{Var}(\vec{x}) = \frac{1}{n-1} \sum_{i=1}^n (x_i - \mu_{\vec{x}})^2$$



#### Standard deviation

The standard deviation $\text{SD}(\vec{x})$ or numeric vector $\vec{x}$ is just the square root of the variance:

$$ \text{SD}(\vec{x}) = \sqrt{\text{Var}(\vec{x})} = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \mu_{\vec{x}})^2}$$

Let's calculate the (biased) variance and standard deviation for the `average_price` of different types of avocados:

```{r}
avocado_data %>% 
  group_by(type) %>% 
  summarize(
    variance_price = var(average_price),
    stddev_price   = sd(average_price),
  )
```


#### Quantile

For a vector $\vec{x}$ of at least ordinal measures, we can generalize the concept of a median to an arbitrary quantile. A $k$% quantile is the element $x_i$ in $\vec{x}$, such that $k$% of the data in $\vec{x}$ lies below $x_i$. If this definition does not yield a unique element for some $k$% threshold, similar methods to what we saw for the median are applied. 


We can use the base R function `quantile` to obtain the 10%, 25%, 50% and 85% quantiles (just arbitrary picks) for the `average_price` in the avocado data set:

```{r}
quantile(
  # vector of observations
  x = avocado_data$average_price, 
  # which quantiles 
  probs = c(0.1, 0.25, 0.5, 0.85)
)
```

This tells us, for instance, that only about ten percent of the data observations had prices lower than \$0.93.

### Quantifying confidence with bootstrapping


[Bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) is an elegant way to obtain measures of confidence for summary statistics. These measures of confidence can be used for parameter inference, too. We will discuss parameter inference at length in Section \@ref(ch-03-04-parameter-inference). In this course, we will not use bootstrapping as an alternative approach to parameter inference. We will, however, follow a common practice (at least in some areas of Cognitive Psychology) to use **bootstrapped 95% confidence intervals of the mean** as part of descriptive statistics, i.e., in summaries and plots of the data.

The bootstrap is a method from a more general class of algorithms, namely so-called **resampling methods**. The general idea is, roughly put, that we treat the data at hand as the true representation of reality. We then imagine that we run an experiment on that (restricted, hypothetical) reality. We then ask ourselves: what would we estimate (e.g., as a mean) in any such hypothetical experiment. The more these hypothetical measures derived from hypothetical experiments based on a hypothetical reality differ, the less confident we are in the estimate. Sounds weird, but is mind-blowingly elegant.

An algorithm for constructing a 95% confidence interval of the mean of vector $D$ of numeric data with length $k$ looks as follows:

1. take $k$ samples from $D$ with replacement, call this $D^{\textrm{rep}}$^[$D^{\textrm{rep}}$ is short for "repeated Data". We will use this concept more later on. The idea is that we consider "hypothetical data", which we have not perceived, but which we might have. Repeated data is (usually) of the same shape and form as the original, observed data, which is also sometimes noted as $D^{\textrm{obs}}$ for clarity in comparison to $D^{\textrm{rep}}$.]
2. calculate the mean $\mu(D^{\textrm{rep}})$ of the newly sampled data
3. repeat steps 1 and 2 to gather $r$ means of different resamples of $D$; call the result vector $\mu_{\textrm{sampled}}$
4. the boundaries of the 95% inner quantile of $\mu_{\textrm{sampled}}$ are the bootstrapped 95% confidence interval of the mean

The higher $r$, i.e., the more samples we take, the better the estimate. The higher $k$, i.e., the more observations we have to begin with, the less variable the means $\mu(D^{\textrm{rep}})$ of the resampled data will usually be. Hence, usually, the higher $k$, the smaller the bootstrapped 95% confidence interval of the mean.

Here is a convenience function that we will use throughout the book to produce bootstrapped 95% confidence intervals of the mean:

```{r}
## takes a vector of numbers and returns bootstrapped 95% ConfInt
## for the mean, based on `n_resamples` re-samples (default: 1000)
bootstrapped_CI <-  function(data_vector, n_resamples = 1000) {
  resampled_means <- map_dbl(1:n_resamples, function(i) {
    mean(sample(x = data_vector, 
                size = length(data_vector), 
                replace = T)
    )
  }
  )
  tibble(
    'lower' = quantile(resampled_means, 0.025),
    'mean'  = mean(data_vector),
    'upper' = quantile(resampled_means, 0.975)
  ) 
}
```

Applying this method to the vector of average avocado prices we get:

```{r}
bootstrapped_CI(avocado_data$average_price)
```

Notice that, since `average_price` has length `r length(avocado_data$average_price)`, i.e., we have $k = `r length(avocado_data$average_price)`$ observations in the data, the bootstrapped 95% confidence interval is rather narrow. Compare this against a case of $k = 300$ obtained by only looking at the first 300 entries in `average_price`:

```{r}
# first 300 observations of `average price` only
smaller_data <- avocado_data$average_price[1:300]
bootstrapped_CI(smaller_data)
```

The mean is different (because we are looking at earlier time points), but importantly, the interval is larger because, with only 300 observations, we have less confidence in the estimate.

#### Excursion: Summary functions with multiple outputs, using nested tibbles

To obtain summary statistics for different groups of a variable, we can use the function `bootstrapped_CI` conveniently in concert with **nested tibbles**, as demonstrated here:

```{r}
avocado_data %>%
  group_by(type) %>%
  # nest all columns except grouping-column 'type' in a tibble
  # the name of the new column is 'price_tibbles'
  nest(.key = "price_tibbles") %>% 
  # collect the summary statistics for each nested tibble
  # the outcome is a new column with nested tibbles
  summarise(
    CIs = map(price_tibbles, function(d) bootstrapped_CI(d$average_price))
  ) %>% 
  # unnest the newly created nested tibble
  unnest(CIs)
```


Using nesting, in this case, is helpful because we only want to run the bootstrap function once, but we need both of the numbers it returns. The following explains nesting based on this example.

To understand what is going on with nested tibbles, notice that the `nest` function in this example creates a nested tibble with just two rows, one for each value of the variable `type`, each of which contains a tibble that contains all the data. The column `price_tibbles` in the first row contains the whole data for all observations for conventional avocado:

```{r, eval = T, warnings = FALSE, message=FALSE}
avocado_data %>%
  group_by(type) %>%
  # nest all columns except grouping-column 'type' in a tibble
  # the name of the new column is 'price_tibbles'
  nest(.key = "price_tibbles") %>% 
  # extract new column with tibble
  pull(price_tibbles) %>% 
  # peak at the first entry in this vector
  .[1] %>%  head()
```

After nesting, we call the custom function `bootstrapped_CI` on the variable `average_price` inside of every nested tibble, so first for the conventional, then the organic avocados. The result is a nested tibble. If we now look inside the new column `CI`, we see that its cells contain tibbles with the output of each call of `bootstrapped_CI`:

```{r, eval = T}
avocado_data %>%
  group_by(type) %>%
  # nest all columns except grouping-column 'type' in a tibble
  # the name of the new column is 'price_tibbles'
  nest(.key = "price_tibbles") %>% 
  # collect the summary statistics for each nested tibble
  # the outcome is a new column with nested tibbles
  summarise(
    CIs = map(price_tibbles, function(d) bootstrapped_CI(d$average_price))
  ) %>%
  # extract new column vector with nested tibbles
  pull(CIs) %>% 
  # peak at the first entry
  .[1] %>%  head()
```

Finally, we unnest the new column `CIs` to obtain the final result (code repeated from above):


```{r}
avocado_data %>%
  group_by(type) %>%
  # nest all columns except grouping-column 'type' in a tibble
  # the name of the new column is 'price_tibbles'
  nest(.key = "price_tibbles") %>% 
  # collect the summary statistics for each nested tibble
  # the outcome is a new column with nested tibbles
  summarise(
    CIs = map(price_tibbles, function(d) bootstrapped_CI(d$average_price))
  ) %>% 
  # unnest the newly created nested tibble
  unnest(CIs)
```


<!-- HW compare bootstrapped CI with variance -->


## Co-variance & correlation {#Chap-02-03-summary-statistics-2D}

### Covariance

Let $\vec{x}$ and $\vec{y}$ be two vectors of numeric data of the same length, such that all pairs of $x_i$ and $y_i$ are associated observations. For example, the vectors `avocado_data$total_volume_sold` and `avocado_data$average_price` would be such vectors. The covariance between $\vec{x}$ and $\vec{y}$ measures, intuitively put, the degree to which changes in one vector correspond with changes in the other. Formally, covariance is defined as follows (notice that we use $n-1$ in the denominator, to obtain an unbiased estimator if the means are unknown):

$$\text{Cov}(\vec{x},\vec{y}) = \frac{1}{n-1} \ \sum_{i=1}^n (x_i - \mu_{\vec{x}}) \ (y_i - \mu_{\vec{y}})$$

There is a visually intuitive geometric interpretation of covariance. To see this, let's look at a short contrived example. 

```{r}
contrived_example <- 
  tribble(
    ~x,   ~y,
    2,    2,
    2.5,  4,
    3.5,  2.5,
    4,    3.5
  )
```

First, notice that the mean of `x` and `y` is 3:

```{r}
means_contr_example <- map_df(contrived_example, mean)
means_contr_example
```

We can then compute the covariance as follows:

```{r}
contrived_example <- 
  contrived_example %>% 
  mutate(
    area_rectangle =                    (x-mean(x)) * (y - mean(y)),
    covariance     =  1/ (n()-1) * sum((x-mean(x)) * (y - mean(y)))
  )
contrived_example
```

Similar to what we did with variance, we can give a geometrical interpretation of covariance. Figure \@ref(fig:chap-02-03-covariance) shows the four summands contributing to the covariance of the `contrived_example`. What this graph clearly shows is that summands can have different signs. If $x_i$ and $y_i$ are both bigger than the mean, or if both are smaller than the mean, then the corresponding summand is positive. Otherwise, the corresponding summand is negative. This means that covariance captures the degree to which pairs $x_i$ and $y_i$ tend to deviate from the mean in the same general direction. A positive covariance is indicative of a positive general association between $\vec{x}$ and $\vec{y}$, while a negative covariance suggests that as you increase $x_i$, the associated $y_i$ becomes smaller.


```{r chap-02-03-covariance, echo = F}
contrived_example %>% 
  ggplot(aes(x, y)) +
  geom_rect(
    aes(
      xmin = map_dbl(x, function(i) {min(i, means_contr_example$x)}),
      ymin = map_dbl(y, function(i) {min(i, means_contr_example$y)}),
      xmax = map_dbl(x, function(i) {max(i, means_contr_example$x)}),
      ymax = map_dbl(y, function(i) {max(i, means_contr_example$y)}),
      fill = - area_rectangle
    )
  ) +
  geom_point(color = "darkorange", size = 4) +
  geom_point(data = means_contr_example, color = "white", size = 10) +
  theme(legend.position = "none") +
  geom_text(
    aes(
      x = c(2.5, 2.75,  3.25, 3.5),
      y = c(2.5,  3.5,  2.75, 3.25), 
      label = area_rectangle
    ), 
    color = "white"
  ) +
  ggtitle("Rectangular areas contributing to the computation of covariance")
```



We can, of course, also calculate covariance just with a built-in base R function, `cov`:

```{r}
with(contrived_example, cov(x,y))
```


And, using this function, we can calculate the covariance between `total_volume_sold` and `average_price` in the avocado data:

```{r}
with(avocado_data, cov(log(total_volume_sold), average_price))
```

Interestingly, the negative covariance in this example suggests that across all associated data pairs, the larger `total_volume_sold`, the lower `average_price`. It is important that this is a descriptive statistics, and that this is not to be interpreted as evidence or causal relation between the two measures of interest. Not in this example, not in any other. Covariance describes associated data points; it is not evidence for causal relationships.

### Correlation

The problem with covariance is that it is not invariant under linear transformation. Consider the `contrived_example` from above once more. The original data had the following covariance:

```{r}
with(contrived_example, cov(x,y))
```

But if we just linearly transform, say, vector `y` to `1000 * y + 500` (e.g., because we switch to an equivalent, but numerically different measuring scale), we obtain:

```{r}
with(contrived_example, cov(x,1000 * y + 500))
```

To compensate for this problem, we can look at **Bravais-Pearson correlation**, which is is covariance standardized by standard deviations:

$$r_{\vec{x}\vec{y}} = \frac{\text{Cov}(\vec{x}, \vec{y})}{\text{SD}(\vec{x}) \ \text{SD}(\vec{y})}$$

Let's check invariance under linear transformation, using the built-in function `cor`. The correlation coefficient for the original data is:

```{r}
with(contrived_example, cor(x,y))
```

The correlation coefficient for the data with linearly transformed `y` is:

```{r}
with(contrived_example, cor(x,1000 * y + 500))
```

Indeed, the correlation coefficient is nicely bounded to lie between -1 and 1. A correlation coefficient of 0 is to be interpreted as the absence of any correlation. A correlation coefficient of 1 is a perfect positive correlation (the higher $x_i$, the higher $y_i$), and -1 indicates a perfect negative correlation (the higher $x_i$, the lower $y_i$). Again, pronounced positive or negative correlations are *not* to be confused with strong evidence for a causal relation. It is just a descriptive statistic on the properties of associated measurements.

In the avocado data, the logarithm of `total_volume_sold` shows a noteworthy correlation with `average_price`. This is also visible in Figure \@ref(fig:chap-02-03-avocado-scatter).

```{r}
with(avocado_data, cor(log(total_volume_sold), average_price))
```


```{r chap-02-03-avocado-scatter, echo = F, fig.cap="Scatter plot of avocado prices, plotted against (logarithms of) the total amount sold. The black line is a linear regression line indicating the (negative) correlation between these measures (more on this later)."}
avocado_data %>% 
  ggplot(aes(x = log(total_volume_sold), y = average_price)) +
  geom_point(color = "darkgray", alpha = 0.3) +
  geom_smooth(color = "black", method = "lm") +
  xlab('Logarithm of total volume sold') +
  ylab('Average price') +
  ggtitle("Avocado prices plotted against the (log) amount sold")
```
