---
editor_options: 
  markdown: 
    wrap: sentence
---

# Comparing frequentist and Bayesian statistics {#ch-05-02-comparison-freq-Bayes}

Bayesian methods allow for probability distributions over latent variables, like model parameters or models themselves.
Frequentist methods do not.
That is the most striking difference between these two approaches to data analysis.
At the heart, this difference is one based on conceptual considerations about what we may or may not attach probabilities to.
Still, conceptual questions aside, there are also further consequences of this difference.
The most obvious is that, usually, Bayesian approaches are more complex to compute or analyze but provide richer information, such as a full distribution rather than just a point- and an interval-estimate.
This chapter will explore some of the more or less obvious differences in order to also contribute a better understanding of both approaches in isolation.


## Frequentist and Bayesian statistical models

Section \@ref(Chap-03-03-models-general) introduced the notion of a Bayesian statistical model as a pair consisting of a likelihood function

$$ P_M(D_\text{DV} \mid D_\text{IV}, \theta) $$

and a prior over model parameters

$$ P_M(\theta)\,.$$

Normally, the frequentist approach is not model-centric, but rather describes its methods as an arsenal of situation-specific tests.
The explicit model-centric explanation of a selection of frequentist tests given in the previous chapter showed that the frequentist models that underlie the computation of $p$-values eradicate all free model parameters by assigning them a single value in one of two ways:

1.  fixing a parameter to the value dictated by the relevant null-hypothesis; or
2.  estimating the value of a parameter directly from the data (e.g., the standard deviation in a $t$-test).

Beyond $p$-values and significance testing, we may say that a *frequentist model* consists only of a likelihood, assuming -as it were-, but never actually using, a flat prior over any remaining free model parameters.

The upshot of this is that, conceptual quibbles about the nature of probability notwithstanding, from a technical point of view frequentist models can be thought of as just special cases of Bayesian models (with parameters either fixed to a single value somehow, or assuming flat priors).
Seeing this subsumption relation is insightful because it implies that frequentist concepts like $p$-value, $\alpha$-error or statistical power all directly import into the Bayesian domain (whether they are equally important and useful or not).
We will visit, for instance, the notion of a Bayesian $p$-value later in this chapter.

## Approximation: in the model or through the computation

Standard frequentist methods often rely on assumptions, e.g., $\chi^2$-tests cash in the fact that, given enough data, it is safe to assume a normal distribution for data that is *de facto* not normally distributed.
In this sense, frequentist statistics has approximation built into the models, but often uses clear-cut mathematical analysis to derive results based on these approximation assumptions.

Bayesian models, on the other hand, frequently do not make these approximations in their models.
But since the posterior inference is hard, if not impossible to solve analytically, the Bayesian approach relies on approximating the Bayesian inference, e.g., via sampling techniques.
In this way, the Bayesian approach shifts the approximation into the computation, not the researcher's assumptions about the data-generating process as such.

Notice, however, that Bayesian models can incorporate the same (kind of) approximations the frequentist approach often critically relies on.
At the same time, the frequentist approach can rely similarly on numerical approximation of its key quantitative notions.
The next section shows an example of this, namely the approximate computation of $p$-values through Monte Carlo sampling.

## MC-simulated $p$-values

Let's reconsider the 24/7 data set, where we have $k=7$ observations of 'heads' in $N=24$ tosses of a coin.

```{r}
# 24/7 data
k_obs <- 7
n_obs <- 24
```

The question of interest is whether the coin is fair, i.e., whether $\theta_c = 0.5$.
R's built-in function `binom.test` calculates a binomial test and produces a $p$-value which is calculated precisely (since this is possible and cheap in this case).

```{r}
binom.test(7,24)
```

It is also possible to approximate a $p$-value by Monte Carlo simulation.
Notice that the definition of a $p$-value repeated here from Section \@ref(ch-03-05-hypothesis-p-values) is just a statement about the probability that a random variable (from which we can take samples with MC simulation) delivers a value below a fixed threshold:

$$
p\left(D_{\text{obs}}\right) = P\left(T^{|H_0} \succeq^{H_{0,a}} t\left(D_{\text{obs}}\right)\right)  % = P(\mathcal{D}^{|H_0} \in \{D \mid t(D) \ge t(D_{\text{obs}})\}) 
$$

So here goes:

```{r}
# specify how many Monte Carlo samples to take
x_reps <- 500000

# build a vector of likelihoods (= the relevant test statistic)
#   for hypothetical data observations, which are 
#   sampled based on the assumption that H0 is true
lhs <- map_dbl(1:x_reps, function(i) {
  # hypothetical data assuming H0 is true
  k_hyp <- rbinom(1, size = n_obs, prob = 0.5)
  # likelihood of that hypothetical observation
  dbinom(k_hyp, size = n_obs, prob = 0.5)
})

# likelihood (= test statistic) of the observed data
lh_obs = dbinom(k_obs, size = n_obs, prob = 0.5)

# proportion of samples with a lower or equal likelihood than 
#   the observed data 
mean(lhs <= lh_obs) %>% show()
```


Monte Carlo sampling for $p$-value approximation is always possible, even for cases where we cannot rely on known simplifying assumptions.

## Bayesian $p$-values & model checking

The previous section showed how to approximate a $p$-value with Monte Carlo sampling.
Notice that nothing in this sampling-based approach hinges on the model having no free parameters.
Indeed, we can similarly approximate so-called *Bayesian predictive $p$-values*.
Bayesian predictive $p$-values have a good role to play in Bayesian data analysis: they are one possible tool for *model checking* a.k.a. *model criticism*.

Suppose we have a Bayesian model for the binomial 24/7 data.
The model consists of the usual likelihood function, but also has a prior (maybe from previous research, or maybe obtained from training the model on a training data set):

$$
\theta_c \sim \text{Beta}(11,2)
$$

Notice that this is a biased prior, placing more weight on the idea that the coin is biased towards heads.
In model checking we ask whether the given model could be a plausible model for some data at hand.
We are not comparing models, we just "check" or "test" (!) the model as such.
Acing the test doesn't mean that there could not be much better models.
Failing the test doesn't mean that we know of a better model (we may just have to do more thinking).

Let's approximate a Bayesian predictive $p$-value for this Bayesian model and the 24/7 data.
The calculations are analogous to those in the previous section.

```{r}
# 24/7 data
k_obs <- 7
n_obs <- 24

# specify how many Monte Carlo samples to take
x_reps <- 500000

# build a vector of likelihoods (= the relevant test statistic)
#   for hypothetical data observations, which are 
#   sampled based on the assumption that the
#   Bayesian model to be tested is true
lhs <- map_dbl(1:x_reps, function(i) {
  # hypothetical data assuming the model is true
  #   first sample from the prior
  #   then sample from the likelihood
  theta_hyp <- rbeta(1, 11, 2)
  k_hyp <- rbinom(1, size = n_obs, prob = theta_hyp)
  # likelihood of that hypothetical observation
  dbinom(k_hyp, size = n_obs, prob = theta_hyp)
})

# likelihood (= test statistic) of the observed data
#   determined using MC sampling
lh_obs = map_dbl(1:x_reps, function(i){
  theta_hyp <- rbeta(1, 11, 2)
  dbinom(k_obs, size = n_obs, prob = theta_hyp)
}) %>% mean()
  

# proportion of samples with a lower or equal likelihood than 
#   the observed data 
mean(lhs <= lh_obs) %>% show()
```

This Bayesian predictive $p$-value is rather low, suggesting that this model (prior & likelihood) is *NOT* a good model for the 24/7 data set.

We can use Bayesian $p$-values for any Bayesian model, whether built on a prior or posterior distribution.
A common application of Bayesian $p$-values in model checking are so-called **posterior predictive checks**.
We compute a Bayesian posterior for observed data $D_\text{obs}$ and then test, via a Bayesian posterior predictive $p$-value, whether the trained model is actually a good model for $D_\text{obs}$ itself.
If the $p$-value is high, that's no cause for hysterical glee.
It just means that there is no cause for alarm.
If the Bayesian posterior predictive $p$-value is very low, the posterior predictive test has failed, and that means that the model, even when trained on the data $D_\text{obs}$, is *NOT* a good model of that very data.
The model must miss something crucial about the data $D_\text{obs}$.
Better start researching what that is and build a better model if possible.

Most importantly, these considerations of Bayesian $p$-values show that frequentist testing has a clear analog in the Bayesian realm, namely as model checking.

## Comparing Bayesian and frequentist estimates {#ch-05-01-estimation-comparison}

As discussed in Chapter \@ref(ch-03-04-parameter-estimation), parameter estimation is traditionally governed by two measures: (i) a point-estimate for the best parameter value, and (ii) an interval-estimate for a range of values that are considered "good enough". Table \@ref(tab:ch-05-01-estimation-overview) gives the most salient answers that the Bayesian and the frequentist approaches give.

```{r ch-05-01-estimation-overview, echo = F}
table_data <- tribble(
  ~estimate, ~Bayesian, ~frequentist,
  "best value", "mean of posterior", "maximum likelihood estimate",
  "interval range", "credible interval (HDI)", "confidence interval"
)
knitr::kable(
  table_data,
  escape = F,
  caption = "Common methods of obtaining point-valued and interval-range estimates for parameters, given some data, in frequentist and Bayesian approaches.", 
  booktabs = TRUE
)
```

For Bayesians, point-valued and interval-based estimates are just summary statistics to efficiently communicate about or reason with the main thing: the full posterior distribution.
For the frequentist, the point-valued and interval-based estimates might be all there is.
Computing a full posterior can be very hard.
Computing point-estimates is usually much simpler.
Yet, all the trouble of having to specify priors, and having to calculate a much more complex mathematical object, can pay off.
An example which is intuitive enough is that of a likelihood function in a multi-dimensional parameter space where there is an infinite collection of parameter values that maximize the likelihood function (think of a plateau).
Asking a godly oracle for "the" MLE can be disastrously misleading.
The full posterior will show the quirkiness.
In other words, to find an MLE can be an ill-posed problem where exploring the posterior surface is not.

Practical issues aside, there are also conceptual arguments that can be pinned against each other.
Suppose you do not know the bias of a coin, you flip it once and it lands heads.
The case in mathematical notation: $k=1$, $N=1$.
As a frequentist, your "best" estimate of the coin's bias is that it is 100% rigged: it will *never* land tails.
As a Bayesian, with uninformed priors, your "best" estimate is, following Laplace rule, $\frac{k+1}{N+2} = \frac{2}{3}$.
Notice that there might be different notions of what counts as "best" in place.
Still, the frequentist "best" estimate seems rather extreme.

What about interval-ranged estimates?
Which is the better tool, confidence intervals or credible intervals?
-- This is hard to answer.
Numerical simulations can help answer these questions.[^relation-bayes-frequentism-1]
The idea is simple but immensely powerful.
We simulate, repeatedly, a ground-truth and synthetic results for fictitious experiments, and then we apply the statistical tests/procedures to these fictitious data sets.
Since we know the ground-truth, we can check which tests/procedures got it right.

[^relation-bayes-frequentism-1]: Even where the math seems daunting, simulation methods are much more tangible, applicable and sometimes require only basic programming experience.

Let's look at a simulation set-up to compare credible intervals to confidence intervals, the latter of which are calculated by asymptotic approximation or the so-called exact method (see the info-box in Section \@ref(ch-05-01-frequentist-testing-confidence-intervals)).
To do so, we repeatedly sample a ground-truth (e.g., a known coin bias $\theta_{\text{true}}$) from a flat distribution over $[0;1]$.[^relation-bayes-frequentism-2].
We then simulate an experiment in a synthetic world with $\theta_{\text{true}}$, using a fixed value of $n$, here taken from the set $n \in \set{10, 25, 100, 1000}$.
We then construct a confidence interval (either approximately or precisely) and a 95% credible interval; for each of the three interval estimates.
We check whether the ground-truth $\theta_{\text{true}}$ is *not* included in any given interval estimate.
We calculate the mean number of times such as non-inclusion (errors!) happen for each kind of interval estimate.
The code below implements this and the figure below shows the results based on 10,000 samples of $\theta_{\text{true}}$.

[^relation-bayes-frequentism-2]: This is already not innocuous.
    We are fixing, as it were, an assumption about how likely ground-truths should actually occur in the real world.

```{r}
# how many "true" thetas to sample
n_samples <- 10000 
# sample a "true" theta
theta_true <- runif(n = n_samples)
# create data frame to store results in
results <- expand.grid(
  theta_true = theta_true,
  n_flips = c(10, 25, 100, 1000)
) %>% 
  as_tibble() %>% 
  mutate(
    outcome = 0,
    norm_approx = 0,
    exact = 0,
    Bayes_HDI = 0
  )
  
for (i in 1:nrow(results)) {
  
  # sample fictitious experimental outcome for current true theta
  results$outcome[i] <- rbinom(
    n = 1, 
    size = results$n_flips[i], 
    prob = results$theta_true[i]
  )
  
  # get CI based on asymptotic Gaussian
  norm_approx_CI <- binom::binom.confint(
    results$outcome[i], 
    results$n_flips[i], 
    method = "asymptotic"
  )
  results$norm_approx[i] <- !(
    norm_approx_CI$lower <= results$theta_true[i] && 
      norm_approx_CI$upper >= results$theta_true[i]
    )
  
  # get CI based on exact method
  exact_CI <- binom::binom.confint(
    results$outcome[i], 
    results$n_flips[i], 
    method = "exact"
  )
  results$exact[i] <- !(
    exact_CI$lower <= results$theta_true[i] && 
      exact_CI$upper >= results$theta_true[i]
  )
  
  # get 95% HDI (flat priors)
  Bayes_HDI <- binom::binom.bayes(
    results$outcome[i], 
    results$n_flips[i], 
    type = "highest", 
    prior.shape1 = 1, 
    prior.shape2 = 1
  )
  results$Bayes_HDI[i] <- !(
    Bayes_HDI$lower <= results$theta_true[i] && 
      Bayes_HDI$upper >= results$theta_true[i]
  )
}

results %>% 
  gather(key = "method", "Type_1", norm_approx, exact, Bayes_HDI) %>% 
  group_by(method, n_flips) %>% 
  dplyr::summarize(avg_type_1 = mean(Type_1)) %>% 
  ungroup() %>% 
  mutate(
    method = factor(
      method, 
      ordered = T, 
      levels = c("norm_approx", "Bayes_HDI", "exact")
    )
  ) %>% 
  ggplot(aes(x = as.factor(n_flips), y = avg_type_1, color = method)) + 
  geom_point(size = 3) + geom_line(aes(group = method), size = 1.3) +
  xlab("number of flips per experiment") +
  ylab("proportion of exclusions of true theta")


```

These results show a few interesting things.
For one, looking at the error-level of the exact confidence intervals, we see that the $\alpha$-level of frequentist statistics is an *upper bound* on the amount of error.
For a discrete sample space, the actual error rate can be substantially lower.
Second, the approximate method for computing confidence intervals is off unless the sample size warrants the approximation.
This stresses the importance of caring about when an approximation underlying a frequentist test is (not) warranted.
Thirdly, the Bayesian credible interval has a "perfect match" to the assumed $\alpha$-level for all sample sizes.
However, we must take into account that the simulation assumes that the Bayesian analysis "knows the true prior".
We have actually sampled the latent parameter $\theta$ from a uniform distribution; and we have used a flat prior for the Bayesian calculations.
Obviously, the more the prior divergences from the true distribution, and the fewer data observations we have, the more errors will the Bayesian approach make.

::: {.exercises}
**Exercise 9.5**

Pick the correct answer:

The most frequently used point-estimate of Bayesian parameter estimation looks at...

a.  ...the median of the posterior distribution.

b.  ...the maximum likelihood estimate.

c.  ...the mean of the posterior distribution.

d.  ...the normalizing constant in Bayes rule.

::: {.collapsibleSolution}
<button class="trigger">

Solution

</button>

::: {.content}
Statement c.
is correct.
:::
:::

The most frequently used interval-based estimate in frequentist approaches is...

a.  ...the support of the likelihood distribution.

b.  ...the confidence interval.

c.  ...the hypothesis interval.

d.  ...the 95% highest-density interval of the maximum likelihood estimate.

::: {.collapsibleSolution}
<button class="trigger">

Solution

</button>

::: {.content}
Statement b.
is correct.
:::
:::
:::

## Beliefs, decisions and long-term error

Bayesianism is about beliefs, frequentism is about action choices (at least in the post-Fisherian, Neyman-Pearson and modern NHST variant).
Bayesians can layer a decision procedure on top of the inferred probabilities, but they do not have to.

The Neyman-Pearson variant of frequentism, on the other hand, is inseparably tied to a choice criterion, thereby aiming to provide the long-term error control that motivates this approach.
If Bayesian approaches adopt a fixed decision routine, like Kruschke's ternary decision rules outlined in Chapter \@ref(ch-03-07-hypothesis-testing-Bayes), they can be subjected to considerations of long-term error control.
It can then even make sense to perform power calculations similar to those in the frequentist approach (usually: simulation based).

## Evidence for the null

Frequentist analyses in the style of Neyman-Pearson do allow for a categorical decision to "accept the null-hypothesis".
This requires specification of a (point-valued) alternative hypothesis and it requires sufficient statistical power (see Section \@ref(ch-03-04-hypothesis-significance-errors)).
Nonetheless, this approach still relies on using a $p$-value derived from the assumption that the null-hypothesis is true.
This is still a measure of testing whether the null-model is a plausible model of the data. 
The frequentist approach does not offer a direct and intuitively interpretable measure of evidence in favor of the null-hypothesis.

Arguably, the most straightforward measure of evidence in favor of the null-hypothesis involves assigning some relative probability to it.
This can only be achieved under a Bayesian approach.
For example, using model comparison, a Bayesian approach to testing a null-hypothesis is able to conclude that there is evidence in favor of the null-hypothesis (when compared against some alternative) without this necessarily being tight to (i) a point-valued alternative hypothesis or (ii) a binary decision in favor of the null-hypothesis.

## Three pillars of data analysis {#Chap-05-02-models-three-pillars}

There are three main uses for models in statistical data analysis:

1. **Parameter estimation**: Based on model $M$ and data $D$, we try to infer which value of the parameter vector $\theta$ we should believe in or work with (e.g., base our decision on). Parameter estimation can also serve knowledge gain, especially if (some component of) $\theta$ is theoretically interesting.
2. **Model comparison**: If we formulate at least two alternative models, we can ask which model better explains or better predicts some data. In some of its guises, model comparison helps with the question of whether a given data set provides evidence in favor of one model and against another other, and if so, how much.
3. **Prediction**: Models can also be used to make predictions about future or hypothetical data observations. 

The frequentist and the Bayesian approach each have their specific methods and techniques to do estimation, comparison, and prediction. Even within each approach (frequentist or Bayesian) and a particular goal (estimation, comparison, or prediction) there is not necessarily unanimity about the best method or technique. 

Table \@ref(tab:ch-03-03-pillars-of-DA) lists the most common/salient methods used for each goal in the frequentist and Bayesian approach, as discussed in the previous chapters.

```{r ch-03-03-pillars-of-DA, echo = F}
table_data <- tribble(
  ~`inferential goal`, ~target, ~frequentist, ~Bayesian,
  "estimation", "$\\theta$",  "MLE: $\\hat{\\theta} = \\arg \\max_{\\theta}\  P_M(D \\mid \\theta)$", "posterior: $P_M(\\theta \\mid D)$",
  "comparison", "$M$", "AIC, LR-test", "Bayes factor",
  "prediction", "$D$", "MLE-based: $P_M(D_{rep} \\mid \\hat{\\theta})$", "Posterior-based: $P_M(D_{rep} \\mid D)$"
)
knitr::kable(
  table_data,
  escape = F,
  caption = "Most common/salient methods of frequentist and Bayesian approaches for the three major goals of model-based data analysis. The abbreviations used are: MLE for 'maximum likelihood estimate', AIC for 'Akaike information criterion', LR-test for 'likelihood-ratio test' and $D_{rep}$ for 'repeat data'.", 
  booktabs = TRUE
)
```

The three pillars of data analysis mentioned above are tightly related, of course. 
For one, model comparison is often parasitic on prediction: whereas prediction asks which data is to be expected, given the model, model comparison looks at how well a given data set is or would have been predicted by different models. 
For another, parameter estimation and data predictions are something like each others' reverse operations. 

## Testing hypotheses by estimation, comparison & model checking

Each of the "three pillars of data analysis" discussed in the previous section can be used to test a statistical hypothesis.
This is where we see a further difference between Bayesian and frequentist approaches.

Bayesian hypothesis testing uses either parameter estimation or model comparison (e.g., Bayes factors), as discussed extensively in Chapter \@ref(ch-03-07-hypothesis-testing-Bayes).

Frequentist hypothesis testing in terms of $p$-values is based on the third pillar "prediction".
To see this, recall that $p$-values are derived from the assumption that the point-valued null hypothesis is true.
We then ask: based on a model which assumes that the null-hypothesis is true, would we be surprised by the data we observed?; which is the same as asking: would we have *predicted* the data we actually saw?

Seeing this difference also explains why sometimes frequentist and Bayesian approaches give different results when testing the same null-hypothesis based on the same observed data.
The next section discusses such a case.

## Jeffreys-Lindley paradox

Often, Bayesian and frequentist methods yield qualitatively similar results.
But sometimes results diverge.
A prominent case of divergence is known as the Savage-Lindley paradox.
The case is not really a "paradox" in a strict sense. 
It's a case where predictions are clearly divergent, and it raises attention for the differences between frequentist and Bayesian testing of point-valued null hypotheses.

Let's take the following data.

```{r}
k = 49581
N = 98451
```

The point-valued null hypothesis is whether the binomial rate is unbiased, so $\theta_c = 0.5$.

```{r}
binom.test(k, N)$p.value
```

Based on the standard $\alpha$-level of $0.05$, frequentism thus prescribes to reject $H_0$.

In contrast, using the Savage-Dickey method to compute the Bayes factor, we find strong support *in favor of* $H_0$.

```{r}
dbeta(0.5, k + 1, N - k + 1)
```  

The reason why these methods give different results is because they *are* conceptually completely different things. There is no genuine paradox.

Frequentist testing is a form of model checking. 
The question addressed by the frequentist hypothesis test is whether a model that assumes that $\theta_c = 0.5$ is such that, if we assume that this model is true, the data above appears surprising.

The Bayesian method used above hinges on the comparison of two models.
The question addressed by the Bayesian comparison-based hypothesis test is which of two models better predicts the observed data from an *ex ante* point of view (i.e., before having seen the data): the first model assumes that $\theta_c = 0.5$ and the second model assumes that $\theta_c \sim \text{Beta}(1,1)$.

For a large $N$, like in the example at hand, it can be the case that $\theta_c = 0.5$ is a bad explanation for the data, so that a model-checking test rejects this null hypothesis.
At the same time, the alternative model with $\theta_c \sim \text{Beta}(1,1)$ is *even worse* than the model $\theta_c = 0.5$, because it puts credence on many values for $\theta_c$ that are very, very bad predictors of the data.

None of these considerations lend themselves to a principled argument for or against frequentism or Bayesianism.
The lesson to be learned is that these different approaches ask different questions (about models and data).
The agile data analyst will diligently check each concrete research context for which method is most conducive to gaining the insights relevant for the given purpose.

## Explicit beliefs vs. implicit intentions

The main objection against Bayesianism which motivated and still drives the frequentist program is that Bayesian priors are *subjective*, and therefore to be regarded as less scientific than hard *objectively justifiable* ingredients, such as likelihood functions.
A modern Bayesian riposte bites the bullet, chews it well, and spits it back.
While priors are subjective, they are at least explicit.
They are completely out in the open and, if the data is available, predictions for any other set of prior assumptions can simply be tested.
A debate about which "subjective priors" to choose is "objectively" possible.
In contrast, the frequentist notion of a $p$-value (and with it the confidence interval) relies on something *even more* mystic, namely the researcher's intentions during the data collection, something that is not even in principle openly scrutinizable after the fact. 
To see how central frequentist notions rely on implicit intentions and counterfactual assumptions about data we could have seen but didn't, let's consider an example (see also @Wagenmakers2007:A-practical-sol and @kruschke2015 for discussion).

The example is based on the 24/7 data again.
This time, we are going to look at two cases.

1. **Stop at $N=24$**: The researchers decided in advance to collect $N=24$ data points. They found $k=7$ heads in this experiment.
2. **Stop at $k=7$**: The researchers decided to flip their coin until they observed $k=7$ heads. It took them $N=24$ tosses in their experiment.

The research question is, as usual, whether $\theta_c = 0.5$.

A common intuition is to say: why would the manner of data collection matter?
Data is data.
We base our inference on data.
We don't base our inference on how the data was obtained.
Right? -
Wrong if you are a frequentist.

The manner of data collection dictates what other possible observations of the experiment are.
These in turn matter for computing $p$-values.
For the "Stop at $N=24$" case, the likelihood function is what we used before, the Binomial distribution:

$$ 
\text{Binomial}(k ; n = 24, \theta = 0.5) = {{N}\choose{k}} \theta^{k} \, (1-\theta)^{n-k} 
$$

We therefore obtain the $p$-value-based result that the null hypothesis cannot be rejected (at $\alpha = 0.05$).

```{r, echo = FALSE, fig.align='center', fig.width=5, fig.height=3}
  
plotData = data.frame(x = 0:24, y = dbinom(0:24, 24, 0.5))
plotData2 = data.frame(x = c(0:7, 17:24), y = dbinom(c(0:7, 17:24), 24, 0.5))
sig.plot = ggplot(plotData, aes(x = x , y = y )) + geom_bar(stat = "identity", fill = "skyblue", width = 0.35) +
  geom_bar(data = plotData2, aes(x = x, y = y), stat = "identity", fill = "darkblue", width = 0.35) +
  geom_hline(yintercept=dbinom(7,24,0.5)) + xlab("k") + ylab("B(k | N = 24, theta = 0.5)") +
  # geom_text(data.frame(x = 3, y = 0.05, label = paste0("p = " , round(1-sum(dbinom(8:16, 24, 0.5)),3), collapse = "")), aes(x = x, y = y, label = label)) 
  geom_text(x = 3, y = 0.03, label = paste0("p = " , round(1-sum(dbinom(8:16, 24, 0.5)),3), collapse = ""))
sig.plot
```

But when we look at the "Stop at $k=7$" case, we need a different likelihood function.
In principle, we might have had to flip the coin for more than $N=24$ times until receiving $k=7$ heads.
The likelihood function needed for this case is the *negative Binomial distribution*:

$$ \text{neg-Binomial}(n ; k = 7, \theta = 0.5) = \frac{k}{n} \choose{n}{k} \theta^{k} \, (1-\theta)^{n - k}$$

The resulting sampling distribution and the $p$-value we obtain for it are shown in the plot below.

```{r, echo = FALSE, fig.align='center', fig.width=4, fig.height=3}

negBinom <- function(k, N, theta) {
  sapply(1:length(k), function(x) k[x]/N * dbinom(k[x], N, theta))
}
  
plotData = data.frame(x = 7:35, 
                      y = negBinom(7, 7:35, 0.5))
plotData2 = data.frame(x = 24:35,
                       y = negBinom(7, 24:35, 0.5))
myplot = ggplot(plotData, aes(x = x , y = y )) + geom_bar(stat = "identity", fill = "skyblue", width = 0.35) +
  geom_bar(data = plotData2, aes(x = x, y = y), stat = "identity", fill = "darkblue", width = 0.35) +
  geom_hline(yintercept=7/24*dbinom(7,24,0.5)) + xlab("N") + ylab("NB(N | k = 7, theta = 0.5)") +
  # geom_text(data.frame(x = 3, y = 0.05, label = paste0("p = " , round(1-sum(dbinom(8:16, 24, 0.5)),3), collapse = "")), aes(x = x, y = y, label = label)) 
  geom_text(x = 30, y = 0.015, label = paste0("p = " , round(1-sum(negBinom(7, 7:23, 0.5)),3), collapse = ""))
myplot

```

So, with the exact same data but different assumptions about how this data was generated, we get a different $p$-value; indeed, a difference that spans the significance boundary of $\alpha = 0.05$.
The researcher's intentions about how to collect data influence the statistical analysis.
Dependence on researcher intentions is worse than dependence on subjective priors, because it is impossible to verify *ex post* what the precise data-generating protocol was.

Wait!
Doesn't Bayesian inference have this problem?
No, it doesn't.
The difference in likelihood functions used above is a different normalizing constant.
The normalizing constant cancels out in parameter estimation, and also in model-comparison (if we assume that both models compared use the same likelihood function).^[If we want to ask: "Which likelihood function better explains the data: Binomial or negative Binomial?", we can, of course, compare the appropriate models, so that the non-cancellation of the normalizing constants is exactly what we want.]
The case of model comparison is obvious.
To see that normalizing constants cancel out for parameter estimation, consider this:

$$
\begin{align*}
P(\theta \mid D) & = \frac{P(\theta) \ P(D \mid \theta)}{\int_{\theta'} P(\theta') \ P(D \mid \theta')} \\
& = \frac{ \frac{1}{X} \ P(\theta) \ P(D \mid \theta)}{ \ \frac{1}{X}\ \int_{\theta'} P(\theta') \ P(D \mid \theta')} \\
& = \frac{P(\theta) \ \frac{1}{X}\ P(D \mid \theta)}{  \int_{\theta'} P(\theta') \ \frac{1}{X}\ P(D \mid \theta')}
\end{align*}
$$


